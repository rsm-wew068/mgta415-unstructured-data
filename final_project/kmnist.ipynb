{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split, Dataset, SubsetRandomSampler\n",
    "from collections import Counter\n",
    "import os\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images Shape: (232365, 28, 28)\n",
      "Train Labels Shape: (232365,)\n",
      "Test Images Shape: (38547, 28, 28)\n",
      "Test Labels Shape: (38547,)\n"
     ]
    }
   ],
   "source": [
    "# Define paths to downloaded files\n",
    "\n",
    "train_images_path = \"data/KMNIST/raw/k49-train-imgs.npz\"\n",
    "train_labels_path = \"data/KMNIST/raw/k49-train-labels.npz\"\n",
    "test_images_path = \"data/KMNIST/raw/k49-test-imgs.npz\"\n",
    "test_labels_path = \"data/KMNIST/raw/k49-test-labels.npz\"\n",
    "\n",
    "# Load the dataset\n",
    "train_images = np.load(train_images_path)['arr_0']\n",
    "train_labels = np.load(train_labels_path)['arr_0']\n",
    "test_images = np.load(test_images_path)['arr_0']\n",
    "test_labels = np.load(test_labels_path)['arr_0']\n",
    "\n",
    "print(f\"Train Images Shape: {train_images.shape}\")  # (270912, 28, 28)\n",
    "print(f\"Train Labels Shape: {train_labels.shape}\")  # (270912,)\n",
    "print(f\"Test Images Shape: {test_images.shape}\")    # (45792, 28, 28)\n",
    "print(f\"Test Labels Shape: {test_labels.shape}\")    # (45792,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 232365\n",
      "Total testing samples: 38547\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize pixel values\n",
    "])\n",
    "\n",
    "# Custom dataset class\n",
    "class Kuzushiji49Dataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert to PIL image and apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Create dataset instances\n",
    "train_set = Kuzushiji49Dataset(train_images, train_labels, transform=transform)\n",
    "test_set = Kuzushiji49Dataset(test_images, test_labels, transform=transform)\n",
    "\n",
    "# Create DataLoaders for batch processing\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Total training samples: {len(train_set)}\")\n",
    "print(f\"Total testing samples: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Extract entire train and test data\n",
    "train_data = torch.cat([data for data, _ in train_loader], dim=0)\n",
    "train_labels = torch.cat([labels for _, labels in train_loader], dim=0)\n",
    "\n",
    "test_data = torch.cat([data for data, _ in test_loader], dim=0)\n",
    "test_labels = torch.cat([labels for _, labels in test_loader], dim=0)\n",
    "\n",
    "# Flatten images from (N, 1, 28, 28) -> (N, 784)\n",
    "train_data = train_data.view(train_data.shape[0], -1).to(device)  # (60000, 784)\n",
    "test_data = test_data.view(test_data.shape[0], -1).to(device)  # (10000, 784)\n",
    "\n",
    "train_labels = train_labels.to(device)\n",
    "test_labels = test_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>codepoint</th>\n",
       "      <th>char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>U+3042</td>\n",
       "      <td>あ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>U+3044</td>\n",
       "      <td>い</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>U+3046</td>\n",
       "      <td>う</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>U+3048</td>\n",
       "      <td>え</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>U+304A</td>\n",
       "      <td>お</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>U+304B</td>\n",
       "      <td>か</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>U+304D</td>\n",
       "      <td>き</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>U+304F</td>\n",
       "      <td>く</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>U+3051</td>\n",
       "      <td>け</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>U+3053</td>\n",
       "      <td>こ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>U+3055</td>\n",
       "      <td>さ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>U+3057</td>\n",
       "      <td>し</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>U+3059</td>\n",
       "      <td>す</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>U+305B</td>\n",
       "      <td>せ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>U+305D</td>\n",
       "      <td>そ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>U+305F</td>\n",
       "      <td>た</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>U+3061</td>\n",
       "      <td>ち</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>U+3064</td>\n",
       "      <td>つ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>U+3066</td>\n",
       "      <td>て</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>U+3068</td>\n",
       "      <td>と</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>U+306A</td>\n",
       "      <td>な</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>U+306B</td>\n",
       "      <td>に</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>U+306C</td>\n",
       "      <td>ぬ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>U+306D</td>\n",
       "      <td>ね</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>U+306E</td>\n",
       "      <td>の</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>U+306F</td>\n",
       "      <td>は</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>U+3072</td>\n",
       "      <td>ひ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>U+3075</td>\n",
       "      <td>ふ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>U+3078</td>\n",
       "      <td>へ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>U+307B</td>\n",
       "      <td>ほ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>U+307E</td>\n",
       "      <td>ま</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>U+307F</td>\n",
       "      <td>み</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>U+3080</td>\n",
       "      <td>む</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>U+3081</td>\n",
       "      <td>め</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>U+3082</td>\n",
       "      <td>も</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>U+3084</td>\n",
       "      <td>や</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>U+3086</td>\n",
       "      <td>ゆ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>U+3088</td>\n",
       "      <td>よ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>U+3089</td>\n",
       "      <td>ら</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>U+308A</td>\n",
       "      <td>り</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>U+308B</td>\n",
       "      <td>る</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>U+308C</td>\n",
       "      <td>れ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>U+308D</td>\n",
       "      <td>ろ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>U+308F</td>\n",
       "      <td>わ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>U+3090</td>\n",
       "      <td>ゐ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>U+3091</td>\n",
       "      <td>ゑ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>U+3092</td>\n",
       "      <td>を</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>U+3093</td>\n",
       "      <td>ん</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>U+309D</td>\n",
       "      <td>ゝ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index codepoint char\n",
       "0       0    U+3042    あ\n",
       "1       1    U+3044    い\n",
       "2       2    U+3046    う\n",
       "3       3    U+3048    え\n",
       "4       4    U+304A    お\n",
       "5       5    U+304B    か\n",
       "6       6    U+304D    き\n",
       "7       7    U+304F    く\n",
       "8       8    U+3051    け\n",
       "9       9    U+3053    こ\n",
       "10     10    U+3055    さ\n",
       "11     11    U+3057    し\n",
       "12     12    U+3059    す\n",
       "13     13    U+305B    せ\n",
       "14     14    U+305D    そ\n",
       "15     15    U+305F    た\n",
       "16     16    U+3061    ち\n",
       "17     17    U+3064    つ\n",
       "18     18    U+3066    て\n",
       "19     19    U+3068    と\n",
       "20     20    U+306A    な\n",
       "21     21    U+306B    に\n",
       "22     22    U+306C    ぬ\n",
       "23     23    U+306D    ね\n",
       "24     24    U+306E    の\n",
       "25     25    U+306F    は\n",
       "26     26    U+3072    ひ\n",
       "27     27    U+3075    ふ\n",
       "28     28    U+3078    へ\n",
       "29     29    U+307B    ほ\n",
       "30     30    U+307E    ま\n",
       "31     31    U+307F    み\n",
       "32     32    U+3080    む\n",
       "33     33    U+3081    め\n",
       "34     34    U+3082    も\n",
       "35     35    U+3084    や\n",
       "36     36    U+3086    ゆ\n",
       "37     37    U+3088    よ\n",
       "38     38    U+3089    ら\n",
       "39     39    U+308A    り\n",
       "40     40    U+308B    る\n",
       "41     41    U+308C    れ\n",
       "42     42    U+308D    ろ\n",
       "43     43    U+308F    わ\n",
       "44     44    U+3090    ゐ\n",
       "45     45    U+3091    ゑ\n",
       "46     46    U+3092    を\n",
       "47     47    U+3093    ん\n",
       "48     48    U+309D    ゝ"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_path = 'data/KMNIST/raw/k49_classmap.csv'\n",
    "mapping = pd.read_csv(mapping_path)\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Plot first 5 images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(10, 2))\n",
    "for i in range(0,10):\n",
    "    axes[i].imshow(train_images[i].squeeze(), cmap='gray')\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(f\"Label: {train_labels[i].item()}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subsets = np.array([125000, 75000, 50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(test_data, prototype_data, prototype_labels, k=1):\n",
    "    # Compute full pairwise distance matrix in one go\n",
    "    print(\"Computing full distance matrix...\")\n",
    "    distances = torch.cdist(test_data, prototype_data)  # Shape: (10000, 60000)\n",
    "\n",
    "    # Get indices of k nearest neighbors\n",
    "    k_indices = torch.topk(distances, k, largest=False).indices  # Shape: (10000, k)\n",
    "\n",
    "    # Retrieve the k nearest labels\n",
    "    k_labels = prototype_labels[k_indices]  # Shape: (10000, k)\n",
    "\n",
    "    # Majority voting for prediction\n",
    "    pred_labels = torch.mode(k_labels, dim=1).values  # Shape: (10000,)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = (pred_labels == test_labels).float().mean().item()\n",
    "    print(f'{k}-NN accuracy on full test set (no batching): {accuracy:.4f}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8306\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8298\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8294\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8291\n",
      "Computing full distance matrix...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     prototype_train_data \u001b[38;5;241m=\u001b[39m train_data[random_indicies]\n\u001b[1;32m      7\u001b[0m     prototype_train_labels \u001b[38;5;241m=\u001b[39m train_labels[random_indicies]\n\u001b[0;32m----> 8\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprototype_train_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprototype_train_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     accuracy_list\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[1;32m     10\u001b[0m accuracy_dict[subset] \u001b[38;5;241m=\u001b[39m accuracy_list\n",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m, in \u001b[0;36mcompute_accuracy\u001b[0;34m(test_data, prototype_data, prototype_labels, k)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_accuracy\u001b[39m(test_data, prototype_data, prototype_labels, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Compute full pairwise distance matrix in one go\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing full distance matrix...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprototype_data\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (10000, 60000)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Get indices of k nearest neighbors\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     k_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(distances, k, largest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mindices  \u001b[38;5;66;03m# Shape: (10000, k)\u001b[39;00m\n",
      "File \u001b[0;32m~/.rsm-msba/lib/python3.11/site-packages/torch/functional.py:1483\u001b[0m, in \u001b[0;36mcdist\u001b[0;34m(x1, x2, p, compute_mode)\u001b[0m\n\u001b[1;32m   1479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   1480\u001b[0m         cdist, (x1, x2), x1, x2, p\u001b[38;5;241m=\u001b[39mp, compute_mode\u001b[38;5;241m=\u001b[39mcompute_mode\n\u001b[1;32m   1481\u001b[0m     )\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist_if_necessary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mcdist(x1, x2, p, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracy_dict = {}\n",
    "for subset in num_subsets:\n",
    "    accuracy_list = []\n",
    "    for _ in range(10):\n",
    "        random_indicies = torch.randperm(train_data.shape[0])[:subset]\n",
    "        prototype_train_data = train_data[random_indicies]\n",
    "        prototype_train_labels = train_labels[random_indicies]\n",
    "        accuracy = compute_accuracy(test_data, prototype_train_data, prototype_train_labels)\n",
    "        accuracy_list.append(accuracy)\n",
    "    accuracy_dict[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125000    0.829268\n",
       " 75000     0.804447\n",
       " 50000     0.782390\n",
       " 10000     0.666864\n",
       " dtype: float64,\n",
       " 125000    0.000761\n",
       " 75000     0.001689\n",
       " 50000     0.001913\n",
       " 10000     0.003843\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df = pd.DataFrame(accuracy_dict)\n",
    "accuracy_df.mean(), accuracy_df.std(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8315\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8296\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8246\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8284\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8299\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8058\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8041\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8043\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8030\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8060\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7788\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7808\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7804\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7797\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7830\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6693\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6675\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6656\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6698\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6680\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8162\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8164\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8166\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8150\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8165\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7864\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7868\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7914\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7883\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7885\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7654\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7625\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7634\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7620\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7645\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.6409\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.6323\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.6312\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.6339\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.6388\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8122\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8132\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8129\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8130\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8119\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7843\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7835\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7802\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7824\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7835\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7552\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7592\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7583\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7584\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7570\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.6313\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.6247\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.6250\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.6266\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.6270\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8058\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8041\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8035\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8058\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8054\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7745\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7734\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7724\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7730\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7758\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7487\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7460\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7497\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7464\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7459\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.6142\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.6188\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.6182\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.6122\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.6215\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7973\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7959\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7956\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7959\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7961\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7649\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7629\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7639\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7629\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7642\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7368\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7375\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7420\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7361\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7398\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.6068\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.6094\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.6070\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.5984\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.6056\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7879\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7895\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7902\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7895\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7902\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7585\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7564\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7593\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7559\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7553\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7279\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7273\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7320\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7286\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7264\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5972\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.6000\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5976\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5940\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5924\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7807\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7834\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7827\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7819\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7815\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7477\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7492\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7512\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7487\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7516\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7202\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7218\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7234\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7204\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7208\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5824\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5866\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5860\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5902\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5873\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7741\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7780\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7760\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7774\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7764\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7423\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7454\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7451\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7437\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7409\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7142\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7147\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7135\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7162\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7090\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5667\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5776\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5749\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5788\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5744\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7694\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7713\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7696\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7686\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7700\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7374\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7363\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7356\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7343\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7381\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7055\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7090\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7048\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7071\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7078\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5750\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5693\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5667\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5720\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5646\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7660\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7673\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7659\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7650\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7640\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7331\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7299\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7284\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7306\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7285\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7010\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6994\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7053\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7041\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7003\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5577\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5529\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5567\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5588\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5596\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7574\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7614\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7592\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7603\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7603\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7234\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7245\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7260\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7258\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7267\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6966\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6985\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6929\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6933\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6916\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5488\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5562\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5554\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5531\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5541\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7544\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7560\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7569\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7558\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7525\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7177\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7207\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7186\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7225\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7165\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6880\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6909\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6878\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6852\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6900\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5495\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5439\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5501\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5466\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5527\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7520\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7513\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7489\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7507\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7496\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7153\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7122\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7126\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7117\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7132\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6855\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6835\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6852\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6848\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6865\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.5377\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.5366\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.5386\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.5462\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.5412\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7466\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7443\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7451\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7464\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7465\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7096\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7099\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7079\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7088\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7096\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6792\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6781\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6797\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6777\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6767\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.5341\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.5287\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.5317\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.5408\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.5374\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7415\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7402\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7409\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7435\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7420\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7027\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7052\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7062\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7086\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7018\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6739\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6719\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6721\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6742\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6742\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.5194\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.5308\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.5310\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.5298\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.5252\n"
     ]
    }
   ],
   "source": [
    "full_accuracy_df_random = pd.DataFrame()\n",
    "\n",
    "num_neighbors = np.arange(1,31,2)\n",
    "\n",
    "for k in num_neighbors:\n",
    "    accuracy_dict_random_per_neigbor = {}\n",
    "    for subset in num_subsets:\n",
    "        accuracy_list = []\n",
    "        for _ in range(5):\n",
    "            random_indicies = torch.randperm(train_data.shape[0])[:subset]\n",
    "            prototype_train_data = train_data[random_indicies]\n",
    "            prototype_train_labels = train_labels[random_indicies]\n",
    "            accuracy = compute_accuracy(test_data, prototype_train_data, prototype_train_labels, k)\n",
    "            accuracy_list.append(accuracy)\n",
    "        accuracy_dict_random_per_neigbor[subset] = accuracy_list\n",
    "    temp = pd.DataFrame(accuracy_dict_random_per_neigbor)\n",
    "    temp['k'] = k\n",
    "    full_accuracy_df_random = pd.concat([full_accuracy_df_random, temp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(full_accuracy_df_random['k'], full_accuracy_df_random[125000], label='125000')\n",
    "plt.scatter(full_accuracy_df_random['k'], full_accuracy_df_random[10000], label='10000')\n",
    "plt.scatter(full_accuracy_df_random['k'], full_accuracy_df_random[75000], label='75000')\n",
    "plt.scatter(full_accuracy_df_random['k'], full_accuracy_df_random[50000], label='50000')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('K Nearest Neighbors vs Accuracy (Random Prototyping)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering\n",
    "\n",
    "def optimized_kmeans(X, k, num_iters=100, tol=1e-4, batch_size=5000, device='cpu'):\n",
    "    X = X.to(device, dtype=torch.float32)  # Ensure correct dtype\n",
    "    N, D = X.shape\n",
    "\n",
    "    # Initialize centroids randomly\n",
    "    indices = torch.randperm(N)[:k]\n",
    "    centroids = X[indices]\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        cluster_assignments = torch.empty(N, dtype=torch.long, device=device)\n",
    "\n",
    "        # Compute distances in batches to save memory\n",
    "        for j in range(0, N, batch_size):\n",
    "            batch = X[j:j+batch_size]\n",
    "            distances = torch.cdist(batch, centroids)  # Compute distance for this batch\n",
    "            cluster_assignments[j:j+batch_size] = torch.argmin(distances, dim=1)  # Assign cluster\n",
    "\n",
    "        # Compute new centroids\n",
    "        new_centroids = torch.zeros_like(centroids)\n",
    "        counts = torch.zeros(k, device=device)\n",
    "\n",
    "        for c in range(k):\n",
    "            cluster_indices = (cluster_assignments == c).nonzero(as_tuple=True)[0]\n",
    "            if cluster_indices.numel() > 0:\n",
    "                new_centroids[c] = X[cluster_indices].mean(dim=0)\n",
    "                counts[c] = cluster_indices.numel()\n",
    "            else:\n",
    "                # Assign the farthest point to avoid empty clusters\n",
    "                farthest_point = X[torch.argmax(torch.cdist(X, centroids[c].unsqueeze(0)), dim=0)]\n",
    "                new_centroids[c] = farthest_point\n",
    "\n",
    "        # Check for convergence\n",
    "        if torch.allclose(new_centroids, centroids, atol=tol):\n",
    "            print(f'Converged at iteration {i}')\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return cluster_assignments, centroids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to assign labels to centroids\n",
    "def assign_labels(cluster_labels, y_true, k):\n",
    "    \"\"\"\n",
    "    Assigns a label to each K-Means cluster using majority voting.\n",
    "    \n",
    "    Args:\n",
    "    - cluster_labels (Tensor): Cluster assignments for each point\n",
    "    - y_true (Tensor): True MNIST labels\n",
    "    - k (int): Number of clusters\n",
    "\n",
    "    Returns:\n",
    "    - cluster_to_label (list): List where index `i` corresponds to cluster `i`'s assigned label\n",
    "    \"\"\"\n",
    "    cluster_to_label = [-1] * k  # Initialize list with -1 for empty clusters\n",
    "\n",
    "    for cluster in range(k):\n",
    "        # Get all true labels for this cluster\n",
    "        cluster_indices = (cluster_labels == cluster).nonzero(as_tuple=True)[0]\n",
    "        true_labels = y_true[cluster_indices]\n",
    "\n",
    "        # Find the most common label in this cluster\n",
    "        if len(true_labels) > 0:\n",
    "            most_common_label = Counter(true_labels.tolist()).most_common(1)[0][0]\n",
    "            cluster_to_label[cluster] = most_common_label\n",
    "\n",
    "    return cluster_to_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 6\n",
      "Converged at iteration 9\n",
      "Converged at iteration 14\n",
      "Converged at iteration 30\n"
     ]
    }
   ],
   "source": [
    "for subset in num_subsets:\n",
    "    if os.path.exists(f\"kmnist_centroids/kmnist_kmeans_centroids_{subset}.pth\"):\n",
    "        continue\n",
    "    else:\n",
    "        cluster_labels, centroids = optimized_kmeans(train_data, subset)\n",
    "        cluster_to_label = assign_labels(cluster_labels, train_labels, subset)\n",
    "        if os.path.exists('kmnist_centroids') == False:\n",
    "            os.mkdir('kmnist_centroids')\n",
    "        torch.save(centroids, f\"kmnist_centroids/kmnist_kmeans_centroids_{subset}.pth\")\n",
    "        torch.save(cluster_to_label, f\"kmnist_centroids/kmnist_kmeans_cluster_labels_{subset}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8392\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8298\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8226\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7893\n"
     ]
    }
   ],
   "source": [
    "accuracy_dict_kmeans = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "    prototype_train_data = torch.load(f\"kmnist_centroids/kmnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"kmnist_centroids/kmnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    accuracy = compute_accuracy(test_data, prototype_train_data, prototype_train_labels)\n",
    "    accuracy_dict_kmeans[subset] = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>125000</th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.839209</td>\n",
       "      <td>0.829844</td>\n",
       "      <td>0.82258</td>\n",
       "      <td>0.78927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     125000    75000    50000    10000 \n",
       "0  0.839209  0.829844  0.82258  0.78927"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df_kmeans = pd.DataFrame(accuracy_dict_kmeans)\n",
    "accuracy_df_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying size K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8392\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8298\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8226\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7893\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8314\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8228\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8119\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7637\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8322\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8188\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8104\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7581\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8263\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8117\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8029\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7458\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8200\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8057\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7963\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7356\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8156\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8008\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7892\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7244\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8093\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7941\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7821\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7160\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8033\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7891\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7753\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7080\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7988\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7840\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7692\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6993\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7921\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7792\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7640\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6914\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7884\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7748\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7594\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6842\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7844\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7718\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7531\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6779\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7816\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7665\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7491\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6722\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7767\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7620\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7445\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6681\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7734\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7580\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7414\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6625\n"
     ]
    }
   ],
   "source": [
    "full_accuracy_df_kmeans = pd.DataFrame()\n",
    "\n",
    "num_neighbors = np.arange(1,30,2)\n",
    "\n",
    "for k in num_neighbors:\n",
    "    accuracy_dict_kmean_per_neigbor = {}\n",
    "    for subset in num_subsets:\n",
    "        prototype_train_data = torch.load(f\"kmnist_centroids/kmnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "        prototype_train_labels = torch.tensor(torch.load(f\"kmnist_centroids/kmnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "        accuracy = compute_accuracy(test_data, prototype_train_data, prototype_train_labels, k)\n",
    "        accuracy_dict_kmean_per_neigbor[subset] = [accuracy]\n",
    "    temp = pd.DataFrame(accuracy_dict_kmean_per_neigbor)\n",
    "    full_accuracy_df_kmeans = pd.concat([full_accuracy_df_kmeans, temp], axis=0)\n",
    "full_accuracy_df_kmeans['k'] = num_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>125000</th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.839209</td>\n",
       "      <td>0.829844</td>\n",
       "      <td>0.822580</td>\n",
       "      <td>0.789270</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.831401</td>\n",
       "      <td>0.822788</td>\n",
       "      <td>0.811866</td>\n",
       "      <td>0.763743</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.832205</td>\n",
       "      <td>0.818819</td>\n",
       "      <td>0.810413</td>\n",
       "      <td>0.758139</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.826264</td>\n",
       "      <td>0.811736</td>\n",
       "      <td>0.802916</td>\n",
       "      <td>0.745843</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.820012</td>\n",
       "      <td>0.805666</td>\n",
       "      <td>0.796275</td>\n",
       "      <td>0.735621</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.815576</td>\n",
       "      <td>0.800841</td>\n",
       "      <td>0.789192</td>\n",
       "      <td>0.724440</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.809350</td>\n",
       "      <td>0.794121</td>\n",
       "      <td>0.782058</td>\n",
       "      <td>0.716035</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.789063</td>\n",
       "      <td>0.775287</td>\n",
       "      <td>0.707967</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.798843</td>\n",
       "      <td>0.783952</td>\n",
       "      <td>0.769243</td>\n",
       "      <td>0.699302</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.792072</td>\n",
       "      <td>0.779205</td>\n",
       "      <td>0.764002</td>\n",
       "      <td>0.691364</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.788414</td>\n",
       "      <td>0.774846</td>\n",
       "      <td>0.759359</td>\n",
       "      <td>0.684178</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.784445</td>\n",
       "      <td>0.771759</td>\n",
       "      <td>0.753055</td>\n",
       "      <td>0.677874</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.781617</td>\n",
       "      <td>0.766493</td>\n",
       "      <td>0.749111</td>\n",
       "      <td>0.672244</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.776740</td>\n",
       "      <td>0.761953</td>\n",
       "      <td>0.744546</td>\n",
       "      <td>0.668068</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.773419</td>\n",
       "      <td>0.758010</td>\n",
       "      <td>0.741407</td>\n",
       "      <td>0.662464</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     125000     75000     50000     10000   k\n",
       "0  0.839209  0.829844  0.822580  0.789270   1\n",
       "0  0.831401  0.822788  0.811866  0.763743   3\n",
       "0  0.832205  0.818819  0.810413  0.758139   5\n",
       "0  0.826264  0.811736  0.802916  0.745843   7\n",
       "0  0.820012  0.805666  0.796275  0.735621   9\n",
       "0  0.815576  0.800841  0.789192  0.724440  11\n",
       "0  0.809350  0.794121  0.782058  0.716035  13\n",
       "0  0.803279  0.789063  0.775287  0.707967  15\n",
       "0  0.798843  0.783952  0.769243  0.699302  17\n",
       "0  0.792072  0.779205  0.764002  0.691364  19\n",
       "0  0.788414  0.774846  0.759359  0.684178  21\n",
       "0  0.784445  0.771759  0.753055  0.677874  23\n",
       "0  0.781617  0.766493  0.749111  0.672244  25\n",
       "0  0.776740  0.761953  0.744546  0.668068  27\n",
       "0  0.773419  0.758010  0.741407  0.662464  29"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_accuracy_df_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for subset in num_subsets:\n",
    "    plt.plot(full_accuracy_df_kmeans['k'], full_accuracy_df_kmeans[subset], label=f'{subset}')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('K Nearest Neighbors vs Accuracy (Base K Means)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)  # Fully connected layer\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)  # No activation, since CrossEntropyLoss applies softmax\n",
    "    \n",
    "    def init_weights(self):\n",
    "        nn.init.zeros_(self.linear.weight)\n",
    "        nn.init.zeros_(self.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True, num_workers=8, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28 * 28\n",
    "num_classes = 49\n",
    "logit_model = LogisticRegression(input_dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_sdg = optim.SGD(logit_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.5014\n",
      "Epoch [2/100], Loss: 1.2947\n",
      "Epoch [3/100], Loss: 1.2648\n",
      "Epoch [4/100], Loss: 1.2498\n",
      "Epoch [5/100], Loss: 1.2403\n",
      "Epoch [6/100], Loss: 1.2325\n",
      "Epoch [7/100], Loss: 1.2274\n",
      "Epoch [8/100], Loss: 1.2237\n",
      "Epoch [9/100], Loss: 1.2195\n",
      "Epoch [10/100], Loss: 1.2168\n",
      "Epoch [11/100], Loss: 1.2134\n",
      "Epoch [12/100], Loss: 1.2112\n",
      "Epoch [13/100], Loss: 1.2092\n",
      "Epoch [14/100], Loss: 1.2070\n",
      "Epoch [15/100], Loss: 1.2065\n",
      "Epoch [16/100], Loss: 1.2045\n",
      "Epoch [17/100], Loss: 1.2028\n",
      "Epoch [18/100], Loss: 1.2019\n",
      "Epoch [19/100], Loss: 1.2012\n",
      "Epoch [20/100], Loss: 1.2012\n",
      "Epoch [21/100], Loss: 1.1982\n",
      "Epoch [22/100], Loss: 1.1980\n",
      "Epoch [23/100], Loss: 1.1967\n",
      "Epoch [24/100], Loss: 1.1967\n",
      "Epoch [25/100], Loss: 1.1953\n",
      "Epoch [26/100], Loss: 1.1949\n",
      "Epoch [27/100], Loss: 1.1934\n",
      "Epoch [28/100], Loss: 1.1937\n",
      "Epoch [29/100], Loss: 1.1920\n",
      "Epoch [30/100], Loss: 1.1922\n",
      "Epoch [31/100], Loss: 1.1922\n",
      "Epoch [32/100], Loss: 1.1923\n",
      "Stopping early at epoch 32 (Loss improvement < 0.0001 for 3 epochs)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-4  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "previous_loss = float('inf')  # Track the best loss\n",
    "epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "loss_list_logit = []\n",
    "for epoch in range(num_epochs):\n",
    "    logit_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = logit_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer_sdg.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer_sdg.step()\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "    current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "    loss_list_logit.append(current_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if previous_loss - current_loss < tolerance:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "            break\n",
    "    else:\n",
    "        epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "    previous_loss = current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Base Logit: 57.02%\n"
     ]
    }
   ],
   "source": [
    "logit_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28)\n",
    "        outputs = logit_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy Base Logit: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipschitz Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.5010\n",
      "Epoch [2/100], Loss: 1.2955\n",
      "Epoch [3/100], Loss: 1.2642\n",
      "Epoch [4/100], Loss: 1.2485\n",
      "Epoch [5/100], Loss: 1.2410\n",
      "Epoch [6/100], Loss: 1.2330\n",
      "Epoch [7/100], Loss: 1.2266\n",
      "Epoch [8/100], Loss: 1.2242\n",
      "Epoch [9/100], Loss: 1.2191\n",
      "Epoch [10/100], Loss: 1.2158\n",
      "Epoch [11/100], Loss: 1.2132\n",
      "Epoch [12/100], Loss: 1.2110\n",
      "Epoch [13/100], Loss: 1.2101\n",
      "Epoch [14/100], Loss: 1.2075\n",
      "Epoch [15/100], Loss: 1.2063\n",
      "Epoch [16/100], Loss: 1.2044\n",
      "Epoch [17/100], Loss: 1.2031\n",
      "Epoch [18/100], Loss: 1.2022\n",
      "Epoch [19/100], Loss: 1.2013\n",
      "Epoch [20/100], Loss: 1.1988\n",
      "Epoch [21/100], Loss: 1.1996\n",
      "Epoch [22/100], Loss: 1.1987\n",
      "Epoch [23/100], Loss: 1.1983\n",
      "Epoch [24/100], Loss: 1.1963\n",
      "Epoch [25/100], Loss: 1.1960\n",
      "Epoch [26/100], Loss: 1.1945\n",
      "Epoch [27/100], Loss: 1.1948\n",
      "Epoch [28/100], Loss: 1.1934\n",
      "Epoch [29/100], Loss: 1.1930\n",
      "Epoch [30/100], Loss: 1.1923\n",
      "Epoch [31/100], Loss: 1.1931\n",
      "Epoch [32/100], Loss: 1.1916\n",
      "Epoch [33/100], Loss: 1.1910\n",
      "Epoch [34/100], Loss: 1.1895\n",
      "Epoch [35/100], Loss: 1.1901\n",
      "Epoch [36/100], Loss: 1.1896\n",
      "Epoch [37/100], Loss: 1.1890\n",
      "Epoch [38/100], Loss: 1.1882\n",
      "Epoch [39/100], Loss: 1.1882\n",
      "Epoch [40/100], Loss: 1.1883\n",
      "Epoch [41/100], Loss: 1.1875\n",
      "Epoch [42/100], Loss: 1.1871\n",
      "Epoch [43/100], Loss: 1.1871\n",
      "Epoch [44/100], Loss: 1.1864\n",
      "Epoch [45/100], Loss: 1.1865\n",
      "Epoch [46/100], Loss: 1.1856\n",
      "Epoch [47/100], Loss: 1.1857\n",
      "Epoch [48/100], Loss: 1.1855\n",
      "Epoch [49/100], Loss: 1.1848\n",
      "Epoch [50/100], Loss: 1.1854\n",
      "Epoch [51/100], Loss: 1.1854\n",
      "Epoch [52/100], Loss: 1.1844\n",
      "Epoch [53/100], Loss: 1.1842\n",
      "Epoch [54/100], Loss: 1.1836\n",
      "Epoch [55/100], Loss: 1.1837\n",
      "Epoch [56/100], Loss: 1.1837\n",
      "Epoch [57/100], Loss: 1.1831\n",
      "Epoch [58/100], Loss: 1.1833\n",
      "Epoch [59/100], Loss: 1.1845\n",
      "Epoch [60/100], Loss: 1.1832\n",
      "Epoch [61/100], Loss: 1.1841\n",
      "Epoch [62/100], Loss: 1.1828\n",
      "Epoch [63/100], Loss: 1.1826\n",
      "Epoch [64/100], Loss: 1.1826\n",
      "Epoch [65/100], Loss: 1.1820\n",
      "Epoch [66/100], Loss: 1.1817\n",
      "Epoch [67/100], Loss: 1.1814\n",
      "Epoch [68/100], Loss: 1.1813\n",
      "Epoch [69/100], Loss: 1.1808\n",
      "Epoch [70/100], Loss: 1.1808\n",
      "Epoch [71/100], Loss: 1.1816\n",
      "Epoch [72/100], Loss: 1.1805\n",
      "Epoch [73/100], Loss: 1.1814\n",
      "Epoch [74/100], Loss: 1.1802\n",
      "Epoch [75/100], Loss: 1.1809\n",
      "Epoch [76/100], Loss: 1.1806\n",
      "Epoch [77/100], Loss: 1.1801\n",
      "Epoch [78/100], Loss: 1.1796\n",
      "Epoch [79/100], Loss: 1.1806\n",
      "Epoch [80/100], Loss: 1.1799\n",
      "Epoch [81/100], Loss: 1.1798\n",
      "Epoch [82/100], Loss: 1.1794\n",
      "Epoch [83/100], Loss: 1.1798\n",
      "Epoch [84/100], Loss: 1.1800\n",
      "Epoch [85/100], Loss: 1.1792\n",
      "Epoch [86/100], Loss: 1.1789\n",
      "Epoch [87/100], Loss: 1.1792\n",
      "Epoch [88/100], Loss: 1.1797\n",
      "Epoch [89/100], Loss: 1.1791\n",
      "Epoch [90/100], Loss: 1.1795\n",
      "Epoch [91/100], Loss: 1.1795\n",
      "Epoch [92/100], Loss: 1.1784\n",
      "Epoch [93/100], Loss: 1.1787\n",
      "Epoch [94/100], Loss: 1.1793\n",
      "Epoch [95/100], Loss: 1.1787\n",
      "Epoch [96/100], Loss: 1.1781\n",
      "Epoch [97/100], Loss: 1.1785\n",
      "Epoch [98/100], Loss: 1.1779\n",
      "Epoch [99/100], Loss: 1.1782\n",
      "Epoch [100/100], Loss: 1.1785\n"
     ]
    }
   ],
   "source": [
    "# Define Adam Optimizer\n",
    "logit_model_lipschitz = LogisticRegression(input_dim, num_classes)\n",
    "optimizer_sdg = optim.SGD(logit_model_lipschitz.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Early stopping parameters\n",
    "tolerance = 1e-4  # Minimum change in loss to continue training\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "previous_loss = float('inf')\n",
    "epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 100  # Maximum epochs\n",
    "loss_list_logit_lip = []\n",
    "for epoch in range(num_epochs):\n",
    "    logit_model_lipschitz.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(-1, 28 * 28)  # Flatten images\n",
    "\n",
    "        outputs = logit_model_lipschitz(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        logit_model_lipschitz.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        L = max(param.grad.abs().max().item() for param in logit_model_lipschitz.parameters() if param.grad is not None)\n",
    "        adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "        # Update optimizer's learning rate\n",
    "        for param_group in optimizer_sdg.param_groups:\n",
    "            param_group['lr'] = adaptive_lr\n",
    "\n",
    "        optimizer_sdg.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    current_loss = total_loss / len(train_loader)\n",
    "    loss_list_logit_lip.append(current_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if previous_loss - current_loss < tolerance:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "            break\n",
    "    else:\n",
    "        epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "    previous_loss = current_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy Newtonian Logit: 0.5461\n"
     ]
    }
   ],
   "source": [
    "logit_model_lipschitz.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(-1, 28*28)\n",
    "        \n",
    "        outputs = logit_model_lipschitz(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "        all_preds.extend(predicted.cpu().numpy())  # Collect predictions\n",
    "        all_labels.extend(labels.cpu().numpy())  # Collect true labels\n",
    "\n",
    "balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "print(f\"Balanced Accuracy Newtonian Logit: {balanced_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMNIST_CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(KMNIST_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 256)  # Corrected input size\n",
    "        self.fc2 = nn.Linear(256, num_classes)  # Adjust output layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, start_dim=1)  # Dynamically flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = KMNIST_CNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_sdg = optim.SGD(cnn_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(train_set))\n",
    "val_size = len(train_set) - train_size\n",
    "train_set_small, val_set = random_split(train_set, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_cnn = DataLoader(\n",
    "    train_set_small, batch_size=256, shuffle=True, \n",
    "    num_workers=8, pin_memory=True\n",
    ")\n",
    "val_loader_cnn = DataLoader(\n",
    "    val_set, batch_size=256, shuffle=False, \n",
    "    num_workers=8, pin_memory=True\n",
    ")\n",
    "test_loader_cnn = DataLoader(\n",
    "    test_set, batch_size=256, shuffle=False, \n",
    "    num_workers=8, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 1.1611 Val Loss: 0.4539\n",
      "Epoch [2/50], Train Loss: 0.3388 Val Loss: 0.2831\n",
      "Epoch [3/50], Train Loss: 0.2280 Val Loss: 0.2317\n",
      "Epoch [4/50], Train Loss: 0.1783 Val Loss: 0.2092\n",
      "Epoch [5/50], Train Loss: 0.1468 Val Loss: 0.1903\n",
      "Epoch [6/50], Train Loss: 0.1240 Val Loss: 0.1874\n",
      "Epoch [7/50], Train Loss: 0.1055 Val Loss: 0.1773\n",
      "Epoch [8/50], Train Loss: 0.0909 Val Loss: 0.1799\n",
      "Epoch [9/50], Train Loss: 0.0787 Val Loss: 0.1757\n",
      "Epoch [10/50], Train Loss: 0.0679 Val Loss: 0.1683\n",
      "Epoch [11/50], Train Loss: 0.0579 Val Loss: 0.1778\n",
      "Epoch [12/50], Train Loss: 0.0508 Val Loss: 0.1768\n",
      "Epoch [13/50], Train Loss: 0.0436 Val Loss: 0.1819\n",
      "Epoch [14/50], Train Loss: 0.0379 Val Loss: 0.1827\n",
      "Epoch [15/50], Train Loss: 0.0325 Val Loss: 0.1905\n",
      "Stopping early at epoch 15 (No improvement in 5 epochs)\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "loss_list_cnn = []\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "\n",
    "    for images, labels in train_loader_cnn:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = cnn_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer_sdg.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer_sdg.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader_cnn)\n",
    "\n",
    "    # Validation loop\n",
    "    cnn_model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_cnn:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = cnn_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "\n",
    "    val_loss /= len(val_loader_cnn)\n",
    "    loss_list_cnn.append(train_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy: 0.9115\n"
     ]
    }
   ],
   "source": [
    "cnn_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_cnn:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = cnn_model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        all_preds.extend(predicted.cpu().numpy())  # Collect predictions\n",
    "        all_labels.extend(labels.cpu().numpy())  # Collect true labels\n",
    "\n",
    "balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "print(f\"Balanced Accuracy: {balanced_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Lipschitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_lipschitz = KMNIST_CNN(num_classes).to(device)\n",
    "optimizer_sdg = optim.SGD(cnn_model_lipschitz.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.5578 Val Loss: 0.2258\n",
      "Epoch [2/50], Train Loss: 0.1738 Val Loss: 0.1809\n",
      "Epoch [3/50], Train Loss: 0.1181 Val Loss: 0.1806\n",
      "Epoch [4/50], Train Loss: 0.0907 Val Loss: 0.1683\n",
      "Epoch [5/50], Train Loss: 0.0697 Val Loss: 0.1687\n",
      "Epoch [6/50], Train Loss: 0.0591 Val Loss: 0.1761\n",
      "Epoch [7/50], Train Loss: 0.0497 Val Loss: 0.1801\n",
      "Epoch [8/50], Train Loss: 0.0429 Val Loss: 0.1735\n",
      "Epoch [9/50], Train Loss: 0.0406 Val Loss: 0.1883\n",
      "Stopping early at epoch 9 (No improvement in 5 epochs)\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "loss_list_cnn_lip = []\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_model_lipschitz.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader_cnn:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = cnn_model_lipschitz(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "    \n",
    "        # Backward pass\n",
    "        optimizer_sdg.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "\n",
    "        L = max(param.grad.abs().max().item() for param in cnn_model_lipschitz.parameters() if param.grad is not None)\n",
    "        adaptive_lr = min(0.1, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "        # Update optimizer's learning rate\n",
    "        for param_group in optimizer_sdg.param_groups:\n",
    "            param_group['lr'] = adaptive_lr\n",
    "\n",
    "        optimizer_sdg.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader_cnn)\n",
    "\n",
    "    # Validation loop\n",
    "    cnn_model_lipschitz.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_cnn:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = cnn_model_lipschitz(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader_cnn)\n",
    "    loss_list_cnn_lip.append(train_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Newtonian Logit: 92.05%\n"
     ]
    }
   ],
   "source": [
    "cnn_model_lipschitz.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_cnn:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = cnn_model_lipschitz(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy Newtonian Logit: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict = {\n",
    "    'cnn': loss_list_cnn,\n",
    "    'cnn_lip': loss_list_cnn_lip\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model in loss_dict:\n",
    "    plt.plot(np.arange(len(loss_dict[model])), loss_dict[model], label=model)\n",
    "\n",
    "plt.title('Epoch vs Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Prototypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with subset size: 125000\n",
      "Epoch [1/100], Loss: 1.5203\n",
      "Epoch [2/100], Loss: 1.3111\n",
      "Epoch [3/100], Loss: 1.2788\n",
      "Epoch [4/100], Loss: 1.2608\n",
      "Epoch [5/100], Loss: 1.2493\n",
      "Epoch [6/100], Loss: 1.2411\n",
      "Epoch [7/100], Loss: 1.2340\n",
      "Epoch [8/100], Loss: 1.2264\n",
      "Epoch [9/100], Loss: 1.2222\n",
      "Epoch [10/100], Loss: 1.2181\n",
      "Epoch [11/100], Loss: 1.2135\n",
      "Epoch [12/100], Loss: 1.2113\n",
      "Epoch [13/100], Loss: 1.2082\n",
      "Epoch [14/100], Loss: 1.2052\n",
      "Epoch [15/100], Loss: 1.2032\n",
      "Epoch [16/100], Loss: 1.2021\n",
      "Epoch [17/100], Loss: 1.2004\n",
      "Epoch [18/100], Loss: 1.1974\n",
      "Epoch [19/100], Loss: 1.1960\n",
      "Epoch [20/100], Loss: 1.1940\n",
      "Epoch [21/100], Loss: 1.1932\n",
      "Epoch [22/100], Loss: 1.1908\n",
      "Epoch [23/100], Loss: 1.1901\n",
      "Epoch [24/100], Loss: 1.1877\n",
      "Epoch [25/100], Loss: 1.1875\n",
      "Epoch [26/100], Loss: 1.1863\n",
      "Epoch [27/100], Loss: 1.1854\n",
      "Epoch [28/100], Loss: 1.1828\n",
      "Epoch [29/100], Loss: 1.1833\n",
      "Epoch [30/100], Loss: 1.1828\n",
      "Epoch [31/100], Loss: 1.1825\n",
      "Stopping early at epoch 31 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 125000, Epoch [31/100], Loss: 1.1825\n",
      "Test Accuracy Base Logit: 56.82%\n",
      "Training with subset size: 125000\n",
      "Epoch [1/100], Loss: 1.5174\n",
      "Epoch [2/100], Loss: 1.3098\n",
      "Epoch [3/100], Loss: 1.2778\n",
      "Epoch [4/100], Loss: 1.2609\n",
      "Epoch [5/100], Loss: 1.2493\n",
      "Epoch [6/100], Loss: 1.2410\n",
      "Epoch [7/100], Loss: 1.2330\n",
      "Epoch [8/100], Loss: 1.2276\n",
      "Epoch [9/100], Loss: 1.2227\n",
      "Epoch [10/100], Loss: 1.2173\n",
      "Epoch [11/100], Loss: 1.2150\n",
      "Epoch [12/100], Loss: 1.2121\n",
      "Epoch [13/100], Loss: 1.2065\n",
      "Epoch [14/100], Loss: 1.2062\n",
      "Epoch [15/100], Loss: 1.2047\n",
      "Epoch [16/100], Loss: 1.2013\n",
      "Epoch [17/100], Loss: 1.1996\n",
      "Epoch [18/100], Loss: 1.1978\n",
      "Epoch [19/100], Loss: 1.1975\n",
      "Epoch [20/100], Loss: 1.1957\n",
      "Epoch [21/100], Loss: 1.1931\n",
      "Epoch [22/100], Loss: 1.1921\n",
      "Epoch [23/100], Loss: 1.1909\n",
      "Epoch [24/100], Loss: 1.1891\n",
      "Epoch [25/100], Loss: 1.1871\n",
      "Epoch [26/100], Loss: 1.1875\n",
      "Epoch [27/100], Loss: 1.1860\n",
      "Epoch [28/100], Loss: 1.1835\n",
      "Epoch [29/100], Loss: 1.1825\n",
      "Epoch [30/100], Loss: 1.1823\n",
      "Epoch [31/100], Loss: 1.1813\n",
      "Epoch [32/100], Loss: 1.1812\n",
      "Stopping early at epoch 32 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 125000, Epoch [32/100], Loss: 1.1812\n",
      "Test Accuracy Base Logit: 55.56%\n",
      "Training with subset size: 125000\n",
      "Epoch [1/100], Loss: 1.5199\n",
      "Epoch [2/100], Loss: 1.3134\n",
      "Epoch [3/100], Loss: 1.2768\n",
      "Epoch [4/100], Loss: 1.2617\n",
      "Epoch [5/100], Loss: 1.2490\n",
      "Epoch [6/100], Loss: 1.2404\n",
      "Epoch [7/100], Loss: 1.2335\n",
      "Epoch [8/100], Loss: 1.2274\n",
      "Epoch [9/100], Loss: 1.2228\n",
      "Epoch [10/100], Loss: 1.2187\n",
      "Epoch [11/100], Loss: 1.2148\n",
      "Epoch [12/100], Loss: 1.2095\n",
      "Epoch [13/100], Loss: 1.2088\n",
      "Epoch [14/100], Loss: 1.2036\n",
      "Epoch [15/100], Loss: 1.2037\n",
      "Epoch [16/100], Loss: 1.2018\n",
      "Epoch [17/100], Loss: 1.1992\n",
      "Epoch [18/100], Loss: 1.1968\n",
      "Epoch [19/100], Loss: 1.1954\n",
      "Epoch [20/100], Loss: 1.1931\n",
      "Epoch [21/100], Loss: 1.1921\n",
      "Epoch [22/100], Loss: 1.1908\n",
      "Epoch [23/100], Loss: 1.1893\n",
      "Epoch [24/100], Loss: 1.1896\n",
      "Epoch [25/100], Loss: 1.1876\n",
      "Epoch [26/100], Loss: 1.1864\n",
      "Epoch [27/100], Loss: 1.1861\n",
      "Epoch [28/100], Loss: 1.1852\n",
      "Epoch [29/100], Loss: 1.1841\n",
      "Epoch [30/100], Loss: 1.1830\n",
      "Epoch [31/100], Loss: 1.1824\n",
      "Epoch [32/100], Loss: 1.1811\n",
      "Epoch [33/100], Loss: 1.1790\n",
      "Epoch [34/100], Loss: 1.1808\n",
      "Epoch [35/100], Loss: 1.1788\n",
      "Epoch [36/100], Loss: 1.1767\n",
      "Epoch [37/100], Loss: 1.1762\n",
      "Epoch [38/100], Loss: 1.1761\n",
      "Epoch [39/100], Loss: 1.1745\n",
      "Epoch [40/100], Loss: 1.1757\n",
      "Epoch [41/100], Loss: 1.1739\n",
      "Epoch [42/100], Loss: 1.1733\n",
      "Epoch [43/100], Loss: 1.1733\n",
      "Epoch [44/100], Loss: 1.1731\n",
      "Stopping early at epoch 44 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 125000, Epoch [44/100], Loss: 1.1731\n",
      "Test Accuracy Base Logit: 55.93%\n",
      "Training with subset size: 125000\n",
      "Epoch [1/100], Loss: 1.5244\n",
      "Epoch [2/100], Loss: 1.3138\n",
      "Epoch [3/100], Loss: 1.2816\n",
      "Epoch [4/100], Loss: 1.2653\n",
      "Epoch [5/100], Loss: 1.2503\n",
      "Epoch [6/100], Loss: 1.2432\n",
      "Epoch [7/100], Loss: 1.2362\n",
      "Epoch [8/100], Loss: 1.2295\n",
      "Epoch [9/100], Loss: 1.2248\n",
      "Epoch [10/100], Loss: 1.2211\n",
      "Epoch [11/100], Loss: 1.2163\n",
      "Epoch [12/100], Loss: 1.2143\n",
      "Epoch [13/100], Loss: 1.2117\n",
      "Epoch [14/100], Loss: 1.2085\n",
      "Epoch [15/100], Loss: 1.2066\n",
      "Epoch [16/100], Loss: 1.2026\n",
      "Epoch [17/100], Loss: 1.2022\n",
      "Epoch [18/100], Loss: 1.1991\n",
      "Epoch [19/100], Loss: 1.1965\n",
      "Epoch [20/100], Loss: 1.1967\n",
      "Epoch [21/100], Loss: 1.1947\n",
      "Epoch [22/100], Loss: 1.1933\n",
      "Epoch [23/100], Loss: 1.1930\n",
      "Epoch [24/100], Loss: 1.1921\n",
      "Epoch [25/100], Loss: 1.1896\n",
      "Epoch [26/100], Loss: 1.1877\n",
      "Epoch [27/100], Loss: 1.1876\n",
      "Epoch [28/100], Loss: 1.1859\n",
      "Epoch [29/100], Loss: 1.1867\n",
      "Epoch [30/100], Loss: 1.1855\n",
      "Epoch [31/100], Loss: 1.1860\n",
      "Epoch [32/100], Loss: 1.1820\n",
      "Epoch [33/100], Loss: 1.1820\n",
      "Epoch [34/100], Loss: 1.1804\n",
      "Epoch [35/100], Loss: 1.1812\n",
      "Epoch [36/100], Loss: 1.1796\n",
      "Epoch [37/100], Loss: 1.1818\n",
      "Epoch [38/100], Loss: 1.1763\n",
      "Epoch [39/100], Loss: 1.1789\n",
      "Epoch [40/100], Loss: 1.1768\n",
      "Epoch [41/100], Loss: 1.1771\n",
      "Epoch [42/100], Loss: 1.1747\n",
      "Epoch [43/100], Loss: 1.1765\n",
      "Epoch [44/100], Loss: 1.1767\n",
      "Epoch [45/100], Loss: 1.1738\n",
      "Epoch [46/100], Loss: 1.1734\n",
      "Epoch [47/100], Loss: 1.1723\n",
      "Epoch [48/100], Loss: 1.1718\n",
      "Epoch [49/100], Loss: 1.1734\n",
      "Epoch [50/100], Loss: 1.1733\n",
      "Stopping early at epoch 50 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 125000, Epoch [50/100], Loss: 1.1733\n",
      "Test Accuracy Base Logit: 55.71%\n",
      "Training with subset size: 125000\n",
      "Epoch [1/100], Loss: 1.5184\n",
      "Epoch [2/100], Loss: 1.3092\n",
      "Epoch [3/100], Loss: 1.2760\n",
      "Epoch [4/100], Loss: 1.2596\n",
      "Epoch [5/100], Loss: 1.2472\n",
      "Epoch [6/100], Loss: 1.2381\n",
      "Epoch [7/100], Loss: 1.2330\n",
      "Epoch [8/100], Loss: 1.2277\n",
      "Epoch [9/100], Loss: 1.2197\n",
      "Epoch [10/100], Loss: 1.2183\n",
      "Epoch [11/100], Loss: 1.2135\n",
      "Epoch [12/100], Loss: 1.2085\n",
      "Epoch [13/100], Loss: 1.2055\n",
      "Epoch [14/100], Loss: 1.2043\n",
      "Epoch [15/100], Loss: 1.2034\n",
      "Epoch [16/100], Loss: 1.2001\n",
      "Epoch [17/100], Loss: 1.1990\n",
      "Epoch [18/100], Loss: 1.1979\n",
      "Epoch [19/100], Loss: 1.1962\n",
      "Epoch [20/100], Loss: 1.1938\n",
      "Epoch [21/100], Loss: 1.1906\n",
      "Epoch [22/100], Loss: 1.1909\n",
      "Epoch [23/100], Loss: 1.1902\n",
      "Epoch [24/100], Loss: 1.1888\n",
      "Epoch [25/100], Loss: 1.1895\n",
      "Epoch [26/100], Loss: 1.1864\n",
      "Epoch [27/100], Loss: 1.1839\n",
      "Epoch [28/100], Loss: 1.1845\n",
      "Epoch [29/100], Loss: 1.1825\n",
      "Epoch [30/100], Loss: 1.1821\n",
      "Epoch [31/100], Loss: 1.1825\n",
      "Epoch [32/100], Loss: 1.1814\n",
      "Epoch [33/100], Loss: 1.1812\n",
      "Epoch [34/100], Loss: 1.1785\n",
      "Epoch [35/100], Loss: 1.1780\n",
      "Epoch [36/100], Loss: 1.1759\n",
      "Epoch [37/100], Loss: 1.1760\n",
      "Epoch [38/100], Loss: 1.1765\n",
      "Epoch [39/100], Loss: 1.1752\n",
      "Epoch [40/100], Loss: 1.1753\n",
      "Epoch [41/100], Loss: 1.1745\n",
      "Epoch [42/100], Loss: 1.1750\n",
      "Stopping early at epoch 42 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 125000, Epoch [42/100], Loss: 1.1750\n",
      "Test Accuracy Base Logit: 55.38%\n",
      "Training with subset size: 125000\n",
      "Epoch [1/100], Loss: 1.5180\n",
      "Epoch [2/100], Loss: 1.3117\n",
      "Epoch [3/100], Loss: 1.2786\n",
      "Epoch [4/100], Loss: 1.2612\n",
      "Epoch [5/100], Loss: 1.2492\n",
      "Epoch [6/100], Loss: 1.2408\n",
      "Epoch [7/100], Loss: 1.2347\n",
      "Epoch [8/100], Loss: 1.2276\n",
      "Epoch [9/100], Loss: 1.2239\n",
      "Epoch [10/100], Loss: 1.2196\n",
      "Epoch [11/100], Loss: 1.2151\n",
      "Epoch [12/100], Loss: 1.2105\n",
      "Epoch [13/100], Loss: 1.2087\n",
      "Epoch [14/100], Loss: 1.2068\n",
      "Epoch [15/100], Loss: 1.2040\n",
      "Epoch [16/100], Loss: 1.2030\n",
      "Epoch [17/100], Loss: 1.2007\n",
      "Epoch [18/100], Loss: 1.1984\n",
      "Epoch [19/100], Loss: 1.1963\n",
      "Epoch [20/100], Loss: 1.1941\n",
      "Epoch [21/100], Loss: 1.1934\n",
      "Epoch [22/100], Loss: 1.1888\n",
      "Epoch [23/100], Loss: 1.1899\n",
      "Epoch [24/100], Loss: 1.1896\n",
      "Epoch [25/100], Loss: 1.1899\n",
      "Stopping early at epoch 25 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 125000, Epoch [25/100], Loss: 1.1899\n",
      "Test Accuracy Base Logit: 55.90%\n",
      "Training with subset size: 125000\n",
      "Epoch [1/100], Loss: 1.5184\n",
      "Epoch [2/100], Loss: 1.3078\n",
      "Epoch [3/100], Loss: 1.2768\n",
      "Epoch [4/100], Loss: 1.2590\n",
      "Epoch [5/100], Loss: 1.2470\n",
      "Epoch [6/100], Loss: 1.2378\n",
      "Epoch [7/100], Loss: 1.2300\n",
      "Epoch [8/100], Loss: 1.2257\n",
      "Epoch [9/100], Loss: 1.2192\n",
      "Epoch [10/100], Loss: 1.2176\n",
      "Epoch [11/100], Loss: 1.2145\n",
      "Epoch [12/100], Loss: 1.2106\n",
      "Epoch [13/100], Loss: 1.2065\n",
      "Epoch [14/100], Loss: 1.2062\n",
      "Epoch [15/100], Loss: 1.2019\n",
      "Epoch [16/100], Loss: 1.1988\n",
      "Epoch [17/100], Loss: 1.1964\n",
      "Epoch [18/100], Loss: 1.1946\n",
      "Epoch [19/100], Loss: 1.1946\n",
      "Epoch [20/100], Loss: 1.1907\n",
      "Epoch [21/100], Loss: 1.1899\n",
      "Epoch [22/100], Loss: 1.1905\n",
      "Epoch [23/100], Loss: 1.1868\n",
      "Epoch [24/100], Loss: 1.1841\n",
      "Epoch [25/100], Loss: 1.1859\n",
      "Epoch [26/100], Loss: 1.1839\n",
      "Epoch [27/100], Loss: 1.1839\n",
      "Epoch [28/100], Loss: 1.1823\n",
      "Epoch [29/100], Loss: 1.1809\n",
      "Epoch [30/100], Loss: 1.1808\n",
      "Epoch [31/100], Loss: 1.1802\n",
      "Epoch [32/100], Loss: 1.1796\n",
      "Stopping early at epoch 32 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 125000, Epoch [32/100], Loss: 1.1796\n",
      "Test Accuracy Base Logit: 55.57%\n",
      "Training with subset size: 125000\n",
      "Epoch [1/100], Loss: 1.5227\n",
      "Epoch [2/100], Loss: 1.3148\n",
      "Epoch [3/100], Loss: 1.2810\n",
      "Epoch [4/100], Loss: 1.2646\n",
      "Epoch [5/100], Loss: 1.2518\n",
      "Epoch [6/100], Loss: 1.2444\n",
      "Epoch [7/100], Loss: 1.2370\n",
      "Epoch [8/100], Loss: 1.2312\n",
      "Epoch [9/100], Loss: 1.2264\n",
      "Epoch [10/100], Loss: 1.2228\n",
      "Epoch [11/100], Loss: 1.2190\n",
      "Epoch [12/100], Loss: 1.2143\n",
      "Epoch [13/100], Loss: 1.2129\n",
      "Epoch [14/100], Loss: 1.2096\n",
      "Epoch [15/100], Loss: 1.2073\n",
      "Epoch [16/100], Loss: 1.2031\n",
      "Epoch [17/100], Loss: 1.2040\n",
      "Epoch [18/100], Loss: 1.2012\n",
      "Epoch [19/100], Loss: 1.1995\n",
      "Epoch [20/100], Loss: 1.1984\n",
      "Epoch [21/100], Loss: 1.1961\n",
      "Epoch [22/100], Loss: 1.1966\n",
      "Epoch [23/100], Loss: 1.1942\n",
      "Epoch [24/100], Loss: 1.1937\n",
      "Epoch [25/100], Loss: 1.1905\n",
      "Epoch [26/100], Loss: 1.1902\n",
      "Epoch [27/100], Loss: 1.1900\n",
      "Epoch [28/100], Loss: 1.1888\n",
      "Epoch [29/100], Loss: 1.1863\n",
      "Epoch [30/100], Loss: 1.1855\n",
      "Epoch [31/100], Loss: 1.1859\n",
      "Epoch [32/100], Loss: 1.1854\n",
      "Stopping early at epoch 32 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 125000, Epoch [32/100], Loss: 1.1854\n",
      "Test Accuracy Base Logit: 55.69%\n",
      "Training with subset size: 125000\n",
      "Epoch [1/100], Loss: 1.5217\n",
      "Epoch [2/100], Loss: 1.3118\n",
      "Epoch [3/100], Loss: 1.2790\n",
      "Epoch [4/100], Loss: 1.2618\n",
      "Epoch [5/100], Loss: 1.2470\n",
      "Epoch [6/100], Loss: 1.2409\n",
      "Epoch [7/100], Loss: 1.2326\n",
      "Epoch [8/100], Loss: 1.2281\n",
      "Epoch [9/100], Loss: 1.2235\n",
      "Epoch [10/100], Loss: 1.2188\n",
      "Epoch [11/100], Loss: 1.2145\n",
      "Epoch [12/100], Loss: 1.2132\n",
      "Epoch [13/100], Loss: 1.2085\n",
      "Epoch [14/100], Loss: 1.2064\n",
      "Epoch [15/100], Loss: 1.2050\n",
      "Epoch [16/100], Loss: 1.1998\n",
      "Epoch [17/100], Loss: 1.2007\n",
      "Epoch [18/100], Loss: 1.1999\n",
      "Epoch [19/100], Loss: 1.1937\n",
      "Epoch [20/100], Loss: 1.1938\n",
      "Epoch [21/100], Loss: 1.1931\n",
      "Epoch [22/100], Loss: 1.1926\n",
      "Stopping early at epoch 22 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 125000, Epoch [22/100], Loss: 1.1926\n",
      "Test Accuracy Base Logit: 56.18%\n",
      "Training with subset size: 125000\n",
      "Epoch [1/100], Loss: 1.5260\n",
      "Epoch [2/100], Loss: 1.3147\n",
      "Epoch [3/100], Loss: 1.2824\n",
      "Epoch [4/100], Loss: 1.2642\n",
      "Epoch [5/100], Loss: 1.2529\n",
      "Epoch [6/100], Loss: 1.2466\n",
      "Epoch [7/100], Loss: 1.2386\n",
      "Epoch [8/100], Loss: 1.2308\n",
      "Epoch [9/100], Loss: 1.2251\n",
      "Epoch [10/100], Loss: 1.2214\n",
      "Epoch [11/100], Loss: 1.2201\n",
      "Epoch [12/100], Loss: 1.2148\n",
      "Epoch [13/100], Loss: 1.2121\n",
      "Epoch [14/100], Loss: 1.2098\n",
      "Epoch [15/100], Loss: 1.2077\n",
      "Epoch [16/100], Loss: 1.2059\n",
      "Epoch [17/100], Loss: 1.2050\n",
      "Epoch [18/100], Loss: 1.2025\n",
      "Epoch [19/100], Loss: 1.2017\n",
      "Epoch [20/100], Loss: 1.1976\n",
      "Epoch [21/100], Loss: 1.1972\n",
      "Epoch [22/100], Loss: 1.1968\n",
      "Epoch [23/100], Loss: 1.1945\n",
      "Epoch [24/100], Loss: 1.1920\n",
      "Epoch [25/100], Loss: 1.1940\n",
      "Epoch [26/100], Loss: 1.1912\n",
      "Epoch [27/100], Loss: 1.1890\n",
      "Epoch [28/100], Loss: 1.1892\n",
      "Epoch [29/100], Loss: 1.1882\n",
      "Epoch [30/100], Loss: 1.1886\n",
      "Epoch [31/100], Loss: 1.1855\n",
      "Epoch [32/100], Loss: 1.1864\n",
      "Epoch [33/100], Loss: 1.1864\n",
      "Epoch [34/100], Loss: 1.1837\n",
      "Epoch [35/100], Loss: 1.1839\n",
      "Epoch [36/100], Loss: 1.1834\n",
      "Epoch [37/100], Loss: 1.1842\n",
      "Stopping early at epoch 37 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 125000, Epoch [37/100], Loss: 1.1842\n",
      "Test Accuracy Base Logit: 56.13%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.6248\n",
      "Epoch [2/100], Loss: 1.3399\n",
      "Epoch [3/100], Loss: 1.2935\n",
      "Epoch [4/100], Loss: 1.2713\n",
      "Epoch [5/100], Loss: 1.2534\n",
      "Epoch [6/100], Loss: 1.2387\n",
      "Epoch [7/100], Loss: 1.2309\n",
      "Epoch [8/100], Loss: 1.2208\n",
      "Epoch [9/100], Loss: 1.2160\n",
      "Epoch [10/100], Loss: 1.2073\n",
      "Epoch [11/100], Loss: 1.2014\n",
      "Epoch [12/100], Loss: 1.1993\n",
      "Epoch [13/100], Loss: 1.1924\n",
      "Epoch [14/100], Loss: 1.1890\n",
      "Epoch [15/100], Loss: 1.1846\n",
      "Epoch [16/100], Loss: 1.1817\n",
      "Epoch [17/100], Loss: 1.1771\n",
      "Epoch [18/100], Loss: 1.1736\n",
      "Epoch [19/100], Loss: 1.1734\n",
      "Epoch [20/100], Loss: 1.1704\n",
      "Epoch [21/100], Loss: 1.1666\n",
      "Epoch [22/100], Loss: 1.1649\n",
      "Epoch [23/100], Loss: 1.1622\n",
      "Epoch [24/100], Loss: 1.1604\n",
      "Epoch [25/100], Loss: 1.1579\n",
      "Epoch [26/100], Loss: 1.1570\n",
      "Epoch [27/100], Loss: 1.1536\n",
      "Epoch [28/100], Loss: 1.1532\n",
      "Epoch [29/100], Loss: 1.1514\n",
      "Epoch [30/100], Loss: 1.1480\n",
      "Epoch [31/100], Loss: 1.1481\n",
      "Epoch [32/100], Loss: 1.1464\n",
      "Epoch [33/100], Loss: 1.1463\n",
      "Epoch [34/100], Loss: 1.1438\n",
      "Epoch [35/100], Loss: 1.1416\n",
      "Epoch [36/100], Loss: 1.1420\n",
      "Epoch [37/100], Loss: 1.1395\n",
      "Epoch [38/100], Loss: 1.1370\n",
      "Epoch [39/100], Loss: 1.1369\n",
      "Epoch [40/100], Loss: 1.1379\n",
      "Epoch [41/100], Loss: 1.1350\n",
      "Epoch [42/100], Loss: 1.1323\n",
      "Epoch [43/100], Loss: 1.1338\n",
      "Epoch [44/100], Loss: 1.1334\n",
      "Epoch [45/100], Loss: 1.1314\n",
      "Epoch [46/100], Loss: 1.1287\n",
      "Epoch [47/100], Loss: 1.1295\n",
      "Epoch [48/100], Loss: 1.1297\n",
      "Epoch [49/100], Loss: 1.1283\n",
      "Epoch [50/100], Loss: 1.1260\n",
      "Epoch [51/100], Loss: 1.1270\n",
      "Epoch [52/100], Loss: 1.1257\n",
      "Epoch [53/100], Loss: 1.1245\n",
      "Epoch [54/100], Loss: 1.1261\n",
      "Epoch [55/100], Loss: 1.1227\n",
      "Epoch [56/100], Loss: 1.1233\n",
      "Epoch [57/100], Loss: 1.1226\n",
      "Epoch [58/100], Loss: 1.1201\n",
      "Epoch [59/100], Loss: 1.1202\n",
      "Epoch [60/100], Loss: 1.1190\n",
      "Epoch [61/100], Loss: 1.1198\n",
      "Epoch [62/100], Loss: 1.1173\n",
      "Epoch [63/100], Loss: 1.1182\n",
      "Epoch [64/100], Loss: 1.1200\n",
      "Epoch [65/100], Loss: 1.1161\n",
      "Epoch [66/100], Loss: 1.1154\n",
      "Epoch [67/100], Loss: 1.1170\n",
      "Epoch [68/100], Loss: 1.1150\n",
      "Epoch [69/100], Loss: 1.1158\n",
      "Epoch [70/100], Loss: 1.1155\n",
      "Epoch [71/100], Loss: 1.1151\n",
      "Stopping early at epoch 71 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [71/100], Loss: 1.1151\n",
      "Test Accuracy Base Logit: 54.97%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.6136\n",
      "Epoch [2/100], Loss: 1.3319\n",
      "Epoch [3/100], Loss: 1.2821\n",
      "Epoch [4/100], Loss: 1.2603\n",
      "Epoch [5/100], Loss: 1.2427\n",
      "Epoch [6/100], Loss: 1.2305\n",
      "Epoch [7/100], Loss: 1.2190\n",
      "Epoch [8/100], Loss: 1.2133\n",
      "Epoch [9/100], Loss: 1.2058\n",
      "Epoch [10/100], Loss: 1.1966\n",
      "Epoch [11/100], Loss: 1.1910\n",
      "Epoch [12/100], Loss: 1.1875\n",
      "Epoch [13/100], Loss: 1.1833\n",
      "Epoch [14/100], Loss: 1.1769\n",
      "Epoch [15/100], Loss: 1.1751\n",
      "Epoch [16/100], Loss: 1.1697\n",
      "Epoch [17/100], Loss: 1.1681\n",
      "Epoch [18/100], Loss: 1.1658\n",
      "Epoch [19/100], Loss: 1.1630\n",
      "Epoch [20/100], Loss: 1.1587\n",
      "Epoch [21/100], Loss: 1.1596\n",
      "Epoch [22/100], Loss: 1.1556\n",
      "Epoch [23/100], Loss: 1.1544\n",
      "Epoch [24/100], Loss: 1.1484\n",
      "Epoch [25/100], Loss: 1.1488\n",
      "Epoch [26/100], Loss: 1.1474\n",
      "Epoch [27/100], Loss: 1.1462\n",
      "Epoch [28/100], Loss: 1.1417\n",
      "Epoch [29/100], Loss: 1.1415\n",
      "Epoch [30/100], Loss: 1.1400\n",
      "Epoch [31/100], Loss: 1.1395\n",
      "Epoch [32/100], Loss: 1.1376\n",
      "Epoch [33/100], Loss: 1.1359\n",
      "Epoch [34/100], Loss: 1.1335\n",
      "Epoch [35/100], Loss: 1.1326\n",
      "Epoch [36/100], Loss: 1.1332\n",
      "Epoch [37/100], Loss: 1.1324\n",
      "Stopping early at epoch 37 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [37/100], Loss: 1.1324\n",
      "Test Accuracy Base Logit: 55.65%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.6137\n",
      "Epoch [2/100], Loss: 1.3349\n",
      "Epoch [3/100], Loss: 1.2843\n",
      "Epoch [4/100], Loss: 1.2590\n",
      "Epoch [5/100], Loss: 1.2442\n",
      "Epoch [6/100], Loss: 1.2320\n",
      "Epoch [7/100], Loss: 1.2202\n",
      "Epoch [8/100], Loss: 1.2104\n",
      "Epoch [9/100], Loss: 1.2037\n",
      "Epoch [10/100], Loss: 1.2005\n",
      "Epoch [11/100], Loss: 1.1911\n",
      "Epoch [12/100], Loss: 1.1892\n",
      "Epoch [13/100], Loss: 1.1840\n",
      "Epoch [14/100], Loss: 1.1778\n",
      "Epoch [15/100], Loss: 1.1749\n",
      "Epoch [16/100], Loss: 1.1730\n",
      "Epoch [17/100], Loss: 1.1686\n",
      "Epoch [18/100], Loss: 1.1652\n",
      "Epoch [19/100], Loss: 1.1612\n",
      "Epoch [20/100], Loss: 1.1587\n",
      "Epoch [21/100], Loss: 1.1576\n",
      "Epoch [22/100], Loss: 1.1544\n",
      "Epoch [23/100], Loss: 1.1516\n",
      "Epoch [24/100], Loss: 1.1510\n",
      "Epoch [25/100], Loss: 1.1480\n",
      "Epoch [26/100], Loss: 1.1464\n",
      "Epoch [27/100], Loss: 1.1424\n",
      "Epoch [28/100], Loss: 1.1441\n",
      "Epoch [29/100], Loss: 1.1412\n",
      "Epoch [30/100], Loss: 1.1396\n",
      "Epoch [31/100], Loss: 1.1410\n",
      "Epoch [32/100], Loss: 1.1366\n",
      "Epoch [33/100], Loss: 1.1340\n",
      "Epoch [34/100], Loss: 1.1338\n",
      "Epoch [35/100], Loss: 1.1316\n",
      "Epoch [36/100], Loss: 1.1314\n",
      "Epoch [37/100], Loss: 1.1298\n",
      "Epoch [38/100], Loss: 1.1289\n",
      "Epoch [39/100], Loss: 1.1259\n",
      "Epoch [40/100], Loss: 1.1271\n",
      "Epoch [41/100], Loss: 1.1265\n",
      "Epoch [42/100], Loss: 1.1239\n",
      "Epoch [43/100], Loss: 1.1226\n",
      "Epoch [44/100], Loss: 1.1219\n",
      "Epoch [45/100], Loss: 1.1212\n",
      "Epoch [46/100], Loss: 1.1215\n",
      "Stopping early at epoch 46 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [46/100], Loss: 1.1215\n",
      "Test Accuracy Base Logit: 55.60%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.6239\n",
      "Epoch [2/100], Loss: 1.3367\n",
      "Epoch [3/100], Loss: 1.2933\n",
      "Epoch [4/100], Loss: 1.2676\n",
      "Epoch [5/100], Loss: 1.2489\n",
      "Epoch [6/100], Loss: 1.2350\n",
      "Epoch [7/100], Loss: 1.2268\n",
      "Epoch [8/100], Loss: 1.2180\n",
      "Epoch [9/100], Loss: 1.2094\n",
      "Epoch [10/100], Loss: 1.2039\n",
      "Epoch [11/100], Loss: 1.1975\n",
      "Epoch [12/100], Loss: 1.1950\n",
      "Epoch [13/100], Loss: 1.1881\n",
      "Epoch [14/100], Loss: 1.1844\n",
      "Epoch [15/100], Loss: 1.1819\n",
      "Epoch [16/100], Loss: 1.1772\n",
      "Epoch [17/100], Loss: 1.1735\n",
      "Epoch [18/100], Loss: 1.1700\n",
      "Epoch [19/100], Loss: 1.1685\n",
      "Epoch [20/100], Loss: 1.1655\n",
      "Epoch [21/100], Loss: 1.1617\n",
      "Epoch [22/100], Loss: 1.1607\n",
      "Epoch [23/100], Loss: 1.1590\n",
      "Epoch [24/100], Loss: 1.1564\n",
      "Epoch [25/100], Loss: 1.1527\n",
      "Epoch [26/100], Loss: 1.1510\n",
      "Epoch [27/100], Loss: 1.1522\n",
      "Epoch [28/100], Loss: 1.1479\n",
      "Epoch [29/100], Loss: 1.1454\n",
      "Epoch [30/100], Loss: 1.1484\n",
      "Epoch [31/100], Loss: 1.1445\n",
      "Epoch [32/100], Loss: 1.1404\n",
      "Epoch [33/100], Loss: 1.1411\n",
      "Epoch [34/100], Loss: 1.1390\n",
      "Epoch [35/100], Loss: 1.1363\n",
      "Epoch [36/100], Loss: 1.1346\n",
      "Epoch [37/100], Loss: 1.1361\n",
      "Epoch [38/100], Loss: 1.1355\n",
      "Epoch [39/100], Loss: 1.1326\n",
      "Epoch [40/100], Loss: 1.1309\n",
      "Epoch [41/100], Loss: 1.1313\n",
      "Epoch [42/100], Loss: 1.1298\n",
      "Epoch [43/100], Loss: 1.1298\n",
      "Epoch [44/100], Loss: 1.1287\n",
      "Epoch [45/100], Loss: 1.1255\n",
      "Epoch [46/100], Loss: 1.1270\n",
      "Epoch [47/100], Loss: 1.1260\n",
      "Epoch [48/100], Loss: 1.1256\n",
      "Stopping early at epoch 48 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [48/100], Loss: 1.1256\n",
      "Test Accuracy Base Logit: 56.03%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.6174\n",
      "Epoch [2/100], Loss: 1.3310\n",
      "Epoch [3/100], Loss: 1.2872\n",
      "Epoch [4/100], Loss: 1.2603\n",
      "Epoch [5/100], Loss: 1.2400\n",
      "Epoch [6/100], Loss: 1.2297\n",
      "Epoch [7/100], Loss: 1.2206\n",
      "Epoch [8/100], Loss: 1.2124\n",
      "Epoch [9/100], Loss: 1.2031\n",
      "Epoch [10/100], Loss: 1.1956\n",
      "Epoch [11/100], Loss: 1.1923\n",
      "Epoch [12/100], Loss: 1.1851\n",
      "Epoch [13/100], Loss: 1.1816\n",
      "Epoch [14/100], Loss: 1.1779\n",
      "Epoch [15/100], Loss: 1.1711\n",
      "Epoch [16/100], Loss: 1.1684\n",
      "Epoch [17/100], Loss: 1.1666\n",
      "Epoch [18/100], Loss: 1.1631\n",
      "Epoch [19/100], Loss: 1.1639\n",
      "Epoch [20/100], Loss: 1.1606\n",
      "Epoch [21/100], Loss: 1.1555\n",
      "Epoch [22/100], Loss: 1.1525\n",
      "Epoch [23/100], Loss: 1.1539\n",
      "Epoch [24/100], Loss: 1.1510\n",
      "Epoch [25/100], Loss: 1.1474\n",
      "Epoch [26/100], Loss: 1.1465\n",
      "Epoch [27/100], Loss: 1.1424\n",
      "Epoch [28/100], Loss: 1.1436\n",
      "Epoch [29/100], Loss: 1.1419\n",
      "Epoch [30/100], Loss: 1.1402\n",
      "Epoch [31/100], Loss: 1.1379\n",
      "Epoch [32/100], Loss: 1.1372\n",
      "Epoch [33/100], Loss: 1.1360\n",
      "Epoch [34/100], Loss: 1.1330\n",
      "Epoch [35/100], Loss: 1.1327\n",
      "Epoch [36/100], Loss: 1.1292\n",
      "Epoch [37/100], Loss: 1.1289\n",
      "Epoch [38/100], Loss: 1.1266\n",
      "Epoch [39/100], Loss: 1.1278\n",
      "Epoch [40/100], Loss: 1.1273\n",
      "Epoch [41/100], Loss: 1.1254\n",
      "Epoch [42/100], Loss: 1.1235\n",
      "Epoch [43/100], Loss: 1.1225\n",
      "Epoch [44/100], Loss: 1.1224\n",
      "Epoch [45/100], Loss: 1.1220\n",
      "Stopping early at epoch 45 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [45/100], Loss: 1.1220\n",
      "Test Accuracy Base Logit: 54.79%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.6254\n",
      "Epoch [2/100], Loss: 1.3387\n",
      "Epoch [3/100], Loss: 1.2938\n",
      "Epoch [4/100], Loss: 1.2689\n",
      "Epoch [5/100], Loss: 1.2512\n",
      "Epoch [6/100], Loss: 1.2400\n",
      "Epoch [7/100], Loss: 1.2295\n",
      "Epoch [8/100], Loss: 1.2186\n",
      "Epoch [9/100], Loss: 1.2118\n",
      "Epoch [10/100], Loss: 1.2061\n",
      "Epoch [11/100], Loss: 1.2004\n",
      "Epoch [12/100], Loss: 1.1965\n",
      "Epoch [13/100], Loss: 1.1917\n",
      "Epoch [14/100], Loss: 1.1867\n",
      "Epoch [15/100], Loss: 1.1830\n",
      "Epoch [16/100], Loss: 1.1787\n",
      "Epoch [17/100], Loss: 1.1762\n",
      "Epoch [18/100], Loss: 1.1740\n",
      "Epoch [19/100], Loss: 1.1698\n",
      "Epoch [20/100], Loss: 1.1657\n",
      "Epoch [21/100], Loss: 1.1634\n",
      "Epoch [22/100], Loss: 1.1642\n",
      "Epoch [23/100], Loss: 1.1610\n",
      "Epoch [24/100], Loss: 1.1613\n",
      "Epoch [25/100], Loss: 1.1546\n",
      "Epoch [26/100], Loss: 1.1560\n",
      "Epoch [27/100], Loss: 1.1526\n",
      "Epoch [28/100], Loss: 1.1511\n",
      "Epoch [29/100], Loss: 1.1498\n",
      "Epoch [30/100], Loss: 1.1489\n",
      "Epoch [31/100], Loss: 1.1471\n",
      "Epoch [32/100], Loss: 1.1449\n",
      "Epoch [33/100], Loss: 1.1438\n",
      "Epoch [34/100], Loss: 1.1418\n",
      "Epoch [35/100], Loss: 1.1417\n",
      "Epoch [36/100], Loss: 1.1407\n",
      "Epoch [37/100], Loss: 1.1395\n",
      "Epoch [38/100], Loss: 1.1380\n",
      "Epoch [39/100], Loss: 1.1366\n",
      "Epoch [40/100], Loss: 1.1359\n",
      "Epoch [41/100], Loss: 1.1326\n",
      "Epoch [42/100], Loss: 1.1331\n",
      "Epoch [43/100], Loss: 1.1317\n",
      "Epoch [44/100], Loss: 1.1312\n",
      "Epoch [45/100], Loss: 1.1299\n",
      "Epoch [46/100], Loss: 1.1308\n",
      "Epoch [47/100], Loss: 1.1280\n",
      "Epoch [48/100], Loss: 1.1280\n",
      "Epoch [49/100], Loss: 1.1264\n",
      "Epoch [50/100], Loss: 1.1259\n",
      "Epoch [51/100], Loss: 1.1230\n",
      "Epoch [52/100], Loss: 1.1248\n",
      "Epoch [53/100], Loss: 1.1224\n",
      "Epoch [54/100], Loss: 1.1255\n",
      "Epoch [55/100], Loss: 1.1227\n",
      "Epoch [56/100], Loss: 1.1230\n",
      "Epoch [57/100], Loss: 1.1208\n",
      "Epoch [58/100], Loss: 1.1205\n",
      "Epoch [59/100], Loss: 1.1196\n",
      "Epoch [60/100], Loss: 1.1189\n",
      "Stopping early at epoch 60 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [60/100], Loss: 1.1189\n",
      "Test Accuracy Base Logit: 55.21%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.6255\n",
      "Epoch [2/100], Loss: 1.3454\n",
      "Epoch [3/100], Loss: 1.2984\n",
      "Epoch [4/100], Loss: 1.2715\n",
      "Epoch [5/100], Loss: 1.2554\n",
      "Epoch [6/100], Loss: 1.2421\n",
      "Epoch [7/100], Loss: 1.2308\n",
      "Epoch [8/100], Loss: 1.2230\n",
      "Epoch [9/100], Loss: 1.2170\n",
      "Epoch [10/100], Loss: 1.2125\n",
      "Epoch [11/100], Loss: 1.2027\n",
      "Epoch [12/100], Loss: 1.1999\n",
      "Epoch [13/100], Loss: 1.1967\n",
      "Epoch [14/100], Loss: 1.1901\n",
      "Epoch [15/100], Loss: 1.1891\n",
      "Epoch [16/100], Loss: 1.1839\n",
      "Epoch [17/100], Loss: 1.1822\n",
      "Epoch [18/100], Loss: 1.1792\n",
      "Epoch [19/100], Loss: 1.1747\n",
      "Epoch [20/100], Loss: 1.1722\n",
      "Epoch [21/100], Loss: 1.1693\n",
      "Epoch [22/100], Loss: 1.1702\n",
      "Epoch [23/100], Loss: 1.1668\n",
      "Epoch [24/100], Loss: 1.1642\n",
      "Epoch [25/100], Loss: 1.1621\n",
      "Epoch [26/100], Loss: 1.1615\n",
      "Epoch [27/100], Loss: 1.1568\n",
      "Epoch [28/100], Loss: 1.1565\n",
      "Epoch [29/100], Loss: 1.1557\n",
      "Epoch [30/100], Loss: 1.1536\n",
      "Epoch [31/100], Loss: 1.1525\n",
      "Epoch [32/100], Loss: 1.1489\n",
      "Epoch [33/100], Loss: 1.1487\n",
      "Epoch [34/100], Loss: 1.1471\n",
      "Epoch [35/100], Loss: 1.1450\n",
      "Epoch [36/100], Loss: 1.1462\n",
      "Epoch [37/100], Loss: 1.1440\n",
      "Epoch [38/100], Loss: 1.1407\n",
      "Epoch [39/100], Loss: 1.1408\n",
      "Epoch [40/100], Loss: 1.1412\n",
      "Epoch [41/100], Loss: 1.1390\n",
      "Epoch [42/100], Loss: 1.1389\n",
      "Epoch [43/100], Loss: 1.1364\n",
      "Epoch [44/100], Loss: 1.1354\n",
      "Epoch [45/100], Loss: 1.1346\n",
      "Epoch [46/100], Loss: 1.1356\n",
      "Epoch [47/100], Loss: 1.1305\n",
      "Epoch [48/100], Loss: 1.1334\n",
      "Epoch [49/100], Loss: 1.1308\n",
      "Epoch [50/100], Loss: 1.1324\n",
      "Epoch [51/100], Loss: 1.1310\n",
      "Epoch [52/100], Loss: 1.1287\n",
      "Epoch [53/100], Loss: 1.1301\n",
      "Epoch [54/100], Loss: 1.1281\n",
      "Epoch [55/100], Loss: 1.1273\n",
      "Epoch [56/100], Loss: 1.1258\n",
      "Epoch [57/100], Loss: 1.1278\n",
      "Epoch [58/100], Loss: 1.1246\n",
      "Epoch [59/100], Loss: 1.1236\n",
      "Epoch [60/100], Loss: 1.1236\n",
      "Epoch [61/100], Loss: 1.1240\n",
      "Epoch [62/100], Loss: 1.1222\n",
      "Epoch [63/100], Loss: 1.1235\n",
      "Epoch [64/100], Loss: 1.1211\n",
      "Epoch [65/100], Loss: 1.1189\n",
      "Epoch [66/100], Loss: 1.1198\n",
      "Epoch [67/100], Loss: 1.1199\n",
      "Epoch [68/100], Loss: 1.1195\n",
      "Stopping early at epoch 68 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [68/100], Loss: 1.1195\n",
      "Test Accuracy Base Logit: 54.49%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.6215\n",
      "Epoch [2/100], Loss: 1.3387\n",
      "Epoch [3/100], Loss: 1.2925\n",
      "Epoch [4/100], Loss: 1.2649\n",
      "Epoch [5/100], Loss: 1.2499\n",
      "Epoch [6/100], Loss: 1.2374\n",
      "Epoch [7/100], Loss: 1.2275\n",
      "Epoch [8/100], Loss: 1.2180\n",
      "Epoch [9/100], Loss: 1.2103\n",
      "Epoch [10/100], Loss: 1.2035\n",
      "Epoch [11/100], Loss: 1.1991\n",
      "Epoch [12/100], Loss: 1.1946\n",
      "Epoch [13/100], Loss: 1.1882\n",
      "Epoch [14/100], Loss: 1.1856\n",
      "Epoch [15/100], Loss: 1.1799\n",
      "Epoch [16/100], Loss: 1.1769\n",
      "Epoch [17/100], Loss: 1.1761\n",
      "Epoch [18/100], Loss: 1.1711\n",
      "Epoch [19/100], Loss: 1.1692\n",
      "Epoch [20/100], Loss: 1.1658\n",
      "Epoch [21/100], Loss: 1.1652\n",
      "Epoch [22/100], Loss: 1.1619\n",
      "Epoch [23/100], Loss: 1.1581\n",
      "Epoch [24/100], Loss: 1.1570\n",
      "Epoch [25/100], Loss: 1.1565\n",
      "Epoch [26/100], Loss: 1.1538\n",
      "Epoch [27/100], Loss: 1.1514\n",
      "Epoch [28/100], Loss: 1.1524\n",
      "Epoch [29/100], Loss: 1.1506\n",
      "Epoch [30/100], Loss: 1.1458\n",
      "Epoch [31/100], Loss: 1.1439\n",
      "Epoch [32/100], Loss: 1.1412\n",
      "Epoch [33/100], Loss: 1.1414\n",
      "Epoch [34/100], Loss: 1.1401\n",
      "Epoch [35/100], Loss: 1.1395\n",
      "Epoch [36/100], Loss: 1.1403\n",
      "Epoch [37/100], Loss: 1.1367\n",
      "Epoch [38/100], Loss: 1.1352\n",
      "Epoch [39/100], Loss: 1.1346\n",
      "Epoch [40/100], Loss: 1.1352\n",
      "Epoch [41/100], Loss: 1.1341\n",
      "Epoch [42/100], Loss: 1.1306\n",
      "Epoch [43/100], Loss: 1.1316\n",
      "Epoch [44/100], Loss: 1.1283\n",
      "Epoch [45/100], Loss: 1.1288\n",
      "Epoch [46/100], Loss: 1.1275\n",
      "Epoch [47/100], Loss: 1.1256\n",
      "Epoch [48/100], Loss: 1.1247\n",
      "Epoch [49/100], Loss: 1.1270\n",
      "Epoch [50/100], Loss: 1.1242\n",
      "Epoch [51/100], Loss: 1.1215\n",
      "Epoch [52/100], Loss: 1.1223\n",
      "Epoch [53/100], Loss: 1.1234\n",
      "Epoch [54/100], Loss: 1.1199\n",
      "Epoch [55/100], Loss: 1.1205\n",
      "Epoch [56/100], Loss: 1.1188\n",
      "Epoch [57/100], Loss: 1.1200\n",
      "Epoch [58/100], Loss: 1.1190\n",
      "Epoch [59/100], Loss: 1.1172\n",
      "Epoch [60/100], Loss: 1.1190\n",
      "Epoch [61/100], Loss: 1.1157\n",
      "Epoch [62/100], Loss: 1.1163\n",
      "Epoch [63/100], Loss: 1.1146\n",
      "Epoch [64/100], Loss: 1.1160\n",
      "Epoch [65/100], Loss: 1.1138\n",
      "Epoch [66/100], Loss: 1.1138\n",
      "Epoch [67/100], Loss: 1.1121\n",
      "Epoch [68/100], Loss: 1.1124\n",
      "Epoch [69/100], Loss: 1.1129\n",
      "Epoch [70/100], Loss: 1.1109\n",
      "Epoch [71/100], Loss: 1.1113\n",
      "Epoch [72/100], Loss: 1.1110\n",
      "Epoch [73/100], Loss: 1.1086\n",
      "Epoch [74/100], Loss: 1.1087\n",
      "Epoch [75/100], Loss: 1.1085\n",
      "Epoch [76/100], Loss: 1.1102\n",
      "Stopping early at epoch 76 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [76/100], Loss: 1.1102\n",
      "Test Accuracy Base Logit: 54.96%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.6218\n",
      "Epoch [2/100], Loss: 1.3358\n",
      "Epoch [3/100], Loss: 1.2897\n",
      "Epoch [4/100], Loss: 1.2642\n",
      "Epoch [5/100], Loss: 1.2465\n",
      "Epoch [6/100], Loss: 1.2352\n",
      "Epoch [7/100], Loss: 1.2265\n",
      "Epoch [8/100], Loss: 1.2177\n",
      "Epoch [9/100], Loss: 1.2088\n",
      "Epoch [10/100], Loss: 1.1996\n",
      "Epoch [11/100], Loss: 1.1970\n",
      "Epoch [12/100], Loss: 1.1910\n",
      "Epoch [13/100], Loss: 1.1851\n",
      "Epoch [14/100], Loss: 1.1814\n",
      "Epoch [15/100], Loss: 1.1779\n",
      "Epoch [16/100], Loss: 1.1762\n",
      "Epoch [17/100], Loss: 1.1727\n",
      "Epoch [18/100], Loss: 1.1707\n",
      "Epoch [19/100], Loss: 1.1637\n",
      "Epoch [20/100], Loss: 1.1626\n",
      "Epoch [21/100], Loss: 1.1616\n",
      "Epoch [22/100], Loss: 1.1607\n",
      "Epoch [23/100], Loss: 1.1549\n",
      "Epoch [24/100], Loss: 1.1526\n",
      "Epoch [25/100], Loss: 1.1512\n",
      "Epoch [26/100], Loss: 1.1500\n",
      "Epoch [27/100], Loss: 1.1485\n",
      "Epoch [28/100], Loss: 1.1464\n",
      "Epoch [29/100], Loss: 1.1470\n",
      "Epoch [30/100], Loss: 1.1423\n",
      "Epoch [31/100], Loss: 1.1415\n",
      "Epoch [32/100], Loss: 1.1406\n",
      "Epoch [33/100], Loss: 1.1364\n",
      "Epoch [34/100], Loss: 1.1358\n",
      "Epoch [35/100], Loss: 1.1348\n",
      "Epoch [36/100], Loss: 1.1357\n",
      "Epoch [37/100], Loss: 1.1351\n",
      "Epoch [38/100], Loss: 1.1290\n",
      "Epoch [39/100], Loss: 1.1294\n",
      "Epoch [40/100], Loss: 1.1292\n",
      "Epoch [41/100], Loss: 1.1301\n",
      "Stopping early at epoch 41 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [41/100], Loss: 1.1301\n",
      "Test Accuracy Base Logit: 54.73%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.6173\n",
      "Epoch [2/100], Loss: 1.3323\n",
      "Epoch [3/100], Loss: 1.2857\n",
      "Epoch [4/100], Loss: 1.2616\n",
      "Epoch [5/100], Loss: 1.2425\n",
      "Epoch [6/100], Loss: 1.2326\n",
      "Epoch [7/100], Loss: 1.2213\n",
      "Epoch [8/100], Loss: 1.2131\n",
      "Epoch [9/100], Loss: 1.2051\n",
      "Epoch [10/100], Loss: 1.1990\n",
      "Epoch [11/100], Loss: 1.1902\n",
      "Epoch [12/100], Loss: 1.1869\n",
      "Epoch [13/100], Loss: 1.1840\n",
      "Epoch [14/100], Loss: 1.1812\n",
      "Epoch [15/100], Loss: 1.1770\n",
      "Epoch [16/100], Loss: 1.1728\n",
      "Epoch [17/100], Loss: 1.1703\n",
      "Epoch [18/100], Loss: 1.1670\n",
      "Epoch [19/100], Loss: 1.1619\n",
      "Epoch [20/100], Loss: 1.1612\n",
      "Epoch [21/100], Loss: 1.1575\n",
      "Epoch [22/100], Loss: 1.1570\n",
      "Epoch [23/100], Loss: 1.1518\n",
      "Epoch [24/100], Loss: 1.1512\n",
      "Epoch [25/100], Loss: 1.1508\n",
      "Epoch [26/100], Loss: 1.1477\n",
      "Epoch [27/100], Loss: 1.1438\n",
      "Epoch [28/100], Loss: 1.1428\n",
      "Epoch [29/100], Loss: 1.1456\n",
      "Epoch [30/100], Loss: 1.1417\n",
      "Epoch [31/100], Loss: 1.1391\n",
      "Epoch [32/100], Loss: 1.1362\n",
      "Epoch [33/100], Loss: 1.1375\n",
      "Epoch [34/100], Loss: 1.1365\n",
      "Epoch [35/100], Loss: 1.1341\n",
      "Epoch [36/100], Loss: 1.1316\n",
      "Epoch [37/100], Loss: 1.1322\n",
      "Epoch [38/100], Loss: 1.1310\n",
      "Epoch [39/100], Loss: 1.1286\n",
      "Epoch [40/100], Loss: 1.1260\n",
      "Epoch [41/100], Loss: 1.1299\n",
      "Epoch [42/100], Loss: 1.1248\n",
      "Epoch [43/100], Loss: 1.1277\n",
      "Epoch [44/100], Loss: 1.1235\n",
      "Epoch [45/100], Loss: 1.1227\n",
      "Epoch [46/100], Loss: 1.1213\n",
      "Epoch [47/100], Loss: 1.1209\n",
      "Epoch [48/100], Loss: 1.1195\n",
      "Epoch [49/100], Loss: 1.1190\n",
      "Epoch [50/100], Loss: 1.1199\n",
      "Epoch [51/100], Loss: 1.1162\n",
      "Epoch [52/100], Loss: 1.1172\n",
      "Epoch [53/100], Loss: 1.1179\n",
      "Epoch [54/100], Loss: 1.1171\n",
      "Stopping early at epoch 54 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [54/100], Loss: 1.1171\n",
      "Test Accuracy Base Logit: 55.28%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.7301\n",
      "Epoch [2/100], Loss: 1.3720\n",
      "Epoch [3/100], Loss: 1.3108\n",
      "Epoch [4/100], Loss: 1.2778\n",
      "Epoch [5/100], Loss: 1.2572\n",
      "Epoch [6/100], Loss: 1.2409\n",
      "Epoch [7/100], Loss: 1.2229\n",
      "Epoch [8/100], Loss: 1.2101\n",
      "Epoch [9/100], Loss: 1.2043\n",
      "Epoch [10/100], Loss: 1.1940\n",
      "Epoch [11/100], Loss: 1.1861\n",
      "Epoch [12/100], Loss: 1.1812\n",
      "Epoch [13/100], Loss: 1.1758\n",
      "Epoch [14/100], Loss: 1.1687\n",
      "Epoch [15/100], Loss: 1.1655\n",
      "Epoch [16/100], Loss: 1.1576\n",
      "Epoch [17/100], Loss: 1.1534\n",
      "Epoch [18/100], Loss: 1.1507\n",
      "Epoch [19/100], Loss: 1.1444\n",
      "Epoch [20/100], Loss: 1.1432\n",
      "Epoch [21/100], Loss: 1.1385\n",
      "Epoch [22/100], Loss: 1.1355\n",
      "Epoch [23/100], Loss: 1.1316\n",
      "Epoch [24/100], Loss: 1.1297\n",
      "Epoch [25/100], Loss: 1.1247\n",
      "Epoch [26/100], Loss: 1.1234\n",
      "Epoch [27/100], Loss: 1.1202\n",
      "Epoch [28/100], Loss: 1.1153\n",
      "Epoch [29/100], Loss: 1.1140\n",
      "Epoch [30/100], Loss: 1.1124\n",
      "Epoch [31/100], Loss: 1.1117\n",
      "Epoch [32/100], Loss: 1.1081\n",
      "Epoch [33/100], Loss: 1.1048\n",
      "Epoch [34/100], Loss: 1.1083\n",
      "Epoch [35/100], Loss: 1.1023\n",
      "Epoch [36/100], Loss: 1.1003\n",
      "Epoch [37/100], Loss: 1.0955\n",
      "Epoch [38/100], Loss: 1.0970\n",
      "Epoch [39/100], Loss: 1.0945\n",
      "Epoch [40/100], Loss: 1.0935\n",
      "Epoch [41/100], Loss: 1.0901\n",
      "Epoch [42/100], Loss: 1.0911\n",
      "Epoch [43/100], Loss: 1.0868\n",
      "Epoch [44/100], Loss: 1.0866\n",
      "Epoch [45/100], Loss: 1.0834\n",
      "Epoch [46/100], Loss: 1.0826\n",
      "Epoch [47/100], Loss: 1.0817\n",
      "Epoch [48/100], Loss: 1.0823\n",
      "Stopping early at epoch 48 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [48/100], Loss: 1.0823\n",
      "Test Accuracy Base Logit: 54.83%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.7301\n",
      "Epoch [2/100], Loss: 1.3695\n",
      "Epoch [3/100], Loss: 1.3116\n",
      "Epoch [4/100], Loss: 1.2772\n",
      "Epoch [5/100], Loss: 1.2537\n",
      "Epoch [6/100], Loss: 1.2374\n",
      "Epoch [7/100], Loss: 1.2218\n",
      "Epoch [8/100], Loss: 1.2100\n",
      "Epoch [9/100], Loss: 1.2021\n",
      "Epoch [10/100], Loss: 1.1898\n",
      "Epoch [11/100], Loss: 1.1848\n",
      "Epoch [12/100], Loss: 1.1777\n",
      "Epoch [13/100], Loss: 1.1733\n",
      "Epoch [14/100], Loss: 1.1649\n",
      "Epoch [15/100], Loss: 1.1637\n",
      "Epoch [16/100], Loss: 1.1578\n",
      "Epoch [17/100], Loss: 1.1513\n",
      "Epoch [18/100], Loss: 1.1489\n",
      "Epoch [19/100], Loss: 1.1455\n",
      "Epoch [20/100], Loss: 1.1410\n",
      "Epoch [21/100], Loss: 1.1338\n",
      "Epoch [22/100], Loss: 1.1321\n",
      "Epoch [23/100], Loss: 1.1301\n",
      "Epoch [24/100], Loss: 1.1255\n",
      "Epoch [25/100], Loss: 1.1218\n",
      "Epoch [26/100], Loss: 1.1221\n",
      "Epoch [27/100], Loss: 1.1183\n",
      "Epoch [28/100], Loss: 1.1132\n",
      "Epoch [29/100], Loss: 1.1106\n",
      "Epoch [30/100], Loss: 1.1108\n",
      "Epoch [31/100], Loss: 1.1055\n",
      "Epoch [32/100], Loss: 1.1053\n",
      "Epoch [33/100], Loss: 1.1032\n",
      "Epoch [34/100], Loss: 1.1031\n",
      "Epoch [35/100], Loss: 1.1011\n",
      "Epoch [36/100], Loss: 1.0973\n",
      "Epoch [37/100], Loss: 1.0957\n",
      "Epoch [38/100], Loss: 1.0932\n",
      "Epoch [39/100], Loss: 1.0928\n",
      "Epoch [40/100], Loss: 1.0939\n",
      "Epoch [41/100], Loss: 1.0867\n",
      "Epoch [42/100], Loss: 1.0878\n",
      "Epoch [43/100], Loss: 1.0861\n",
      "Epoch [44/100], Loss: 1.0829\n",
      "Epoch [45/100], Loss: 1.0856\n",
      "Epoch [46/100], Loss: 1.0806\n",
      "Epoch [47/100], Loss: 1.0782\n",
      "Epoch [48/100], Loss: 1.0792\n",
      "Epoch [49/100], Loss: 1.0748\n",
      "Epoch [50/100], Loss: 1.0747\n",
      "Epoch [51/100], Loss: 1.0730\n",
      "Epoch [52/100], Loss: 1.0755\n",
      "Epoch [53/100], Loss: 1.0711\n",
      "Epoch [54/100], Loss: 1.0709\n",
      "Epoch [55/100], Loss: 1.0683\n",
      "Epoch [56/100], Loss: 1.0690\n",
      "Epoch [57/100], Loss: 1.0685\n",
      "Epoch [58/100], Loss: 1.0682\n",
      "Stopping early at epoch 58 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [58/100], Loss: 1.0682\n",
      "Test Accuracy Base Logit: 54.45%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.7292\n",
      "Epoch [2/100], Loss: 1.3762\n",
      "Epoch [3/100], Loss: 1.3140\n",
      "Epoch [4/100], Loss: 1.2812\n",
      "Epoch [5/100], Loss: 1.2593\n",
      "Epoch [6/100], Loss: 1.2443\n",
      "Epoch [7/100], Loss: 1.2270\n",
      "Epoch [8/100], Loss: 1.2159\n",
      "Epoch [9/100], Loss: 1.2054\n",
      "Epoch [10/100], Loss: 1.2000\n",
      "Epoch [11/100], Loss: 1.1928\n",
      "Epoch [12/100], Loss: 1.1833\n",
      "Epoch [13/100], Loss: 1.1785\n",
      "Epoch [14/100], Loss: 1.1720\n",
      "Epoch [15/100], Loss: 1.1669\n",
      "Epoch [16/100], Loss: 1.1589\n",
      "Epoch [17/100], Loss: 1.1566\n",
      "Epoch [18/100], Loss: 1.1550\n",
      "Epoch [19/100], Loss: 1.1483\n",
      "Epoch [20/100], Loss: 1.1463\n",
      "Epoch [21/100], Loss: 1.1424\n",
      "Epoch [22/100], Loss: 1.1400\n",
      "Epoch [23/100], Loss: 1.1355\n",
      "Epoch [24/100], Loss: 1.1333\n",
      "Epoch [25/100], Loss: 1.1341\n",
      "Epoch [26/100], Loss: 1.1263\n",
      "Epoch [27/100], Loss: 1.1231\n",
      "Epoch [28/100], Loss: 1.1207\n",
      "Epoch [29/100], Loss: 1.1220\n",
      "Epoch [30/100], Loss: 1.1162\n",
      "Epoch [31/100], Loss: 1.1117\n",
      "Epoch [32/100], Loss: 1.1110\n",
      "Epoch [33/100], Loss: 1.1093\n",
      "Epoch [34/100], Loss: 1.1068\n",
      "Epoch [35/100], Loss: 1.1042\n",
      "Epoch [36/100], Loss: 1.1052\n",
      "Epoch [37/100], Loss: 1.1022\n",
      "Epoch [38/100], Loss: 1.1018\n",
      "Epoch [39/100], Loss: 1.0988\n",
      "Epoch [40/100], Loss: 1.0984\n",
      "Epoch [41/100], Loss: 1.0943\n",
      "Epoch [42/100], Loss: 1.0909\n",
      "Epoch [43/100], Loss: 1.0899\n",
      "Epoch [44/100], Loss: 1.0904\n",
      "Epoch [45/100], Loss: 1.0900\n",
      "Epoch [46/100], Loss: 1.0894\n",
      "Stopping early at epoch 46 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [46/100], Loss: 1.0894\n",
      "Test Accuracy Base Logit: 54.34%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.7294\n",
      "Epoch [2/100], Loss: 1.3726\n",
      "Epoch [3/100], Loss: 1.3141\n",
      "Epoch [4/100], Loss: 1.2809\n",
      "Epoch [5/100], Loss: 1.2590\n",
      "Epoch [6/100], Loss: 1.2416\n",
      "Epoch [7/100], Loss: 1.2289\n",
      "Epoch [8/100], Loss: 1.2164\n",
      "Epoch [9/100], Loss: 1.2060\n",
      "Epoch [10/100], Loss: 1.1990\n",
      "Epoch [11/100], Loss: 1.1868\n",
      "Epoch [12/100], Loss: 1.1835\n",
      "Epoch [13/100], Loss: 1.1784\n",
      "Epoch [14/100], Loss: 1.1713\n",
      "Epoch [15/100], Loss: 1.1677\n",
      "Epoch [16/100], Loss: 1.1612\n",
      "Epoch [17/100], Loss: 1.1610\n",
      "Epoch [18/100], Loss: 1.1565\n",
      "Epoch [19/100], Loss: 1.1478\n",
      "Epoch [20/100], Loss: 1.1463\n",
      "Epoch [21/100], Loss: 1.1386\n",
      "Epoch [22/100], Loss: 1.1389\n",
      "Epoch [23/100], Loss: 1.1340\n",
      "Epoch [24/100], Loss: 1.1301\n",
      "Epoch [25/100], Loss: 1.1282\n",
      "Epoch [26/100], Loss: 1.1239\n",
      "Epoch [27/100], Loss: 1.1205\n",
      "Epoch [28/100], Loss: 1.1231\n",
      "Epoch [29/100], Loss: 1.1164\n",
      "Epoch [30/100], Loss: 1.1155\n",
      "Epoch [31/100], Loss: 1.1154\n",
      "Epoch [32/100], Loss: 1.1125\n",
      "Epoch [33/100], Loss: 1.1101\n",
      "Epoch [34/100], Loss: 1.1074\n",
      "Epoch [35/100], Loss: 1.1073\n",
      "Epoch [36/100], Loss: 1.1015\n",
      "Epoch [37/100], Loss: 1.1000\n",
      "Epoch [38/100], Loss: 1.0960\n",
      "Epoch [39/100], Loss: 1.0968\n",
      "Epoch [40/100], Loss: 1.0945\n",
      "Epoch [41/100], Loss: 1.0934\n",
      "Epoch [42/100], Loss: 1.0894\n",
      "Epoch [43/100], Loss: 1.0914\n",
      "Epoch [44/100], Loss: 1.0883\n",
      "Epoch [45/100], Loss: 1.0858\n",
      "Epoch [46/100], Loss: 1.0849\n",
      "Epoch [47/100], Loss: 1.0869\n",
      "Epoch [48/100], Loss: 1.0808\n",
      "Epoch [49/100], Loss: 1.0814\n",
      "Epoch [50/100], Loss: 1.0778\n",
      "Epoch [51/100], Loss: 1.0798\n",
      "Epoch [52/100], Loss: 1.0762\n",
      "Epoch [53/100], Loss: 1.0779\n",
      "Epoch [54/100], Loss: 1.0752\n",
      "Epoch [55/100], Loss: 1.0739\n",
      "Epoch [56/100], Loss: 1.0719\n",
      "Epoch [57/100], Loss: 1.0716\n",
      "Epoch [58/100], Loss: 1.0695\n",
      "Epoch [59/100], Loss: 1.0717\n",
      "Epoch [60/100], Loss: 1.0668\n",
      "Epoch [61/100], Loss: 1.0664\n",
      "Epoch [62/100], Loss: 1.0664\n",
      "Epoch [63/100], Loss: 1.0660\n",
      "Stopping early at epoch 63 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [63/100], Loss: 1.0660\n",
      "Test Accuracy Base Logit: 54.53%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.7203\n",
      "Epoch [2/100], Loss: 1.3634\n",
      "Epoch [3/100], Loss: 1.3053\n",
      "Epoch [4/100], Loss: 1.2730\n",
      "Epoch [5/100], Loss: 1.2494\n",
      "Epoch [6/100], Loss: 1.2336\n",
      "Epoch [7/100], Loss: 1.2200\n",
      "Epoch [8/100], Loss: 1.2077\n",
      "Epoch [9/100], Loss: 1.1971\n",
      "Epoch [10/100], Loss: 1.1853\n",
      "Epoch [11/100], Loss: 1.1786\n",
      "Epoch [12/100], Loss: 1.1751\n",
      "Epoch [13/100], Loss: 1.1716\n",
      "Epoch [14/100], Loss: 1.1635\n",
      "Epoch [15/100], Loss: 1.1551\n",
      "Epoch [16/100], Loss: 1.1519\n",
      "Epoch [17/100], Loss: 1.1461\n",
      "Epoch [18/100], Loss: 1.1441\n",
      "Epoch [19/100], Loss: 1.1406\n",
      "Epoch [20/100], Loss: 1.1349\n",
      "Epoch [21/100], Loss: 1.1322\n",
      "Epoch [22/100], Loss: 1.1267\n",
      "Epoch [23/100], Loss: 1.1274\n",
      "Epoch [24/100], Loss: 1.1216\n",
      "Epoch [25/100], Loss: 1.1215\n",
      "Epoch [26/100], Loss: 1.1162\n",
      "Epoch [27/100], Loss: 1.1120\n",
      "Epoch [28/100], Loss: 1.1102\n",
      "Epoch [29/100], Loss: 1.1085\n",
      "Epoch [30/100], Loss: 1.1064\n",
      "Epoch [31/100], Loss: 1.1026\n",
      "Epoch [32/100], Loss: 1.1003\n",
      "Epoch [33/100], Loss: 1.1005\n",
      "Epoch [34/100], Loss: 1.0979\n",
      "Epoch [35/100], Loss: 1.0964\n",
      "Epoch [36/100], Loss: 1.0959\n",
      "Epoch [37/100], Loss: 1.0927\n",
      "Epoch [38/100], Loss: 1.0891\n",
      "Epoch [39/100], Loss: 1.0914\n",
      "Epoch [40/100], Loss: 1.0873\n",
      "Epoch [41/100], Loss: 1.0833\n",
      "Epoch [42/100], Loss: 1.0830\n",
      "Epoch [43/100], Loss: 1.0839\n",
      "Epoch [44/100], Loss: 1.0833\n",
      "Stopping early at epoch 44 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [44/100], Loss: 1.0833\n",
      "Test Accuracy Base Logit: 55.25%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.7203\n",
      "Epoch [2/100], Loss: 1.3633\n",
      "Epoch [3/100], Loss: 1.3027\n",
      "Epoch [4/100], Loss: 1.2655\n",
      "Epoch [5/100], Loss: 1.2435\n",
      "Epoch [6/100], Loss: 1.2280\n",
      "Epoch [7/100], Loss: 1.2132\n",
      "Epoch [8/100], Loss: 1.2022\n",
      "Epoch [9/100], Loss: 1.1942\n",
      "Epoch [10/100], Loss: 1.1845\n",
      "Epoch [11/100], Loss: 1.1769\n",
      "Epoch [12/100], Loss: 1.1713\n",
      "Epoch [13/100], Loss: 1.1629\n",
      "Epoch [14/100], Loss: 1.1588\n",
      "Epoch [15/100], Loss: 1.1526\n",
      "Epoch [16/100], Loss: 1.1461\n",
      "Epoch [17/100], Loss: 1.1397\n",
      "Epoch [18/100], Loss: 1.1362\n",
      "Epoch [19/100], Loss: 1.1352\n",
      "Epoch [20/100], Loss: 1.1317\n",
      "Epoch [21/100], Loss: 1.1275\n",
      "Epoch [22/100], Loss: 1.1222\n",
      "Epoch [23/100], Loss: 1.1224\n",
      "Epoch [24/100], Loss: 1.1187\n",
      "Epoch [25/100], Loss: 1.1152\n",
      "Epoch [26/100], Loss: 1.1101\n",
      "Epoch [27/100], Loss: 1.1077\n",
      "Epoch [28/100], Loss: 1.1100\n",
      "Epoch [29/100], Loss: 1.1048\n",
      "Epoch [30/100], Loss: 1.1026\n",
      "Epoch [31/100], Loss: 1.1026\n",
      "Epoch [32/100], Loss: 1.0958\n",
      "Epoch [33/100], Loss: 1.0955\n",
      "Epoch [34/100], Loss: 1.0935\n",
      "Epoch [35/100], Loss: 1.0879\n",
      "Epoch [36/100], Loss: 1.0892\n",
      "Epoch [37/100], Loss: 1.0858\n",
      "Epoch [38/100], Loss: 1.0852\n",
      "Epoch [39/100], Loss: 1.0839\n",
      "Epoch [40/100], Loss: 1.0818\n",
      "Epoch [41/100], Loss: 1.0816\n",
      "Epoch [42/100], Loss: 1.0799\n",
      "Epoch [43/100], Loss: 1.0792\n",
      "Epoch [44/100], Loss: 1.0733\n",
      "Epoch [45/100], Loss: 1.0754\n",
      "Epoch [46/100], Loss: 1.0722\n",
      "Epoch [47/100], Loss: 1.0700\n",
      "Epoch [48/100], Loss: 1.0707\n",
      "Epoch [49/100], Loss: 1.0698\n",
      "Epoch [50/100], Loss: 1.0675\n",
      "Epoch [51/100], Loss: 1.0653\n",
      "Epoch [52/100], Loss: 1.0662\n",
      "Epoch [53/100], Loss: 1.0626\n",
      "Epoch [54/100], Loss: 1.0641\n",
      "Epoch [55/100], Loss: 1.0614\n",
      "Epoch [56/100], Loss: 1.0587\n",
      "Epoch [57/100], Loss: 1.0596\n",
      "Epoch [58/100], Loss: 1.0556\n",
      "Epoch [59/100], Loss: 1.0568\n",
      "Epoch [60/100], Loss: 1.0559\n",
      "Epoch [61/100], Loss: 1.0537\n",
      "Epoch [62/100], Loss: 1.0536\n",
      "Epoch [63/100], Loss: 1.0515\n",
      "Epoch [64/100], Loss: 1.0523\n",
      "Epoch [65/100], Loss: 1.0533\n",
      "Epoch [66/100], Loss: 1.0498\n",
      "Epoch [67/100], Loss: 1.0470\n",
      "Epoch [68/100], Loss: 1.0499\n",
      "Epoch [69/100], Loss: 1.0475\n",
      "Epoch [70/100], Loss: 1.0493\n",
      "Epoch [71/100], Loss: 1.0464\n",
      "Epoch [72/100], Loss: 1.0454\n",
      "Epoch [73/100], Loss: 1.0446\n",
      "Epoch [74/100], Loss: 1.0416\n",
      "Epoch [75/100], Loss: 1.0392\n",
      "Epoch [76/100], Loss: 1.0415\n",
      "Epoch [77/100], Loss: 1.0399\n",
      "Epoch [78/100], Loss: 1.0415\n",
      "Epoch [79/100], Loss: 1.0392\n",
      "Epoch [80/100], Loss: 1.0391\n",
      "Epoch [81/100], Loss: 1.0385\n",
      "Epoch [82/100], Loss: 1.0358\n",
      "Epoch [83/100], Loss: 1.0359\n",
      "Epoch [84/100], Loss: 1.0350\n",
      "Epoch [85/100], Loss: 1.0369\n",
      "Stopping early at epoch 85 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [85/100], Loss: 1.0369\n",
      "Test Accuracy Base Logit: 54.06%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.7155\n",
      "Epoch [2/100], Loss: 1.3590\n",
      "Epoch [3/100], Loss: 1.2958\n",
      "Epoch [4/100], Loss: 1.2629\n",
      "Epoch [5/100], Loss: 1.2431\n",
      "Epoch [6/100], Loss: 1.2263\n",
      "Epoch [7/100], Loss: 1.2112\n",
      "Epoch [8/100], Loss: 1.2017\n",
      "Epoch [9/100], Loss: 1.1885\n",
      "Epoch [10/100], Loss: 1.1806\n",
      "Epoch [11/100], Loss: 1.1745\n",
      "Epoch [12/100], Loss: 1.1659\n",
      "Epoch [13/100], Loss: 1.1623\n",
      "Epoch [14/100], Loss: 1.1532\n",
      "Epoch [15/100], Loss: 1.1499\n",
      "Epoch [16/100], Loss: 1.1448\n",
      "Epoch [17/100], Loss: 1.1406\n",
      "Epoch [18/100], Loss: 1.1386\n",
      "Epoch [19/100], Loss: 1.1314\n",
      "Epoch [20/100], Loss: 1.1285\n",
      "Epoch [21/100], Loss: 1.1245\n",
      "Epoch [22/100], Loss: 1.1198\n",
      "Epoch [23/100], Loss: 1.1190\n",
      "Epoch [24/100], Loss: 1.1156\n",
      "Epoch [25/100], Loss: 1.1121\n",
      "Epoch [26/100], Loss: 1.1093\n",
      "Epoch [27/100], Loss: 1.1069\n",
      "Epoch [28/100], Loss: 1.1028\n",
      "Epoch [29/100], Loss: 1.0991\n",
      "Epoch [30/100], Loss: 1.1010\n",
      "Epoch [31/100], Loss: 1.0964\n",
      "Epoch [32/100], Loss: 1.0947\n",
      "Epoch [33/100], Loss: 1.0919\n",
      "Epoch [34/100], Loss: 1.0918\n",
      "Epoch [35/100], Loss: 1.0891\n",
      "Epoch [36/100], Loss: 1.0851\n",
      "Epoch [37/100], Loss: 1.0839\n",
      "Epoch [38/100], Loss: 1.0821\n",
      "Epoch [39/100], Loss: 1.0806\n",
      "Epoch [40/100], Loss: 1.0789\n",
      "Epoch [41/100], Loss: 1.0764\n",
      "Epoch [42/100], Loss: 1.0767\n",
      "Epoch [43/100], Loss: 1.0713\n",
      "Epoch [44/100], Loss: 1.0742\n",
      "Epoch [45/100], Loss: 1.0718\n",
      "Epoch [46/100], Loss: 1.0702\n",
      "Epoch [47/100], Loss: 1.0685\n",
      "Epoch [48/100], Loss: 1.0659\n",
      "Epoch [49/100], Loss: 1.0640\n",
      "Epoch [50/100], Loss: 1.0650\n",
      "Epoch [51/100], Loss: 1.0587\n",
      "Epoch [52/100], Loss: 1.0635\n",
      "Epoch [53/100], Loss: 1.0616\n",
      "Epoch [54/100], Loss: 1.0604\n",
      "Epoch [55/100], Loss: 1.0584\n",
      "Epoch [56/100], Loss: 1.0588\n",
      "Epoch [57/100], Loss: 1.0563\n",
      "Epoch [58/100], Loss: 1.0545\n",
      "Epoch [59/100], Loss: 1.0520\n",
      "Epoch [60/100], Loss: 1.0530\n",
      "Epoch [61/100], Loss: 1.0526\n",
      "Epoch [62/100], Loss: 1.0503\n",
      "Epoch [63/100], Loss: 1.0491\n",
      "Epoch [64/100], Loss: 1.0500\n",
      "Epoch [65/100], Loss: 1.0472\n",
      "Epoch [66/100], Loss: 1.0449\n",
      "Epoch [67/100], Loss: 1.0458\n",
      "Epoch [68/100], Loss: 1.0451\n",
      "Epoch [69/100], Loss: 1.0446\n",
      "Stopping early at epoch 69 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [69/100], Loss: 1.0446\n",
      "Test Accuracy Base Logit: 54.10%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.7280\n",
      "Epoch [2/100], Loss: 1.3712\n",
      "Epoch [3/100], Loss: 1.3078\n",
      "Epoch [4/100], Loss: 1.2758\n",
      "Epoch [5/100], Loss: 1.2509\n",
      "Epoch [6/100], Loss: 1.2347\n",
      "Epoch [7/100], Loss: 1.2222\n",
      "Epoch [8/100], Loss: 1.2085\n",
      "Epoch [9/100], Loss: 1.2028\n",
      "Epoch [10/100], Loss: 1.1906\n",
      "Epoch [11/100], Loss: 1.1860\n",
      "Epoch [12/100], Loss: 1.1785\n",
      "Epoch [13/100], Loss: 1.1727\n",
      "Epoch [14/100], Loss: 1.1680\n",
      "Epoch [15/100], Loss: 1.1599\n",
      "Epoch [16/100], Loss: 1.1579\n",
      "Epoch [17/100], Loss: 1.1519\n",
      "Epoch [18/100], Loss: 1.1483\n",
      "Epoch [19/100], Loss: 1.1458\n",
      "Epoch [20/100], Loss: 1.1429\n",
      "Epoch [21/100], Loss: 1.1342\n",
      "Epoch [22/100], Loss: 1.1322\n",
      "Epoch [23/100], Loss: 1.1293\n",
      "Epoch [24/100], Loss: 1.1282\n",
      "Epoch [25/100], Loss: 1.1258\n",
      "Epoch [26/100], Loss: 1.1223\n",
      "Epoch [27/100], Loss: 1.1180\n",
      "Epoch [28/100], Loss: 1.1163\n",
      "Epoch [29/100], Loss: 1.1137\n",
      "Epoch [30/100], Loss: 1.1103\n",
      "Epoch [31/100], Loss: 1.1097\n",
      "Epoch [32/100], Loss: 1.1082\n",
      "Epoch [33/100], Loss: 1.1053\n",
      "Epoch [34/100], Loss: 1.1055\n",
      "Epoch [35/100], Loss: 1.0995\n",
      "Epoch [36/100], Loss: 1.1031\n",
      "Epoch [37/100], Loss: 1.0974\n",
      "Epoch [38/100], Loss: 1.0957\n",
      "Epoch [39/100], Loss: 1.0949\n",
      "Epoch [40/100], Loss: 1.0912\n",
      "Epoch [41/100], Loss: 1.0896\n",
      "Epoch [42/100], Loss: 1.0884\n",
      "Epoch [43/100], Loss: 1.0868\n",
      "Epoch [44/100], Loss: 1.0832\n",
      "Epoch [45/100], Loss: 1.0841\n",
      "Epoch [46/100], Loss: 1.0831\n",
      "Epoch [47/100], Loss: 1.0828\n",
      "Stopping early at epoch 47 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [47/100], Loss: 1.0828\n",
      "Test Accuracy Base Logit: 54.60%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.7330\n",
      "Epoch [2/100], Loss: 1.3743\n",
      "Epoch [3/100], Loss: 1.3147\n",
      "Epoch [4/100], Loss: 1.2805\n",
      "Epoch [5/100], Loss: 1.2596\n",
      "Epoch [6/100], Loss: 1.2426\n",
      "Epoch [7/100], Loss: 1.2285\n",
      "Epoch [8/100], Loss: 1.2183\n",
      "Epoch [9/100], Loss: 1.2093\n",
      "Epoch [10/100], Loss: 1.1970\n",
      "Epoch [11/100], Loss: 1.1898\n",
      "Epoch [12/100], Loss: 1.1830\n",
      "Epoch [13/100], Loss: 1.1779\n",
      "Epoch [14/100], Loss: 1.1743\n",
      "Epoch [15/100], Loss: 1.1689\n",
      "Epoch [16/100], Loss: 1.1635\n",
      "Epoch [17/100], Loss: 1.1582\n",
      "Epoch [18/100], Loss: 1.1536\n",
      "Epoch [19/100], Loss: 1.1475\n",
      "Epoch [20/100], Loss: 1.1463\n",
      "Epoch [21/100], Loss: 1.1453\n",
      "Epoch [22/100], Loss: 1.1411\n",
      "Epoch [23/100], Loss: 1.1373\n",
      "Epoch [24/100], Loss: 1.1320\n",
      "Epoch [25/100], Loss: 1.1302\n",
      "Epoch [26/100], Loss: 1.1279\n",
      "Epoch [27/100], Loss: 1.1254\n",
      "Epoch [28/100], Loss: 1.1243\n",
      "Epoch [29/100], Loss: 1.1199\n",
      "Epoch [30/100], Loss: 1.1176\n",
      "Epoch [31/100], Loss: 1.1134\n",
      "Epoch [32/100], Loss: 1.1150\n",
      "Epoch [33/100], Loss: 1.1115\n",
      "Epoch [34/100], Loss: 1.1099\n",
      "Epoch [35/100], Loss: 1.1072\n",
      "Epoch [36/100], Loss: 1.1067\n",
      "Epoch [37/100], Loss: 1.1028\n",
      "Epoch [38/100], Loss: 1.0999\n",
      "Epoch [39/100], Loss: 1.0981\n",
      "Epoch [40/100], Loss: 1.1005\n",
      "Epoch [41/100], Loss: 1.0958\n",
      "Epoch [42/100], Loss: 1.0939\n",
      "Epoch [43/100], Loss: 1.0945\n",
      "Epoch [44/100], Loss: 1.0903\n",
      "Epoch [45/100], Loss: 1.0901\n",
      "Epoch [46/100], Loss: 1.0880\n",
      "Epoch [47/100], Loss: 1.0867\n",
      "Epoch [48/100], Loss: 1.0867\n",
      "Epoch [49/100], Loss: 1.0833\n",
      "Epoch [50/100], Loss: 1.0845\n",
      "Epoch [51/100], Loss: 1.0825\n",
      "Epoch [52/100], Loss: 1.0806\n",
      "Epoch [53/100], Loss: 1.0824\n",
      "Epoch [54/100], Loss: 1.0782\n",
      "Epoch [55/100], Loss: 1.0764\n",
      "Epoch [56/100], Loss: 1.0765\n",
      "Epoch [57/100], Loss: 1.0763\n",
      "Epoch [58/100], Loss: 1.0748\n",
      "Epoch [59/100], Loss: 1.0727\n",
      "Epoch [60/100], Loss: 1.0716\n",
      "Epoch [61/100], Loss: 1.0711\n",
      "Epoch [62/100], Loss: 1.0690\n",
      "Epoch [63/100], Loss: 1.0694\n",
      "Epoch [64/100], Loss: 1.0685\n",
      "Epoch [65/100], Loss: 1.0661\n",
      "Epoch [66/100], Loss: 1.0652\n",
      "Epoch [67/100], Loss: 1.0629\n",
      "Epoch [68/100], Loss: 1.0648\n",
      "Epoch [69/100], Loss: 1.0646\n",
      "Epoch [70/100], Loss: 1.0641\n",
      "Stopping early at epoch 70 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [70/100], Loss: 1.0641\n",
      "Test Accuracy Base Logit: 53.78%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.7340\n",
      "Epoch [2/100], Loss: 1.3738\n",
      "Epoch [3/100], Loss: 1.3133\n",
      "Epoch [4/100], Loss: 1.2865\n",
      "Epoch [5/100], Loss: 1.2641\n",
      "Epoch [6/100], Loss: 1.2460\n",
      "Epoch [7/100], Loss: 1.2320\n",
      "Epoch [8/100], Loss: 1.2194\n",
      "Epoch [9/100], Loss: 1.2112\n",
      "Epoch [10/100], Loss: 1.1990\n",
      "Epoch [11/100], Loss: 1.1934\n",
      "Epoch [12/100], Loss: 1.1891\n",
      "Epoch [13/100], Loss: 1.1824\n",
      "Epoch [14/100], Loss: 1.1739\n",
      "Epoch [15/100], Loss: 1.1682\n",
      "Epoch [16/100], Loss: 1.1643\n",
      "Epoch [17/100], Loss: 1.1620\n",
      "Epoch [18/100], Loss: 1.1551\n",
      "Epoch [19/100], Loss: 1.1517\n",
      "Epoch [20/100], Loss: 1.1470\n",
      "Epoch [21/100], Loss: 1.1445\n",
      "Epoch [22/100], Loss: 1.1410\n",
      "Epoch [23/100], Loss: 1.1402\n",
      "Epoch [24/100], Loss: 1.1351\n",
      "Epoch [25/100], Loss: 1.1302\n",
      "Epoch [26/100], Loss: 1.1291\n",
      "Epoch [27/100], Loss: 1.1271\n",
      "Epoch [28/100], Loss: 1.1261\n",
      "Epoch [29/100], Loss: 1.1240\n",
      "Epoch [30/100], Loss: 1.1209\n",
      "Epoch [31/100], Loss: 1.1169\n",
      "Epoch [32/100], Loss: 1.1121\n",
      "Epoch [33/100], Loss: 1.1139\n",
      "Epoch [34/100], Loss: 1.1136\n",
      "Epoch [35/100], Loss: 1.1072\n",
      "Epoch [36/100], Loss: 1.1065\n",
      "Epoch [37/100], Loss: 1.1044\n",
      "Epoch [38/100], Loss: 1.1032\n",
      "Epoch [39/100], Loss: 1.0986\n",
      "Epoch [40/100], Loss: 1.0971\n",
      "Epoch [41/100], Loss: 1.0976\n",
      "Epoch [42/100], Loss: 1.0961\n",
      "Epoch [43/100], Loss: 1.0946\n",
      "Epoch [44/100], Loss: 1.0914\n",
      "Epoch [45/100], Loss: 1.0919\n",
      "Epoch [46/100], Loss: 1.0893\n",
      "Epoch [47/100], Loss: 1.0895\n",
      "Epoch [48/100], Loss: 1.0859\n",
      "Epoch [49/100], Loss: 1.0851\n",
      "Epoch [50/100], Loss: 1.0839\n",
      "Epoch [51/100], Loss: 1.0820\n",
      "Epoch [52/100], Loss: 1.0812\n",
      "Epoch [53/100], Loss: 1.0813\n",
      "Epoch [54/100], Loss: 1.0804\n",
      "Stopping early at epoch 54 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [54/100], Loss: 1.0804\n",
      "Test Accuracy Base Logit: 54.13%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 2.4529\n",
      "Epoch [2/100], Loss: 1.6088\n",
      "Epoch [3/100], Loss: 1.4505\n",
      "Epoch [4/100], Loss: 1.3546\n",
      "Epoch [5/100], Loss: 1.2899\n",
      "Epoch [6/100], Loss: 1.2581\n",
      "Epoch [7/100], Loss: 1.2032\n",
      "Epoch [8/100], Loss: 1.1742\n",
      "Epoch [9/100], Loss: 1.1524\n",
      "Epoch [10/100], Loss: 1.1208\n",
      "Epoch [11/100], Loss: 1.1054\n",
      "Epoch [12/100], Loss: 1.0836\n",
      "Epoch [13/100], Loss: 1.0722\n",
      "Epoch [14/100], Loss: 1.0496\n",
      "Epoch [15/100], Loss: 1.0494\n",
      "Epoch [16/100], Loss: 1.0298\n",
      "Epoch [17/100], Loss: 1.0041\n",
      "Epoch [18/100], Loss: 1.0043\n",
      "Epoch [19/100], Loss: 0.9828\n",
      "Epoch [20/100], Loss: 0.9745\n",
      "Epoch [21/100], Loss: 0.9595\n",
      "Epoch [22/100], Loss: 0.9489\n",
      "Epoch [23/100], Loss: 0.9393\n",
      "Epoch [24/100], Loss: 0.9350\n",
      "Epoch [25/100], Loss: 0.9201\n",
      "Epoch [26/100], Loss: 0.9182\n",
      "Epoch [27/100], Loss: 0.9148\n",
      "Epoch [28/100], Loss: 0.8992\n",
      "Epoch [29/100], Loss: 0.8846\n",
      "Epoch [30/100], Loss: 0.8785\n",
      "Epoch [31/100], Loss: 0.8771\n",
      "Epoch [32/100], Loss: 0.8643\n",
      "Epoch [33/100], Loss: 0.8599\n",
      "Epoch [34/100], Loss: 0.8476\n",
      "Epoch [35/100], Loss: 0.8385\n",
      "Epoch [36/100], Loss: 0.8371\n",
      "Epoch [37/100], Loss: 0.8337\n",
      "Epoch [38/100], Loss: 0.8313\n",
      "Epoch [39/100], Loss: 0.8211\n",
      "Epoch [40/100], Loss: 0.8159\n",
      "Epoch [41/100], Loss: 0.8119\n",
      "Epoch [42/100], Loss: 0.8000\n",
      "Epoch [43/100], Loss: 0.7999\n",
      "Epoch [44/100], Loss: 0.7869\n",
      "Epoch [45/100], Loss: 0.7862\n",
      "Epoch [46/100], Loss: 0.7829\n",
      "Epoch [47/100], Loss: 0.7861\n",
      "Epoch [48/100], Loss: 0.7757\n",
      "Epoch [49/100], Loss: 0.7652\n",
      "Epoch [50/100], Loss: 0.7703\n",
      "Epoch [51/100], Loss: 0.7600\n",
      "Epoch [52/100], Loss: 0.7593\n",
      "Epoch [53/100], Loss: 0.7472\n",
      "Epoch [54/100], Loss: 0.7476\n",
      "Epoch [55/100], Loss: 0.7412\n",
      "Epoch [56/100], Loss: 0.7389\n",
      "Epoch [57/100], Loss: 0.7427\n",
      "Epoch [58/100], Loss: 0.7316\n",
      "Epoch [59/100], Loss: 0.7248\n",
      "Epoch [60/100], Loss: 0.7231\n",
      "Epoch [61/100], Loss: 0.7234\n",
      "Epoch [62/100], Loss: 0.7160\n",
      "Epoch [63/100], Loss: 0.7174\n",
      "Epoch [64/100], Loss: 0.7108\n",
      "Epoch [65/100], Loss: 0.7036\n",
      "Epoch [66/100], Loss: 0.7042\n",
      "Epoch [67/100], Loss: 0.6982\n",
      "Epoch [68/100], Loss: 0.6946\n",
      "Epoch [69/100], Loss: 0.6952\n",
      "Epoch [70/100], Loss: 0.6953\n",
      "Epoch [71/100], Loss: 0.6920\n",
      "Epoch [72/100], Loss: 0.6912\n",
      "Epoch [73/100], Loss: 0.6836\n",
      "Epoch [74/100], Loss: 0.6804\n",
      "Epoch [75/100], Loss: 0.6789\n",
      "Epoch [76/100], Loss: 0.6719\n",
      "Epoch [77/100], Loss: 0.6650\n",
      "Epoch [78/100], Loss: 0.6677\n",
      "Epoch [79/100], Loss: 0.6563\n",
      "Epoch [80/100], Loss: 0.6551\n",
      "Epoch [81/100], Loss: 0.6524\n",
      "Epoch [82/100], Loss: 0.6561\n",
      "Epoch [83/100], Loss: 0.6520\n",
      "Epoch [84/100], Loss: 0.6510\n",
      "Epoch [85/100], Loss: 0.6509\n",
      "Epoch [86/100], Loss: 0.6513\n",
      "Epoch [87/100], Loss: 0.6508\n",
      "Stopping early at epoch 87 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [87/100], Loss: 0.6508\n",
      "Test Accuracy Base Logit: 50.08%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 2.4500\n",
      "Epoch [2/100], Loss: 1.6221\n",
      "Epoch [3/100], Loss: 1.4468\n",
      "Epoch [4/100], Loss: 1.3725\n",
      "Epoch [5/100], Loss: 1.2955\n",
      "Epoch [6/100], Loss: 1.2421\n",
      "Epoch [7/100], Loss: 1.2162\n",
      "Epoch [8/100], Loss: 1.1787\n",
      "Epoch [9/100], Loss: 1.1473\n",
      "Epoch [10/100], Loss: 1.1372\n",
      "Epoch [11/100], Loss: 1.1022\n",
      "Epoch [12/100], Loss: 1.0784\n",
      "Epoch [13/100], Loss: 1.0655\n",
      "Epoch [14/100], Loss: 1.0476\n",
      "Epoch [15/100], Loss: 1.0272\n",
      "Epoch [16/100], Loss: 1.0133\n",
      "Epoch [17/100], Loss: 1.0045\n",
      "Epoch [18/100], Loss: 0.9886\n",
      "Epoch [19/100], Loss: 0.9718\n",
      "Epoch [20/100], Loss: 0.9628\n",
      "Epoch [21/100], Loss: 0.9508\n",
      "Epoch [22/100], Loss: 0.9517\n",
      "Epoch [23/100], Loss: 0.9401\n",
      "Epoch [24/100], Loss: 0.9213\n",
      "Epoch [25/100], Loss: 0.9177\n",
      "Epoch [26/100], Loss: 0.9117\n",
      "Epoch [27/100], Loss: 0.9079\n",
      "Epoch [28/100], Loss: 0.8968\n",
      "Epoch [29/100], Loss: 0.8821\n",
      "Epoch [30/100], Loss: 0.8788\n",
      "Epoch [31/100], Loss: 0.8619\n",
      "Epoch [32/100], Loss: 0.8618\n",
      "Epoch [33/100], Loss: 0.8569\n",
      "Epoch [34/100], Loss: 0.8575\n",
      "Epoch [35/100], Loss: 0.8537\n",
      "Epoch [36/100], Loss: 0.8523\n",
      "Epoch [37/100], Loss: 0.8348\n",
      "Epoch [38/100], Loss: 0.8283\n",
      "Epoch [39/100], Loss: 0.8261\n",
      "Epoch [40/100], Loss: 0.8149\n",
      "Epoch [41/100], Loss: 0.8045\n",
      "Epoch [42/100], Loss: 0.8010\n",
      "Epoch [43/100], Loss: 0.8026\n",
      "Epoch [44/100], Loss: 0.7943\n",
      "Epoch [45/100], Loss: 0.7866\n",
      "Epoch [46/100], Loss: 0.7882\n",
      "Epoch [47/100], Loss: 0.7877\n",
      "Epoch [48/100], Loss: 0.7806\n",
      "Epoch [49/100], Loss: 0.7690\n",
      "Epoch [50/100], Loss: 0.7628\n",
      "Epoch [51/100], Loss: 0.7576\n",
      "Epoch [52/100], Loss: 0.7598\n",
      "Epoch [53/100], Loss: 0.7477\n",
      "Epoch [54/100], Loss: 0.7466\n",
      "Epoch [55/100], Loss: 0.7392\n",
      "Epoch [56/100], Loss: 0.7462\n",
      "Epoch [57/100], Loss: 0.7376\n",
      "Epoch [58/100], Loss: 0.7317\n",
      "Epoch [59/100], Loss: 0.7337\n",
      "Epoch [60/100], Loss: 0.7171\n",
      "Epoch [61/100], Loss: 0.7199\n",
      "Epoch [62/100], Loss: 0.7135\n",
      "Epoch [63/100], Loss: 0.7088\n",
      "Epoch [64/100], Loss: 0.7134\n",
      "Epoch [65/100], Loss: 0.7066\n",
      "Epoch [66/100], Loss: 0.6987\n",
      "Epoch [67/100], Loss: 0.7001\n",
      "Epoch [68/100], Loss: 0.6967\n",
      "Epoch [69/100], Loss: 0.6899\n",
      "Epoch [70/100], Loss: 0.6863\n",
      "Epoch [71/100], Loss: 0.6847\n",
      "Epoch [72/100], Loss: 0.6951\n",
      "Epoch [73/100], Loss: 0.6826\n",
      "Epoch [74/100], Loss: 0.6725\n",
      "Epoch [75/100], Loss: 0.6690\n",
      "Epoch [76/100], Loss: 0.6668\n",
      "Epoch [77/100], Loss: 0.6619\n",
      "Epoch [78/100], Loss: 0.6576\n",
      "Epoch [79/100], Loss: 0.6548\n",
      "Epoch [80/100], Loss: 0.6576\n",
      "Epoch [81/100], Loss: 0.6555\n",
      "Epoch [82/100], Loss: 0.6537\n",
      "Epoch [83/100], Loss: 0.6552\n",
      "Epoch [84/100], Loss: 0.6464\n",
      "Epoch [85/100], Loss: 0.6401\n",
      "Epoch [86/100], Loss: 0.6451\n",
      "Epoch [87/100], Loss: 0.6511\n",
      "Epoch [88/100], Loss: 0.6361\n",
      "Epoch [89/100], Loss: 0.6352\n",
      "Epoch [90/100], Loss: 0.6423\n",
      "Epoch [91/100], Loss: 0.6232\n",
      "Epoch [92/100], Loss: 0.6287\n",
      "Epoch [93/100], Loss: 0.6228\n",
      "Epoch [94/100], Loss: 0.6213\n",
      "Epoch [95/100], Loss: 0.6250\n",
      "Epoch [96/100], Loss: 0.6281\n",
      "Epoch [97/100], Loss: 0.6110\n",
      "Epoch [98/100], Loss: 0.6253\n",
      "Epoch [99/100], Loss: 0.6096\n",
      "Epoch [100/100], Loss: 0.6108\n",
      "Subset 10000, Epoch [100/100], Loss: 0.6108\n",
      "Test Accuracy Base Logit: 49.15%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 2.4353\n",
      "Epoch [2/100], Loss: 1.5975\n",
      "Epoch [3/100], Loss: 1.4364\n",
      "Epoch [4/100], Loss: 1.3579\n",
      "Epoch [5/100], Loss: 1.2932\n",
      "Epoch [6/100], Loss: 1.2418\n",
      "Epoch [7/100], Loss: 1.2123\n",
      "Epoch [8/100], Loss: 1.1838\n",
      "Epoch [9/100], Loss: 1.1506\n",
      "Epoch [10/100], Loss: 1.1378\n",
      "Epoch [11/100], Loss: 1.1118\n",
      "Epoch [12/100], Loss: 1.0847\n",
      "Epoch [13/100], Loss: 1.0641\n",
      "Epoch [14/100], Loss: 1.0573\n",
      "Epoch [15/100], Loss: 1.0359\n",
      "Epoch [16/100], Loss: 1.0384\n",
      "Epoch [17/100], Loss: 1.0224\n",
      "Epoch [18/100], Loss: 1.0119\n",
      "Epoch [19/100], Loss: 0.9901\n",
      "Epoch [20/100], Loss: 0.9805\n",
      "Epoch [21/100], Loss: 0.9688\n",
      "Epoch [22/100], Loss: 0.9742\n",
      "Epoch [23/100], Loss: 0.9504\n",
      "Epoch [24/100], Loss: 0.9432\n",
      "Epoch [25/100], Loss: 0.9345\n",
      "Epoch [26/100], Loss: 0.9225\n",
      "Epoch [27/100], Loss: 0.9122\n",
      "Epoch [28/100], Loss: 0.9098\n",
      "Epoch [29/100], Loss: 0.8996\n",
      "Epoch [30/100], Loss: 0.8863\n",
      "Epoch [31/100], Loss: 0.8870\n",
      "Epoch [32/100], Loss: 0.8833\n",
      "Epoch [33/100], Loss: 0.8675\n",
      "Epoch [34/100], Loss: 0.8607\n",
      "Epoch [35/100], Loss: 0.8538\n",
      "Epoch [36/100], Loss: 0.8502\n",
      "Epoch [37/100], Loss: 0.8510\n",
      "Epoch [38/100], Loss: 0.8452\n",
      "Epoch [39/100], Loss: 0.8260\n",
      "Epoch [40/100], Loss: 0.8258\n",
      "Epoch [41/100], Loss: 0.8265\n",
      "Epoch [42/100], Loss: 0.8210\n",
      "Epoch [43/100], Loss: 0.8109\n",
      "Epoch [44/100], Loss: 0.8021\n",
      "Epoch [45/100], Loss: 0.8049\n",
      "Epoch [46/100], Loss: 0.8056\n",
      "Epoch [47/100], Loss: 0.7961\n",
      "Epoch [48/100], Loss: 0.7875\n",
      "Epoch [49/100], Loss: 0.7887\n",
      "Epoch [50/100], Loss: 0.7777\n",
      "Epoch [51/100], Loss: 0.7721\n",
      "Epoch [52/100], Loss: 0.7799\n",
      "Epoch [53/100], Loss: 0.7657\n",
      "Epoch [54/100], Loss: 0.7578\n",
      "Epoch [55/100], Loss: 0.7567\n",
      "Epoch [56/100], Loss: 0.7496\n",
      "Epoch [57/100], Loss: 0.7510\n",
      "Epoch [58/100], Loss: 0.7400\n",
      "Epoch [59/100], Loss: 0.7466\n",
      "Epoch [60/100], Loss: 0.7362\n",
      "Epoch [61/100], Loss: 0.7385\n",
      "Epoch [62/100], Loss: 0.7297\n",
      "Epoch [63/100], Loss: 0.7291\n",
      "Epoch [64/100], Loss: 0.7253\n",
      "Epoch [65/100], Loss: 0.7164\n",
      "Epoch [66/100], Loss: 0.7157\n",
      "Epoch [67/100], Loss: 0.7139\n",
      "Epoch [68/100], Loss: 0.7111\n",
      "Epoch [69/100], Loss: 0.7046\n",
      "Epoch [70/100], Loss: 0.7095\n",
      "Epoch [71/100], Loss: 0.7062\n",
      "Epoch [72/100], Loss: 0.6952\n",
      "Epoch [73/100], Loss: 0.6953\n",
      "Epoch [74/100], Loss: 0.6961\n",
      "Epoch [75/100], Loss: 0.6847\n",
      "Epoch [76/100], Loss: 0.6927\n",
      "Epoch [77/100], Loss: 0.6886\n",
      "Epoch [78/100], Loss: 0.6805\n",
      "Epoch [79/100], Loss: 0.6746\n",
      "Epoch [80/100], Loss: 0.6692\n",
      "Epoch [81/100], Loss: 0.6798\n",
      "Epoch [82/100], Loss: 0.6779\n",
      "Epoch [83/100], Loss: 0.6690\n",
      "Epoch [84/100], Loss: 0.6649\n",
      "Epoch [85/100], Loss: 0.6626\n",
      "Epoch [86/100], Loss: 0.6654\n",
      "Epoch [87/100], Loss: 0.6545\n",
      "Epoch [88/100], Loss: 0.6525\n",
      "Epoch [89/100], Loss: 0.6466\n",
      "Epoch [90/100], Loss: 0.6473\n",
      "Epoch [91/100], Loss: 0.6445\n",
      "Epoch [92/100], Loss: 0.6396\n",
      "Epoch [93/100], Loss: 0.6393\n",
      "Epoch [94/100], Loss: 0.6417\n",
      "Epoch [95/100], Loss: 0.6406\n",
      "Epoch [96/100], Loss: 0.6386\n",
      "Epoch [97/100], Loss: 0.6359\n",
      "Epoch [98/100], Loss: 0.6302\n",
      "Epoch [99/100], Loss: 0.6299\n",
      "Epoch [100/100], Loss: 0.6263\n",
      "Subset 10000, Epoch [100/100], Loss: 0.6263\n",
      "Test Accuracy Base Logit: 49.29%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 2.4192\n",
      "Epoch [2/100], Loss: 1.5912\n",
      "Epoch [3/100], Loss: 1.4238\n",
      "Epoch [4/100], Loss: 1.3263\n",
      "Epoch [5/100], Loss: 1.2828\n",
      "Epoch [6/100], Loss: 1.2379\n",
      "Epoch [7/100], Loss: 1.1909\n",
      "Epoch [8/100], Loss: 1.1610\n",
      "Epoch [9/100], Loss: 1.1293\n",
      "Epoch [10/100], Loss: 1.0996\n",
      "Epoch [11/100], Loss: 1.0783\n",
      "Epoch [12/100], Loss: 1.0563\n",
      "Epoch [13/100], Loss: 1.0519\n",
      "Epoch [14/100], Loss: 1.0212\n",
      "Epoch [15/100], Loss: 1.0094\n",
      "Epoch [16/100], Loss: 0.9911\n",
      "Epoch [17/100], Loss: 0.9778\n",
      "Epoch [18/100], Loss: 0.9673\n",
      "Epoch [19/100], Loss: 0.9549\n",
      "Epoch [20/100], Loss: 0.9460\n",
      "Epoch [21/100], Loss: 0.9358\n",
      "Epoch [22/100], Loss: 0.9236\n",
      "Epoch [23/100], Loss: 0.9204\n",
      "Epoch [24/100], Loss: 0.9140\n",
      "Epoch [25/100], Loss: 0.9016\n",
      "Epoch [26/100], Loss: 0.8923\n",
      "Epoch [27/100], Loss: 0.8786\n",
      "Epoch [28/100], Loss: 0.8721\n",
      "Epoch [29/100], Loss: 0.8735\n",
      "Epoch [30/100], Loss: 0.8546\n",
      "Epoch [31/100], Loss: 0.8466\n",
      "Epoch [32/100], Loss: 0.8390\n",
      "Epoch [33/100], Loss: 0.8277\n",
      "Epoch [34/100], Loss: 0.8237\n",
      "Epoch [35/100], Loss: 0.8163\n",
      "Epoch [36/100], Loss: 0.8071\n",
      "Epoch [37/100], Loss: 0.8028\n",
      "Epoch [38/100], Loss: 0.8041\n",
      "Epoch [39/100], Loss: 0.8000\n",
      "Epoch [40/100], Loss: 0.8113\n",
      "Epoch [41/100], Loss: 0.8009\n",
      "Epoch [42/100], Loss: 0.7801\n",
      "Epoch [43/100], Loss: 0.7778\n",
      "Epoch [44/100], Loss: 0.7779\n",
      "Epoch [45/100], Loss: 0.7689\n",
      "Epoch [46/100], Loss: 0.7578\n",
      "Epoch [47/100], Loss: 0.7557\n",
      "Epoch [48/100], Loss: 0.7549\n",
      "Epoch [49/100], Loss: 0.7524\n",
      "Epoch [50/100], Loss: 0.7406\n",
      "Epoch [51/100], Loss: 0.7448\n",
      "Epoch [52/100], Loss: 0.7399\n",
      "Epoch [53/100], Loss: 0.7409\n",
      "Epoch [54/100], Loss: 0.7281\n",
      "Epoch [55/100], Loss: 0.7232\n",
      "Epoch [56/100], Loss: 0.7166\n",
      "Epoch [57/100], Loss: 0.7092\n",
      "Epoch [58/100], Loss: 0.7079\n",
      "Epoch [59/100], Loss: 0.7071\n",
      "Epoch [60/100], Loss: 0.7009\n",
      "Epoch [61/100], Loss: 0.6984\n",
      "Epoch [62/100], Loss: 0.7092\n",
      "Epoch [63/100], Loss: 0.6877\n",
      "Epoch [64/100], Loss: 0.6864\n",
      "Epoch [65/100], Loss: 0.6917\n",
      "Epoch [66/100], Loss: 0.6795\n",
      "Epoch [67/100], Loss: 0.6824\n",
      "Epoch [68/100], Loss: 0.6699\n",
      "Epoch [69/100], Loss: 0.6699\n",
      "Epoch [70/100], Loss: 0.6622\n",
      "Epoch [71/100], Loss: 0.6649\n",
      "Epoch [72/100], Loss: 0.6634\n",
      "Epoch [73/100], Loss: 0.6605\n",
      "Epoch [74/100], Loss: 0.6601\n",
      "Epoch [75/100], Loss: 0.6480\n",
      "Epoch [76/100], Loss: 0.6502\n",
      "Epoch [77/100], Loss: 0.6421\n",
      "Epoch [78/100], Loss: 0.6406\n",
      "Epoch [79/100], Loss: 0.6435\n",
      "Epoch [80/100], Loss: 0.6407\n",
      "Epoch [81/100], Loss: 0.6359\n",
      "Epoch [82/100], Loss: 0.6353\n",
      "Epoch [83/100], Loss: 0.6282\n",
      "Epoch [84/100], Loss: 0.6276\n",
      "Epoch [85/100], Loss: 0.6301\n",
      "Epoch [86/100], Loss: 0.6251\n",
      "Epoch [87/100], Loss: 0.6319\n",
      "Epoch [88/100], Loss: 0.6150\n",
      "Epoch [89/100], Loss: 0.6113\n",
      "Epoch [90/100], Loss: 0.6081\n",
      "Epoch [91/100], Loss: 0.6081\n",
      "Epoch [92/100], Loss: 0.6057\n",
      "Epoch [93/100], Loss: 0.6057\n",
      "Epoch [94/100], Loss: 0.6029\n",
      "Epoch [95/100], Loss: 0.5975\n",
      "Epoch [96/100], Loss: 0.5942\n",
      "Epoch [97/100], Loss: 0.6001\n",
      "Epoch [98/100], Loss: 0.5958\n",
      "Epoch [99/100], Loss: 0.5876\n",
      "Epoch [100/100], Loss: 0.5857\n",
      "Subset 10000, Epoch [100/100], Loss: 0.5857\n",
      "Test Accuracy Base Logit: 49.94%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 2.4643\n",
      "Epoch [2/100], Loss: 1.6285\n",
      "Epoch [3/100], Loss: 1.4660\n",
      "Epoch [4/100], Loss: 1.3796\n",
      "Epoch [5/100], Loss: 1.3194\n",
      "Epoch [6/100], Loss: 1.2737\n",
      "Epoch [7/100], Loss: 1.2262\n",
      "Epoch [8/100], Loss: 1.2085\n",
      "Epoch [9/100], Loss: 1.1763\n",
      "Epoch [10/100], Loss: 1.1468\n",
      "Epoch [11/100], Loss: 1.1318\n",
      "Epoch [12/100], Loss: 1.1053\n",
      "Epoch [13/100], Loss: 1.0957\n",
      "Epoch [14/100], Loss: 1.0701\n",
      "Epoch [15/100], Loss: 1.0644\n",
      "Epoch [16/100], Loss: 1.0452\n",
      "Epoch [17/100], Loss: 1.0309\n",
      "Epoch [18/100], Loss: 1.0159\n",
      "Epoch [19/100], Loss: 1.0012\n",
      "Epoch [20/100], Loss: 0.9922\n",
      "Epoch [21/100], Loss: 0.9846\n",
      "Epoch [22/100], Loss: 0.9702\n",
      "Epoch [23/100], Loss: 0.9577\n",
      "Epoch [24/100], Loss: 0.9535\n",
      "Epoch [25/100], Loss: 0.9412\n",
      "Epoch [26/100], Loss: 0.9518\n",
      "Epoch [27/100], Loss: 0.9347\n",
      "Epoch [28/100], Loss: 0.9182\n",
      "Epoch [29/100], Loss: 0.9112\n",
      "Epoch [30/100], Loss: 0.8974\n",
      "Epoch [31/100], Loss: 0.8896\n",
      "Epoch [32/100], Loss: 0.8891\n",
      "Epoch [33/100], Loss: 0.8848\n",
      "Epoch [34/100], Loss: 0.8787\n",
      "Epoch [35/100], Loss: 0.8664\n",
      "Epoch [36/100], Loss: 0.8647\n",
      "Epoch [37/100], Loss: 0.8572\n",
      "Epoch [38/100], Loss: 0.8442\n",
      "Epoch [39/100], Loss: 0.8459\n",
      "Epoch [40/100], Loss: 0.8422\n",
      "Epoch [41/100], Loss: 0.8379\n",
      "Epoch [42/100], Loss: 0.8216\n",
      "Epoch [43/100], Loss: 0.8148\n",
      "Epoch [44/100], Loss: 0.8237\n",
      "Epoch [45/100], Loss: 0.8264\n",
      "Epoch [46/100], Loss: 0.8185\n",
      "Epoch [47/100], Loss: 0.8038\n",
      "Epoch [48/100], Loss: 0.7969\n",
      "Epoch [49/100], Loss: 0.7939\n",
      "Epoch [50/100], Loss: 0.7866\n",
      "Epoch [51/100], Loss: 0.7806\n",
      "Epoch [52/100], Loss: 0.7843\n",
      "Epoch [53/100], Loss: 0.7665\n",
      "Epoch [54/100], Loss: 0.7650\n",
      "Epoch [55/100], Loss: 0.7608\n",
      "Epoch [56/100], Loss: 0.7552\n",
      "Epoch [57/100], Loss: 0.7529\n",
      "Epoch [58/100], Loss: 0.7582\n",
      "Epoch [59/100], Loss: 0.7459\n",
      "Epoch [60/100], Loss: 0.7391\n",
      "Epoch [61/100], Loss: 0.7421\n",
      "Epoch [62/100], Loss: 0.7367\n",
      "Epoch [63/100], Loss: 0.7379\n",
      "Epoch [64/100], Loss: 0.7284\n",
      "Epoch [65/100], Loss: 0.7239\n",
      "Epoch [66/100], Loss: 0.7212\n",
      "Epoch [67/100], Loss: 0.7267\n",
      "Epoch [68/100], Loss: 0.7149\n",
      "Epoch [69/100], Loss: 0.7113\n",
      "Epoch [70/100], Loss: 0.7089\n",
      "Epoch [71/100], Loss: 0.7121\n",
      "Epoch [72/100], Loss: 0.7027\n",
      "Epoch [73/100], Loss: 0.7020\n",
      "Epoch [74/100], Loss: 0.7068\n",
      "Epoch [75/100], Loss: 0.6925\n",
      "Epoch [76/100], Loss: 0.6929\n",
      "Epoch [77/100], Loss: 0.6979\n",
      "Epoch [78/100], Loss: 0.6924\n",
      "Epoch [79/100], Loss: 0.6797\n",
      "Epoch [80/100], Loss: 0.6815\n",
      "Epoch [81/100], Loss: 0.6723\n",
      "Epoch [82/100], Loss: 0.6736\n",
      "Epoch [83/100], Loss: 0.6670\n",
      "Epoch [84/100], Loss: 0.6676\n",
      "Epoch [85/100], Loss: 0.6733\n",
      "Epoch [86/100], Loss: 0.6588\n",
      "Epoch [87/100], Loss: 0.6593\n",
      "Epoch [88/100], Loss: 0.6646\n",
      "Epoch [89/100], Loss: 0.6507\n",
      "Epoch [90/100], Loss: 0.6499\n",
      "Epoch [91/100], Loss: 0.6469\n",
      "Epoch [92/100], Loss: 0.6553\n",
      "Epoch [93/100], Loss: 0.6395\n",
      "Epoch [94/100], Loss: 0.6406\n",
      "Epoch [95/100], Loss: 0.6405\n",
      "Epoch [96/100], Loss: 0.6364\n",
      "Epoch [97/100], Loss: 0.6406\n",
      "Epoch [98/100], Loss: 0.6322\n",
      "Epoch [99/100], Loss: 0.6316\n",
      "Epoch [100/100], Loss: 0.6300\n",
      "Subset 10000, Epoch [100/100], Loss: 0.6300\n",
      "Test Accuracy Base Logit: 50.11%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 2.4477\n",
      "Epoch [2/100], Loss: 1.6221\n",
      "Epoch [3/100], Loss: 1.4551\n",
      "Epoch [4/100], Loss: 1.3741\n",
      "Epoch [5/100], Loss: 1.3136\n",
      "Epoch [6/100], Loss: 1.2600\n",
      "Epoch [7/100], Loss: 1.2283\n",
      "Epoch [8/100], Loss: 1.1988\n",
      "Epoch [9/100], Loss: 1.1684\n",
      "Epoch [10/100], Loss: 1.1403\n",
      "Epoch [11/100], Loss: 1.1148\n",
      "Epoch [12/100], Loss: 1.1106\n",
      "Epoch [13/100], Loss: 1.0989\n",
      "Epoch [14/100], Loss: 1.0759\n",
      "Epoch [15/100], Loss: 1.0608\n",
      "Epoch [16/100], Loss: 1.0488\n",
      "Epoch [17/100], Loss: 1.0326\n",
      "Epoch [18/100], Loss: 1.0148\n",
      "Epoch [19/100], Loss: 1.0029\n",
      "Epoch [20/100], Loss: 0.9926\n",
      "Epoch [21/100], Loss: 0.9823\n",
      "Epoch [22/100], Loss: 0.9737\n",
      "Epoch [23/100], Loss: 0.9649\n",
      "Epoch [24/100], Loss: 0.9573\n",
      "Epoch [25/100], Loss: 0.9506\n",
      "Epoch [26/100], Loss: 0.9344\n",
      "Epoch [27/100], Loss: 0.9246\n",
      "Epoch [28/100], Loss: 0.9307\n",
      "Epoch [29/100], Loss: 0.9131\n",
      "Epoch [30/100], Loss: 0.9066\n",
      "Epoch [31/100], Loss: 0.8953\n",
      "Epoch [32/100], Loss: 0.8921\n",
      "Epoch [33/100], Loss: 0.8810\n",
      "Epoch [34/100], Loss: 0.8789\n",
      "Epoch [35/100], Loss: 0.8737\n",
      "Epoch [36/100], Loss: 0.8648\n",
      "Epoch [37/100], Loss: 0.8598\n",
      "Epoch [38/100], Loss: 0.8517\n",
      "Epoch [39/100], Loss: 0.8479\n",
      "Epoch [40/100], Loss: 0.8440\n",
      "Epoch [41/100], Loss: 0.8343\n",
      "Epoch [42/100], Loss: 0.8252\n",
      "Epoch [43/100], Loss: 0.8209\n",
      "Epoch [44/100], Loss: 0.8196\n",
      "Epoch [45/100], Loss: 0.8165\n",
      "Epoch [46/100], Loss: 0.8155\n",
      "Epoch [47/100], Loss: 0.8144\n",
      "Epoch [48/100], Loss: 0.7965\n",
      "Epoch [49/100], Loss: 0.7916\n",
      "Epoch [50/100], Loss: 0.7914\n",
      "Epoch [51/100], Loss: 0.7835\n",
      "Epoch [52/100], Loss: 0.7809\n",
      "Epoch [53/100], Loss: 0.7781\n",
      "Epoch [54/100], Loss: 0.7791\n",
      "Epoch [55/100], Loss: 0.7635\n",
      "Epoch [56/100], Loss: 0.7673\n",
      "Epoch [57/100], Loss: 0.7696\n",
      "Epoch [58/100], Loss: 0.7610\n",
      "Epoch [59/100], Loss: 0.7539\n",
      "Epoch [60/100], Loss: 0.7518\n",
      "Epoch [61/100], Loss: 0.7456\n",
      "Epoch [62/100], Loss: 0.7453\n",
      "Epoch [63/100], Loss: 0.7423\n",
      "Epoch [64/100], Loss: 0.7385\n",
      "Epoch [65/100], Loss: 0.7300\n",
      "Epoch [66/100], Loss: 0.7336\n",
      "Epoch [67/100], Loss: 0.7347\n",
      "Epoch [68/100], Loss: 0.7276\n",
      "Epoch [69/100], Loss: 0.7249\n",
      "Epoch [70/100], Loss: 0.7112\n",
      "Epoch [71/100], Loss: 0.7136\n",
      "Epoch [72/100], Loss: 0.7083\n",
      "Epoch [73/100], Loss: 0.7154\n",
      "Epoch [74/100], Loss: 0.7068\n",
      "Epoch [75/100], Loss: 0.7010\n",
      "Epoch [76/100], Loss: 0.6995\n",
      "Epoch [77/100], Loss: 0.6910\n",
      "Epoch [78/100], Loss: 0.6921\n",
      "Epoch [79/100], Loss: 0.6902\n",
      "Epoch [80/100], Loss: 0.6881\n",
      "Epoch [81/100], Loss: 0.6846\n",
      "Epoch [82/100], Loss: 0.6844\n",
      "Epoch [83/100], Loss: 0.6776\n",
      "Epoch [84/100], Loss: 0.6725\n",
      "Epoch [85/100], Loss: 0.6813\n",
      "Epoch [86/100], Loss: 0.6649\n",
      "Epoch [87/100], Loss: 0.6630\n",
      "Epoch [88/100], Loss: 0.6669\n",
      "Epoch [89/100], Loss: 0.6631\n",
      "Epoch [90/100], Loss: 0.6611\n",
      "Epoch [91/100], Loss: 0.6611\n",
      "Epoch [92/100], Loss: 0.6553\n",
      "Epoch [93/100], Loss: 0.6591\n",
      "Epoch [94/100], Loss: 0.6545\n",
      "Epoch [95/100], Loss: 0.6457\n",
      "Epoch [96/100], Loss: 0.6459\n",
      "Epoch [97/100], Loss: 0.6477\n",
      "Epoch [98/100], Loss: 0.6419\n",
      "Epoch [99/100], Loss: 0.6431\n",
      "Epoch [100/100], Loss: 0.6360\n",
      "Subset 10000, Epoch [100/100], Loss: 0.6360\n",
      "Test Accuracy Base Logit: 49.35%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 2.4394\n",
      "Epoch [2/100], Loss: 1.6111\n",
      "Epoch [3/100], Loss: 1.4483\n",
      "Epoch [4/100], Loss: 1.3535\n",
      "Epoch [5/100], Loss: 1.2977\n",
      "Epoch [6/100], Loss: 1.2661\n",
      "Epoch [7/100], Loss: 1.2097\n",
      "Epoch [8/100], Loss: 1.1822\n",
      "Epoch [9/100], Loss: 1.1589\n",
      "Epoch [10/100], Loss: 1.1265\n",
      "Epoch [11/100], Loss: 1.1191\n",
      "Epoch [12/100], Loss: 1.0930\n",
      "Epoch [13/100], Loss: 1.0654\n",
      "Epoch [14/100], Loss: 1.0572\n",
      "Epoch [15/100], Loss: 1.0478\n",
      "Epoch [16/100], Loss: 1.0278\n",
      "Epoch [17/100], Loss: 1.0073\n",
      "Epoch [18/100], Loss: 0.9984\n",
      "Epoch [19/100], Loss: 0.9867\n",
      "Epoch [20/100], Loss: 0.9708\n",
      "Epoch [21/100], Loss: 0.9592\n",
      "Epoch [22/100], Loss: 0.9527\n",
      "Epoch [23/100], Loss: 0.9425\n",
      "Epoch [24/100], Loss: 0.9368\n",
      "Epoch [25/100], Loss: 0.9271\n",
      "Epoch [26/100], Loss: 0.9202\n",
      "Epoch [27/100], Loss: 0.9148\n",
      "Epoch [28/100], Loss: 0.9052\n",
      "Epoch [29/100], Loss: 0.8919\n",
      "Epoch [30/100], Loss: 0.8843\n",
      "Epoch [31/100], Loss: 0.8789\n",
      "Epoch [32/100], Loss: 0.8695\n",
      "Epoch [33/100], Loss: 0.8633\n",
      "Epoch [34/100], Loss: 0.8620\n",
      "Epoch [35/100], Loss: 0.8561\n",
      "Epoch [36/100], Loss: 0.8445\n",
      "Epoch [37/100], Loss: 0.8456\n",
      "Epoch [38/100], Loss: 0.8357\n",
      "Epoch [39/100], Loss: 0.8333\n",
      "Epoch [40/100], Loss: 0.8265\n",
      "Epoch [41/100], Loss: 0.8226\n",
      "Epoch [42/100], Loss: 0.8172\n",
      "Epoch [43/100], Loss: 0.8060\n",
      "Epoch [44/100], Loss: 0.7992\n",
      "Epoch [45/100], Loss: 0.7917\n",
      "Epoch [46/100], Loss: 0.7880\n",
      "Epoch [47/100], Loss: 0.7886\n",
      "Epoch [48/100], Loss: 0.7933\n",
      "Epoch [49/100], Loss: 0.7739\n",
      "Epoch [50/100], Loss: 0.7850\n",
      "Epoch [51/100], Loss: 0.7787\n",
      "Epoch [52/100], Loss: 0.7750\n",
      "Epoch [53/100], Loss: 0.7648\n",
      "Epoch [54/100], Loss: 0.7567\n",
      "Epoch [55/100], Loss: 0.7509\n",
      "Epoch [56/100], Loss: 0.7491\n",
      "Epoch [57/100], Loss: 0.7415\n",
      "Epoch [58/100], Loss: 0.7361\n",
      "Epoch [59/100], Loss: 0.7373\n",
      "Epoch [60/100], Loss: 0.7369\n",
      "Epoch [61/100], Loss: 0.7239\n",
      "Epoch [62/100], Loss: 0.7243\n",
      "Epoch [63/100], Loss: 0.7245\n",
      "Epoch [64/100], Loss: 0.7193\n",
      "Epoch [65/100], Loss: 0.7162\n",
      "Epoch [66/100], Loss: 0.7096\n",
      "Epoch [67/100], Loss: 0.7017\n",
      "Epoch [68/100], Loss: 0.7015\n",
      "Epoch [69/100], Loss: 0.7095\n",
      "Epoch [70/100], Loss: 0.6927\n",
      "Epoch [71/100], Loss: 0.7008\n",
      "Epoch [72/100], Loss: 0.6992\n",
      "Epoch [73/100], Loss: 0.6919\n",
      "Epoch [74/100], Loss: 0.6828\n",
      "Epoch [75/100], Loss: 0.6850\n",
      "Epoch [76/100], Loss: 0.6851\n",
      "Epoch [77/100], Loss: 0.6751\n",
      "Epoch [78/100], Loss: 0.6741\n",
      "Epoch [79/100], Loss: 0.6700\n",
      "Epoch [80/100], Loss: 0.6640\n",
      "Epoch [81/100], Loss: 0.6682\n",
      "Epoch [82/100], Loss: 0.6687\n",
      "Epoch [83/100], Loss: 0.6615\n",
      "Epoch [84/100], Loss: 0.6500\n",
      "Epoch [85/100], Loss: 0.6527\n",
      "Epoch [86/100], Loss: 0.6475\n",
      "Epoch [87/100], Loss: 0.6508\n",
      "Epoch [88/100], Loss: 0.6493\n",
      "Epoch [89/100], Loss: 0.6447\n",
      "Epoch [90/100], Loss: 0.6444\n",
      "Epoch [91/100], Loss: 0.6451\n",
      "Epoch [92/100], Loss: 0.6420\n",
      "Epoch [93/100], Loss: 0.6359\n",
      "Epoch [94/100], Loss: 0.6326\n",
      "Epoch [95/100], Loss: 0.6244\n",
      "Epoch [96/100], Loss: 0.6295\n",
      "Epoch [97/100], Loss: 0.6275\n",
      "Epoch [98/100], Loss: 0.6249\n",
      "Epoch [99/100], Loss: 0.6223\n",
      "Epoch [100/100], Loss: 0.6242\n",
      "Subset 10000, Epoch [100/100], Loss: 0.6242\n",
      "Test Accuracy Base Logit: 49.37%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 2.4266\n",
      "Epoch [2/100], Loss: 1.6030\n",
      "Epoch [3/100], Loss: 1.4379\n",
      "Epoch [4/100], Loss: 1.3489\n",
      "Epoch [5/100], Loss: 1.2837\n",
      "Epoch [6/100], Loss: 1.2503\n",
      "Epoch [7/100], Loss: 1.2102\n",
      "Epoch [8/100], Loss: 1.1692\n",
      "Epoch [9/100], Loss: 1.1418\n",
      "Epoch [10/100], Loss: 1.1233\n",
      "Epoch [11/100], Loss: 1.1027\n",
      "Epoch [12/100], Loss: 1.0823\n",
      "Epoch [13/100], Loss: 1.0654\n",
      "Epoch [14/100], Loss: 1.0504\n",
      "Epoch [15/100], Loss: 1.0356\n",
      "Epoch [16/100], Loss: 1.0237\n",
      "Epoch [17/100], Loss: 1.0037\n",
      "Epoch [18/100], Loss: 0.9943\n",
      "Epoch [19/100], Loss: 0.9867\n",
      "Epoch [20/100], Loss: 0.9725\n",
      "Epoch [21/100], Loss: 0.9632\n",
      "Epoch [22/100], Loss: 0.9514\n",
      "Epoch [23/100], Loss: 0.9348\n",
      "Epoch [24/100], Loss: 0.9324\n",
      "Epoch [25/100], Loss: 0.9269\n",
      "Epoch [26/100], Loss: 0.9136\n",
      "Epoch [27/100], Loss: 0.9086\n",
      "Epoch [28/100], Loss: 0.9028\n",
      "Epoch [29/100], Loss: 0.8937\n",
      "Epoch [30/100], Loss: 0.8857\n",
      "Epoch [31/100], Loss: 0.8714\n",
      "Epoch [32/100], Loss: 0.8626\n",
      "Epoch [33/100], Loss: 0.8556\n",
      "Epoch [34/100], Loss: 0.8554\n",
      "Epoch [35/100], Loss: 0.8468\n",
      "Epoch [36/100], Loss: 0.8406\n",
      "Epoch [37/100], Loss: 0.8347\n",
      "Epoch [38/100], Loss: 0.8238\n",
      "Epoch [39/100], Loss: 0.8263\n",
      "Epoch [40/100], Loss: 0.8194\n",
      "Epoch [41/100], Loss: 0.8155\n",
      "Epoch [42/100], Loss: 0.8206\n",
      "Epoch [43/100], Loss: 0.8014\n",
      "Epoch [44/100], Loss: 0.7976\n",
      "Epoch [45/100], Loss: 0.7893\n",
      "Epoch [46/100], Loss: 0.7928\n",
      "Epoch [47/100], Loss: 0.7857\n",
      "Epoch [48/100], Loss: 0.7812\n",
      "Epoch [49/100], Loss: 0.7845\n",
      "Epoch [50/100], Loss: 0.7702\n",
      "Epoch [51/100], Loss: 0.7663\n",
      "Epoch [52/100], Loss: 0.7554\n",
      "Epoch [53/100], Loss: 0.7607\n",
      "Epoch [54/100], Loss: 0.7555\n",
      "Epoch [55/100], Loss: 0.7504\n",
      "Epoch [56/100], Loss: 0.7484\n",
      "Epoch [57/100], Loss: 0.7394\n",
      "Epoch [58/100], Loss: 0.7356\n",
      "Epoch [59/100], Loss: 0.7384\n",
      "Epoch [60/100], Loss: 0.7246\n",
      "Epoch [61/100], Loss: 0.7246\n",
      "Epoch [62/100], Loss: 0.7198\n",
      "Epoch [63/100], Loss: 0.7197\n",
      "Epoch [64/100], Loss: 0.7090\n",
      "Epoch [65/100], Loss: 0.7174\n",
      "Epoch [66/100], Loss: 0.7001\n",
      "Epoch [67/100], Loss: 0.7001\n",
      "Epoch [68/100], Loss: 0.6988\n",
      "Epoch [69/100], Loss: 0.6920\n",
      "Epoch [70/100], Loss: 0.6932\n",
      "Epoch [71/100], Loss: 0.6862\n",
      "Epoch [72/100], Loss: 0.6808\n",
      "Epoch [73/100], Loss: 0.6840\n",
      "Epoch [74/100], Loss: 0.6874\n",
      "Epoch [75/100], Loss: 0.6813\n",
      "Epoch [76/100], Loss: 0.6814\n",
      "Epoch [77/100], Loss: 0.6724\n",
      "Epoch [78/100], Loss: 0.6680\n",
      "Epoch [79/100], Loss: 0.6680\n",
      "Epoch [80/100], Loss: 0.6654\n",
      "Epoch [81/100], Loss: 0.6610\n",
      "Epoch [82/100], Loss: 0.6564\n",
      "Epoch [83/100], Loss: 0.6562\n",
      "Epoch [84/100], Loss: 0.6476\n",
      "Epoch [85/100], Loss: 0.6551\n",
      "Epoch [86/100], Loss: 0.6488\n",
      "Epoch [87/100], Loss: 0.6464\n",
      "Epoch [88/100], Loss: 0.6452\n",
      "Epoch [89/100], Loss: 0.6432\n",
      "Epoch [90/100], Loss: 0.6378\n",
      "Epoch [91/100], Loss: 0.6412\n",
      "Epoch [92/100], Loss: 0.6426\n",
      "Epoch [93/100], Loss: 0.6366\n",
      "Epoch [94/100], Loss: 0.6441\n",
      "Epoch [95/100], Loss: 0.6368\n",
      "Epoch [96/100], Loss: 0.6250\n",
      "Epoch [97/100], Loss: 0.6269\n",
      "Epoch [98/100], Loss: 0.6203\n",
      "Epoch [99/100], Loss: 0.6208\n",
      "Epoch [100/100], Loss: 0.6240\n",
      "Subset 10000, Epoch [100/100], Loss: 0.6240\n",
      "Test Accuracy Base Logit: 49.42%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 2.4229\n",
      "Epoch [2/100], Loss: 1.6016\n",
      "Epoch [3/100], Loss: 1.4373\n",
      "Epoch [4/100], Loss: 1.3350\n",
      "Epoch [5/100], Loss: 1.2885\n",
      "Epoch [6/100], Loss: 1.2351\n",
      "Epoch [7/100], Loss: 1.2006\n",
      "Epoch [8/100], Loss: 1.1750\n",
      "Epoch [9/100], Loss: 1.1469\n",
      "Epoch [10/100], Loss: 1.1171\n",
      "Epoch [11/100], Loss: 1.0931\n",
      "Epoch [12/100], Loss: 1.0759\n",
      "Epoch [13/100], Loss: 1.0531\n",
      "Epoch [14/100], Loss: 1.0323\n",
      "Epoch [15/100], Loss: 1.0189\n",
      "Epoch [16/100], Loss: 1.0107\n",
      "Epoch [17/100], Loss: 0.9958\n",
      "Epoch [18/100], Loss: 0.9770\n",
      "Epoch [19/100], Loss: 0.9682\n",
      "Epoch [20/100], Loss: 0.9482\n",
      "Epoch [21/100], Loss: 0.9586\n",
      "Epoch [22/100], Loss: 0.9426\n",
      "Epoch [23/100], Loss: 0.9335\n",
      "Epoch [24/100], Loss: 0.9176\n",
      "Epoch [25/100], Loss: 0.9023\n",
      "Epoch [26/100], Loss: 0.9076\n",
      "Epoch [27/100], Loss: 0.8989\n",
      "Epoch [28/100], Loss: 0.8891\n",
      "Epoch [29/100], Loss: 0.8775\n",
      "Epoch [30/100], Loss: 0.8674\n",
      "Epoch [31/100], Loss: 0.8746\n",
      "Epoch [32/100], Loss: 0.8586\n",
      "Epoch [33/100], Loss: 0.8528\n",
      "Epoch [34/100], Loss: 0.8449\n",
      "Epoch [35/100], Loss: 0.8485\n",
      "Epoch [36/100], Loss: 0.8310\n",
      "Epoch [37/100], Loss: 0.8261\n",
      "Epoch [38/100], Loss: 0.8168\n",
      "Epoch [39/100], Loss: 0.8149\n",
      "Epoch [40/100], Loss: 0.8066\n",
      "Epoch [41/100], Loss: 0.7920\n",
      "Epoch [42/100], Loss: 0.7911\n",
      "Epoch [43/100], Loss: 0.7911\n",
      "Epoch [44/100], Loss: 0.7777\n",
      "Epoch [45/100], Loss: 0.7796\n",
      "Epoch [46/100], Loss: 0.7756\n",
      "Epoch [47/100], Loss: 0.7724\n",
      "Epoch [48/100], Loss: 0.7580\n",
      "Epoch [49/100], Loss: 0.7556\n",
      "Epoch [50/100], Loss: 0.7562\n",
      "Epoch [51/100], Loss: 0.7554\n",
      "Epoch [52/100], Loss: 0.7382\n",
      "Epoch [53/100], Loss: 0.7459\n",
      "Epoch [54/100], Loss: 0.7501\n",
      "Epoch [55/100], Loss: 0.7348\n",
      "Epoch [56/100], Loss: 0.7282\n",
      "Epoch [57/100], Loss: 0.7159\n",
      "Epoch [58/100], Loss: 0.7186\n",
      "Epoch [59/100], Loss: 0.7153\n",
      "Epoch [60/100], Loss: 0.7090\n",
      "Epoch [61/100], Loss: 0.7130\n",
      "Epoch [62/100], Loss: 0.7084\n",
      "Epoch [63/100], Loss: 0.6970\n",
      "Epoch [64/100], Loss: 0.6946\n",
      "Epoch [65/100], Loss: 0.7014\n",
      "Epoch [66/100], Loss: 0.6973\n",
      "Epoch [67/100], Loss: 0.6947\n",
      "Epoch [68/100], Loss: 0.6829\n",
      "Epoch [69/100], Loss: 0.6867\n",
      "Epoch [70/100], Loss: 0.6792\n",
      "Epoch [71/100], Loss: 0.6793\n",
      "Epoch [72/100], Loss: 0.6709\n",
      "Epoch [73/100], Loss: 0.6679\n",
      "Epoch [74/100], Loss: 0.6659\n",
      "Epoch [75/100], Loss: 0.6594\n",
      "Epoch [76/100], Loss: 0.6575\n",
      "Epoch [77/100], Loss: 0.6485\n",
      "Epoch [78/100], Loss: 0.6533\n",
      "Epoch [79/100], Loss: 0.6552\n",
      "Epoch [80/100], Loss: 0.6493\n",
      "Epoch [81/100], Loss: 0.6507\n",
      "Epoch [82/100], Loss: 0.6433\n",
      "Epoch [83/100], Loss: 0.6351\n",
      "Epoch [84/100], Loss: 0.6332\n",
      "Epoch [85/100], Loss: 0.6309\n",
      "Epoch [86/100], Loss: 0.6324\n",
      "Epoch [87/100], Loss: 0.6303\n",
      "Epoch [88/100], Loss: 0.6287\n",
      "Epoch [89/100], Loss: 0.6296\n",
      "Epoch [90/100], Loss: 0.6137\n",
      "Epoch [91/100], Loss: 0.6219\n",
      "Epoch [92/100], Loss: 0.6146\n",
      "Epoch [93/100], Loss: 0.6160\n",
      "Epoch [94/100], Loss: 0.6135\n",
      "Epoch [95/100], Loss: 0.6087\n",
      "Epoch [96/100], Loss: 0.6142\n",
      "Epoch [97/100], Loss: 0.6069\n",
      "Epoch [98/100], Loss: 0.6033\n",
      "Epoch [99/100], Loss: 0.6003\n",
      "Epoch [100/100], Loss: 0.6075\n",
      "Subset 10000, Epoch [100/100], Loss: 0.6075\n",
      "Test Accuracy Base Logit: 49.89%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 2.4223\n",
      "Epoch [2/100], Loss: 1.5888\n",
      "Epoch [3/100], Loss: 1.4324\n",
      "Epoch [4/100], Loss: 1.3377\n",
      "Epoch [5/100], Loss: 1.2756\n",
      "Epoch [6/100], Loss: 1.2418\n",
      "Epoch [7/100], Loss: 1.2004\n",
      "Epoch [8/100], Loss: 1.1769\n",
      "Epoch [9/100], Loss: 1.1501\n",
      "Epoch [10/100], Loss: 1.1166\n",
      "Epoch [11/100], Loss: 1.0941\n",
      "Epoch [12/100], Loss: 1.0718\n",
      "Epoch [13/100], Loss: 1.0533\n",
      "Epoch [14/100], Loss: 1.0473\n",
      "Epoch [15/100], Loss: 1.0276\n",
      "Epoch [16/100], Loss: 1.0220\n",
      "Epoch [17/100], Loss: 0.9917\n",
      "Epoch [18/100], Loss: 0.9919\n",
      "Epoch [19/100], Loss: 0.9716\n",
      "Epoch [20/100], Loss: 0.9649\n",
      "Epoch [21/100], Loss: 0.9593\n",
      "Epoch [22/100], Loss: 0.9378\n",
      "Epoch [23/100], Loss: 0.9350\n",
      "Epoch [24/100], Loss: 0.9249\n",
      "Epoch [25/100], Loss: 0.9129\n",
      "Epoch [26/100], Loss: 0.9029\n",
      "Epoch [27/100], Loss: 0.9058\n",
      "Epoch [28/100], Loss: 0.8947\n",
      "Epoch [29/100], Loss: 0.8793\n",
      "Epoch [30/100], Loss: 0.8719\n",
      "Epoch [31/100], Loss: 0.8718\n",
      "Epoch [32/100], Loss: 0.8615\n",
      "Epoch [33/100], Loss: 0.8495\n",
      "Epoch [34/100], Loss: 0.8428\n",
      "Epoch [35/100], Loss: 0.8459\n",
      "Epoch [36/100], Loss: 0.8385\n",
      "Epoch [37/100], Loss: 0.8228\n",
      "Epoch [38/100], Loss: 0.8181\n",
      "Epoch [39/100], Loss: 0.8085\n",
      "Epoch [40/100], Loss: 0.8032\n",
      "Epoch [41/100], Loss: 0.8147\n",
      "Epoch [42/100], Loss: 0.7971\n",
      "Epoch [43/100], Loss: 0.7942\n",
      "Epoch [44/100], Loss: 0.7919\n",
      "Epoch [45/100], Loss: 0.7827\n",
      "Epoch [46/100], Loss: 0.7740\n",
      "Epoch [47/100], Loss: 0.7753\n",
      "Epoch [48/100], Loss: 0.7702\n",
      "Epoch [49/100], Loss: 0.7667\n",
      "Epoch [50/100], Loss: 0.7597\n",
      "Epoch [51/100], Loss: 0.7539\n",
      "Epoch [52/100], Loss: 0.7529\n",
      "Epoch [53/100], Loss: 0.7443\n",
      "Epoch [54/100], Loss: 0.7393\n",
      "Epoch [55/100], Loss: 0.7411\n",
      "Epoch [56/100], Loss: 0.7328\n",
      "Epoch [57/100], Loss: 0.7378\n",
      "Epoch [58/100], Loss: 0.7195\n",
      "Epoch [59/100], Loss: 0.7193\n",
      "Epoch [60/100], Loss: 0.7271\n",
      "Epoch [61/100], Loss: 0.7142\n",
      "Epoch [62/100], Loss: 0.7138\n",
      "Epoch [63/100], Loss: 0.7042\n",
      "Epoch [64/100], Loss: 0.7027\n",
      "Epoch [65/100], Loss: 0.6992\n",
      "Epoch [66/100], Loss: 0.6980\n",
      "Epoch [67/100], Loss: 0.6920\n",
      "Epoch [68/100], Loss: 0.6840\n",
      "Epoch [69/100], Loss: 0.6874\n",
      "Epoch [70/100], Loss: 0.6818\n",
      "Epoch [71/100], Loss: 0.6790\n",
      "Epoch [72/100], Loss: 0.6746\n",
      "Epoch [73/100], Loss: 0.6709\n",
      "Epoch [74/100], Loss: 0.6719\n",
      "Epoch [75/100], Loss: 0.6741\n",
      "Epoch [76/100], Loss: 0.6701\n",
      "Epoch [77/100], Loss: 0.6627\n",
      "Epoch [78/100], Loss: 0.6624\n",
      "Epoch [79/100], Loss: 0.6606\n",
      "Epoch [80/100], Loss: 0.6556\n",
      "Epoch [81/100], Loss: 0.6567\n",
      "Epoch [82/100], Loss: 0.6535\n",
      "Epoch [83/100], Loss: 0.6496\n",
      "Epoch [84/100], Loss: 0.6457\n",
      "Epoch [85/100], Loss: 0.6344\n",
      "Epoch [86/100], Loss: 0.6391\n",
      "Epoch [87/100], Loss: 0.6367\n",
      "Epoch [88/100], Loss: 0.6338\n",
      "Epoch [89/100], Loss: 0.6242\n",
      "Epoch [90/100], Loss: 0.6255\n",
      "Epoch [91/100], Loss: 0.6288\n",
      "Epoch [92/100], Loss: 0.6218\n",
      "Epoch [93/100], Loss: 0.6225\n",
      "Epoch [94/100], Loss: 0.6216\n",
      "Epoch [95/100], Loss: 0.6210\n",
      "Stopping early at epoch 95 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [95/100], Loss: 0.6210\n",
      "Test Accuracy Base Logit: 49.59%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "logit_accuracy = {}\n",
    "\n",
    "for subset_size in num_subsets:\n",
    "    accuracy_list = []\n",
    "\n",
    "    for _ in range(10):\n",
    "        logit_model = LogisticRegression(input_dim, num_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_sdg = optim.SGD(logit_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "        print(f\"Training with subset size: {subset_size}\")\n",
    "        subset_indices = torch.randperm(train_data.shape[0])[:subset_size]\n",
    "        subset = SubsetRandomSampler(subset_indices)\n",
    "        train_loader = DataLoader(train_set, batch_size=128, sampler=subset)\n",
    "\n",
    "        previous_loss = float('inf')  # Track the best loss\n",
    "        epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            logit_model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = logit_model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer_sdg.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer_sdg.step()\n",
    "\n",
    "                total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "            current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "            # Check for early stopping\n",
    "            if previous_loss - current_loss < tolerance:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "                    break\n",
    "            else:\n",
    "                epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "            previous_loss = current_loss\n",
    "\n",
    "        print(f\"Subset {subset_size}, Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        logit_model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(-1, 28*28)\n",
    "                outputs = logit_model(images)\n",
    "                _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Test Accuracy Base Logit: {accuracy:.2f}%\")\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    logit_accuracy[subset_size] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125000    55.887099\n",
       " 75000     55.169533\n",
       " 50000     54.406309\n",
       " 10000     49.617869\n",
       " dtype: float64,\n",
       " 125000    0.413596\n",
       " 75000     0.477126\n",
       " 50000     0.424888\n",
       " 10000     0.354771\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_df = pd.DataFrame(logit_accuracy)\n",
    "logit_df.mean(), logit_df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipschitz Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with subset size: 125000\n",
      "Epoch [1/100], Loss: 1.5258\n",
      "Epoch [2/100], Loss: 1.3149\n",
      "Epoch [3/100], Loss: 1.2835\n",
      "Epoch [4/100], Loss: 1.2679\n",
      "Epoch [5/100], Loss: 1.2541\n",
      "Epoch [6/100], Loss: 1.2450\n",
      "Epoch [7/100], Loss: 1.2379\n",
      "Epoch [8/100], Loss: 1.2332\n",
      "Epoch [9/100], Loss: 1.2271\n",
      "Epoch [10/100], Loss: 1.2213\n",
      "Epoch [11/100], Loss: 1.2196\n",
      "Epoch [12/100], Loss: 1.2185\n",
      "Epoch [13/100], Loss: 1.2130\n",
      "Epoch [14/100], Loss: 1.2118\n",
      "Epoch [15/100], Loss: 1.2083\n",
      "Epoch [16/100], Loss: 1.2074\n",
      "Epoch [17/100], Loss: 1.2053\n",
      "Epoch [18/100], Loss: 1.2004\n",
      "Epoch [19/100], Loss: 1.2005\n",
      "Epoch [20/100], Loss: 1.1986\n",
      "Epoch [21/100], Loss: 1.1984\n",
      "Epoch [22/100], Loss: 1.1958\n",
      "Epoch [23/100], Loss: 1.1931\n",
      "Epoch [24/100], Loss: 1.1929\n",
      "Epoch [25/100], Loss: 1.1908\n",
      "Epoch [26/100], Loss: 1.1895\n",
      "Epoch [27/100], Loss: 1.1906\n",
      "Epoch [28/100], Loss: 1.1891\n",
      "Epoch [29/100], Loss: 1.1889\n",
      "Epoch [30/100], Loss: 1.1878\n",
      "Epoch [31/100], Loss: 1.1875\n",
      "Epoch [32/100], Loss: 1.1883\n",
      "Epoch [33/100], Loss: 1.1851\n",
      "Epoch [34/100], Loss: 1.1824\n",
      "Epoch [35/100], Loss: 1.1844\n",
      "Epoch [36/100], Loss: 1.1829\n",
      "Epoch [37/100], Loss: 1.1832\n",
      "Epoch [38/100], Loss: 1.1832\n",
      "Epoch [39/100], Loss: 1.1813\n",
      "Epoch [40/100], Loss: 1.1801\n",
      "Epoch [41/100], Loss: 1.1798\n",
      "Epoch [42/100], Loss: 1.1792\n",
      "Epoch [43/100], Loss: 1.1773\n",
      "Epoch [44/100], Loss: 1.1783\n",
      "Epoch [45/100], Loss: 1.1789\n",
      "Epoch [46/100], Loss: 1.1774\n",
      "Epoch [47/100], Loss: 1.1775\n",
      "Epoch [48/100], Loss: 1.1766\n",
      "Epoch [49/100], Loss: 1.1778\n",
      "Epoch [50/100], Loss: 1.1763\n",
      "Epoch [51/100], Loss: 1.1767\n",
      "Epoch [52/100], Loss: 1.1757\n",
      "Epoch [53/100], Loss: 1.1741\n",
      "Epoch [54/100], Loss: 1.1744\n",
      "Epoch [55/100], Loss: 1.1734\n",
      "Epoch [56/100], Loss: 1.1739\n",
      "Epoch [57/100], Loss: 1.1719\n",
      "Epoch [58/100], Loss: 1.1721\n",
      "Epoch [59/100], Loss: 1.1750\n",
      "Epoch [60/100], Loss: 1.1737\n",
      "Epoch [61/100], Loss: 1.1727\n",
      "Epoch [62/100], Loss: 1.1700\n",
      "Epoch [63/100], Loss: 1.1725\n",
      "Epoch [64/100], Loss: 1.1715\n",
      "Epoch [65/100], Loss: 1.1719\n",
      "Epoch [66/100], Loss: 1.1698\n",
      "Epoch [67/100], Loss: 1.1705\n",
      "Epoch [68/100], Loss: 1.1705\n",
      "Epoch [69/100], Loss: 1.1704\n",
      "Stopping early at epoch 69 (Loss improvement < 0.0001 for 3 epochs)\n",
      "Subset 125000, Epoch [69/100], Loss: 1.1704\n",
      "Test Accuracy Logit Lipschitz: 55.91%\n",
      "Epoch [1/100], Loss: 1.5256\n",
      "Epoch [2/100], Loss: 1.3164\n",
      "Epoch [3/100], Loss: 1.2823\n",
      "Epoch [4/100], Loss: 1.2649\n",
      "Epoch [5/100], Loss: 1.2557\n",
      "Epoch [6/100], Loss: 1.2466\n",
      "Epoch [7/100], Loss: 1.2395\n",
      "Epoch [8/100], Loss: 1.2325\n",
      "Epoch [9/100], Loss: 1.2268\n",
      "Epoch [10/100], Loss: 1.2244\n",
      "Epoch [11/100], Loss: 1.2221\n",
      "Epoch [12/100], Loss: 1.2152\n",
      "Epoch [13/100], Loss: 1.2140\n",
      "Epoch [14/100], Loss: 1.2107\n",
      "Epoch [15/100], Loss: 1.2098\n",
      "Epoch [16/100], Loss: 1.2074\n",
      "Epoch [17/100], Loss: 1.2022\n",
      "Epoch [18/100], Loss: 1.2033\n",
      "Epoch [19/100], Loss: 1.2011\n",
      "Epoch [20/100], Loss: 1.1995\n",
      "Epoch [21/100], Loss: 1.1981\n",
      "Epoch [22/100], Loss: 1.1984\n",
      "Epoch [23/100], Loss: 1.1965\n",
      "Epoch [24/100], Loss: 1.1950\n",
      "Epoch [25/100], Loss: 1.1921\n",
      "Epoch [26/100], Loss: 1.1921\n",
      "Epoch [27/100], Loss: 1.1916\n",
      "Epoch [28/100], Loss: 1.1912\n",
      "Epoch [29/100], Loss: 1.1899\n",
      "Epoch [30/100], Loss: 1.1885\n",
      "Epoch [31/100], Loss: 1.1875\n",
      "Epoch [32/100], Loss: 1.1854\n",
      "Epoch [33/100], Loss: 1.1852\n",
      "Epoch [34/100], Loss: 1.1860\n",
      "Epoch [35/100], Loss: 1.1836\n",
      "Epoch [36/100], Loss: 1.1850\n",
      "Epoch [37/100], Loss: 1.1816\n",
      "Epoch [38/100], Loss: 1.1813\n",
      "Epoch [39/100], Loss: 1.1813\n",
      "Epoch [40/100], Loss: 1.1804\n",
      "Epoch [41/100], Loss: 1.1815\n",
      "Epoch [42/100], Loss: 1.1799\n",
      "Epoch [43/100], Loss: 1.1804\n",
      "Epoch [44/100], Loss: 1.1809\n",
      "Epoch [45/100], Loss: 1.1770\n",
      "Epoch [46/100], Loss: 1.1777\n",
      "Epoch [47/100], Loss: 1.1778\n",
      "Epoch [48/100], Loss: 1.1773\n",
      "Epoch [49/100], Loss: 1.1775\n",
      "Epoch [50/100], Loss: 1.1747\n",
      "Epoch [51/100], Loss: 1.1757\n",
      "Epoch [52/100], Loss: 1.1737\n",
      "Epoch [53/100], Loss: 1.1748\n",
      "Epoch [54/100], Loss: 1.1735\n",
      "Epoch [55/100], Loss: 1.1727\n",
      "Epoch [56/100], Loss: 1.1736\n",
      "Epoch [57/100], Loss: 1.1755\n",
      "Epoch [58/100], Loss: 1.1718\n",
      "Epoch [59/100], Loss: 1.1724\n",
      "Epoch [60/100], Loss: 1.1738\n",
      "Epoch [61/100], Loss: 1.1719\n",
      "Epoch [62/100], Loss: 1.1714\n",
      "Epoch [63/100], Loss: 1.1712\n",
      "Epoch [64/100], Loss: 1.1718\n",
      "Epoch [65/100], Loss: 1.1703\n",
      "Epoch [66/100], Loss: 1.1712\n",
      "Epoch [67/100], Loss: 1.1699\n",
      "Epoch [68/100], Loss: 1.1704\n",
      "Epoch [69/100], Loss: 1.1711\n",
      "Epoch [70/100], Loss: 1.1687\n",
      "Epoch [71/100], Loss: 1.1699\n",
      "Epoch [72/100], Loss: 1.1697\n",
      "Epoch [73/100], Loss: 1.1694\n",
      "Epoch [74/100], Loss: 1.1680\n",
      "Epoch [75/100], Loss: 1.1692\n",
      "Epoch [76/100], Loss: 1.1685\n",
      "Epoch [77/100], Loss: 1.1687\n",
      "Epoch [78/100], Loss: 1.1667\n",
      "Epoch [79/100], Loss: 1.1657\n",
      "Epoch [80/100], Loss: 1.1685\n",
      "Epoch [81/100], Loss: 1.1678\n",
      "Epoch [82/100], Loss: 1.1682\n",
      "Epoch [83/100], Loss: 1.1668\n",
      "Epoch [84/100], Loss: 1.1670\n",
      "Epoch [85/100], Loss: 1.1667\n",
      "Epoch [86/100], Loss: 1.1675\n",
      "Epoch [87/100], Loss: 1.1676\n",
      "Epoch [88/100], Loss: 1.1669\n",
      "Epoch [89/100], Loss: 1.1661\n",
      "Epoch [90/100], Loss: 1.1666\n",
      "Epoch [91/100], Loss: 1.1651\n",
      "Epoch [92/100], Loss: 1.1644\n",
      "Epoch [93/100], Loss: 1.1657\n",
      "Epoch [94/100], Loss: 1.1659\n",
      "Epoch [95/100], Loss: 1.1639\n",
      "Epoch [96/100], Loss: 1.1658\n",
      "Epoch [97/100], Loss: 1.1648\n",
      "Epoch [98/100], Loss: 1.1646\n",
      "Epoch [99/100], Loss: 1.1654\n",
      "Epoch [100/100], Loss: 1.1654\n",
      "Subset 125000, Epoch [100/100], Loss: 1.1654\n",
      "Test Accuracy Logit Lipschitz: 55.90%\n",
      "Epoch [1/100], Loss: 1.5189\n",
      "Epoch [2/100], Loss: 1.3101\n",
      "Epoch [3/100], Loss: 1.2774\n",
      "Epoch [4/100], Loss: 1.2580\n",
      "Epoch [5/100], Loss: 1.2487\n",
      "Epoch [6/100], Loss: 1.2400\n",
      "Epoch [7/100], Loss: 1.2317\n",
      "Epoch [8/100], Loss: 1.2278\n",
      "Epoch [9/100], Loss: 1.2222\n",
      "Epoch [10/100], Loss: 1.2170\n",
      "Epoch [11/100], Loss: 1.2147\n",
      "Epoch [12/100], Loss: 1.2124\n",
      "Epoch [13/100], Loss: 1.2057\n",
      "Epoch [14/100], Loss: 1.2050\n",
      "Epoch [15/100], Loss: 1.2032\n",
      "Epoch [16/100], Loss: 1.2000\n",
      "Epoch [17/100], Loss: 1.1990\n",
      "Epoch [18/100], Loss: 1.1980\n",
      "Epoch [19/100], Loss: 1.1955\n",
      "Epoch [20/100], Loss: 1.1932\n",
      "Epoch [21/100], Loss: 1.1919\n",
      "Epoch [22/100], Loss: 1.1913\n",
      "Epoch [23/100], Loss: 1.1896\n",
      "Epoch [24/100], Loss: 1.1885\n",
      "Epoch [25/100], Loss: 1.1855\n",
      "Epoch [26/100], Loss: 1.1857\n",
      "Epoch [27/100], Loss: 1.1859\n",
      "Epoch [28/100], Loss: 1.1820\n",
      "Epoch [29/100], Loss: 1.1836\n",
      "Epoch [30/100], Loss: 1.1827\n",
      "Epoch [31/100], Loss: 1.1799\n",
      "Epoch [32/100], Loss: 1.1796\n",
      "Epoch [33/100], Loss: 1.1796\n",
      "Epoch [34/100], Loss: 1.1797\n",
      "Epoch [35/100], Loss: 1.1775\n",
      "Epoch [36/100], Loss: 1.1783\n",
      "Epoch [37/100], Loss: 1.1780\n",
      "Epoch [38/100], Loss: 1.1767\n",
      "Epoch [39/100], Loss: 1.1765\n",
      "Epoch [40/100], Loss: 1.1760\n",
      "Epoch [41/100], Loss: 1.1743\n",
      "Epoch [42/100], Loss: 1.1745\n",
      "Epoch [43/100], Loss: 1.1730\n",
      "Epoch [44/100], Loss: 1.1710\n",
      "Epoch [45/100], Loss: 1.1724\n",
      "Epoch [46/100], Loss: 1.1712\n",
      "Epoch [47/100], Loss: 1.1708\n",
      "Epoch [48/100], Loss: 1.1709\n",
      "Epoch [49/100], Loss: 1.1687\n",
      "Epoch [50/100], Loss: 1.1689\n",
      "Epoch [51/100], Loss: 1.1697\n",
      "Epoch [52/100], Loss: 1.1695\n",
      "Epoch [53/100], Loss: 1.1689\n",
      "Epoch [54/100], Loss: 1.1702\n",
      "Epoch [55/100], Loss: 1.1666\n",
      "Epoch [56/100], Loss: 1.1669\n",
      "Epoch [57/100], Loss: 1.1656\n",
      "Epoch [58/100], Loss: 1.1660\n",
      "Epoch [59/100], Loss: 1.1689\n",
      "Epoch [60/100], Loss: 1.1658\n",
      "Epoch [61/100], Loss: 1.1665\n",
      "Epoch [62/100], Loss: 1.1676\n",
      "Epoch [63/100], Loss: 1.1651\n",
      "Epoch [64/100], Loss: 1.1645\n",
      "Epoch [65/100], Loss: 1.1641\n",
      "Epoch [66/100], Loss: 1.1648\n",
      "Epoch [67/100], Loss: 1.1616\n",
      "Epoch [68/100], Loss: 1.1644\n",
      "Epoch [69/100], Loss: 1.1666\n",
      "Epoch [70/100], Loss: 1.1643\n",
      "Epoch [71/100], Loss: 1.1648\n",
      "Epoch [72/100], Loss: 1.1633\n",
      "Epoch [73/100], Loss: 1.1628\n",
      "Epoch [74/100], Loss: 1.1618\n",
      "Epoch [75/100], Loss: 1.1627\n",
      "Epoch [76/100], Loss: 1.1630\n",
      "Epoch [77/100], Loss: 1.1629\n",
      "Stopping early at epoch 77 (Loss improvement < 0.0001 for 3 epochs)\n",
      "Subset 125000, Epoch [77/100], Loss: 1.1629\n",
      "Test Accuracy Logit Lipschitz: 56.09%\n",
      "Epoch [1/100], Loss: 1.5184\n",
      "Epoch [2/100], Loss: 1.3090\n",
      "Epoch [3/100], Loss: 1.2774\n",
      "Epoch [4/100], Loss: 1.2612\n",
      "Epoch [5/100], Loss: 1.2478\n",
      "Epoch [6/100], Loss: 1.2374\n",
      "Epoch [7/100], Loss: 1.2308\n",
      "Epoch [8/100], Loss: 1.2245\n",
      "Epoch [9/100], Loss: 1.2199\n",
      "Epoch [10/100], Loss: 1.2153\n",
      "Epoch [11/100], Loss: 1.2124\n",
      "Epoch [12/100], Loss: 1.2081\n",
      "Epoch [13/100], Loss: 1.2056\n",
      "Epoch [14/100], Loss: 1.2021\n",
      "Epoch [15/100], Loss: 1.2015\n",
      "Epoch [16/100], Loss: 1.1998\n",
      "Epoch [17/100], Loss: 1.1982\n",
      "Epoch [18/100], Loss: 1.1958\n",
      "Epoch [19/100], Loss: 1.1937\n",
      "Epoch [20/100], Loss: 1.1939\n",
      "Epoch [21/100], Loss: 1.1912\n",
      "Epoch [22/100], Loss: 1.1899\n",
      "Epoch [23/100], Loss: 1.1882\n",
      "Epoch [24/100], Loss: 1.1872\n",
      "Epoch [25/100], Loss: 1.1847\n",
      "Epoch [26/100], Loss: 1.1841\n",
      "Epoch [27/100], Loss: 1.1853\n",
      "Epoch [28/100], Loss: 1.1820\n",
      "Epoch [29/100], Loss: 1.1823\n",
      "Epoch [30/100], Loss: 1.1806\n",
      "Epoch [31/100], Loss: 1.1791\n",
      "Epoch [32/100], Loss: 1.1806\n",
      "Epoch [33/100], Loss: 1.1783\n",
      "Epoch [34/100], Loss: 1.1778\n",
      "Epoch [35/100], Loss: 1.1787\n",
      "Epoch [36/100], Loss: 1.1756\n",
      "Epoch [37/100], Loss: 1.1753\n",
      "Epoch [38/100], Loss: 1.1749\n",
      "Epoch [39/100], Loss: 1.1766\n",
      "Epoch [40/100], Loss: 1.1761\n",
      "Epoch [41/100], Loss: 1.1733\n",
      "Epoch [42/100], Loss: 1.1721\n",
      "Epoch [43/100], Loss: 1.1728\n",
      "Epoch [44/100], Loss: 1.1714\n",
      "Epoch [45/100], Loss: 1.1709\n",
      "Epoch [46/100], Loss: 1.1705\n",
      "Epoch [47/100], Loss: 1.1697\n",
      "Epoch [48/100], Loss: 1.1707\n",
      "Epoch [49/100], Loss: 1.1696\n",
      "Epoch [50/100], Loss: 1.1690\n",
      "Epoch [51/100], Loss: 1.1692\n",
      "Epoch [52/100], Loss: 1.1686\n",
      "Epoch [53/100], Loss: 1.1669\n",
      "Epoch [54/100], Loss: 1.1680\n",
      "Epoch [55/100], Loss: 1.1670\n",
      "Epoch [56/100], Loss: 1.1669\n",
      "Epoch [57/100], Loss: 1.1664\n",
      "Epoch [58/100], Loss: 1.1662\n",
      "Epoch [59/100], Loss: 1.1643\n",
      "Epoch [60/100], Loss: 1.1663\n",
      "Epoch [61/100], Loss: 1.1654\n",
      "Epoch [62/100], Loss: 1.1646\n",
      "Epoch [63/100], Loss: 1.1644\n",
      "Epoch [64/100], Loss: 1.1636\n",
      "Epoch [65/100], Loss: 1.1641\n",
      "Epoch [66/100], Loss: 1.1639\n",
      "Epoch [67/100], Loss: 1.1632\n",
      "Epoch [68/100], Loss: 1.1649\n",
      "Epoch [69/100], Loss: 1.1626\n",
      "Epoch [70/100], Loss: 1.1618\n",
      "Epoch [71/100], Loss: 1.1619\n",
      "Epoch [72/100], Loss: 1.1626\n",
      "Epoch [73/100], Loss: 1.1629\n",
      "Stopping early at epoch 73 (Loss improvement < 0.0001 for 3 epochs)\n",
      "Subset 125000, Epoch [73/100], Loss: 1.1629\n",
      "Test Accuracy Logit Lipschitz: 56.25%\n",
      "Epoch [1/100], Loss: 1.5174\n",
      "Epoch [2/100], Loss: 1.3089\n",
      "Epoch [3/100], Loss: 1.2758\n",
      "Epoch [4/100], Loss: 1.2568\n",
      "Epoch [5/100], Loss: 1.2467\n",
      "Epoch [6/100], Loss: 1.2378\n",
      "Epoch [7/100], Loss: 1.2288\n",
      "Epoch [8/100], Loss: 1.2219\n",
      "Epoch [9/100], Loss: 1.2176\n",
      "Epoch [10/100], Loss: 1.2152\n",
      "Epoch [11/100], Loss: 1.2097\n",
      "Epoch [12/100], Loss: 1.2078\n",
      "Epoch [13/100], Loss: 1.2058\n",
      "Epoch [14/100], Loss: 1.2025\n",
      "Epoch [15/100], Loss: 1.1978\n",
      "Epoch [16/100], Loss: 1.1985\n",
      "Epoch [17/100], Loss: 1.1957\n",
      "Epoch [18/100], Loss: 1.1921\n",
      "Epoch [19/100], Loss: 1.1914\n",
      "Epoch [20/100], Loss: 1.1891\n",
      "Epoch [21/100], Loss: 1.1887\n",
      "Epoch [22/100], Loss: 1.1856\n",
      "Epoch [23/100], Loss: 1.1865\n",
      "Epoch [24/100], Loss: 1.1842\n",
      "Epoch [25/100], Loss: 1.1845\n",
      "Epoch [26/100], Loss: 1.1817\n",
      "Epoch [27/100], Loss: 1.1804\n",
      "Epoch [28/100], Loss: 1.1820\n",
      "Epoch [29/100], Loss: 1.1773\n",
      "Epoch [30/100], Loss: 1.1787\n",
      "Epoch [31/100], Loss: 1.1787\n",
      "Epoch [32/100], Loss: 1.1769\n",
      "Epoch [33/100], Loss: 1.1746\n",
      "Epoch [34/100], Loss: 1.1762\n",
      "Epoch [35/100], Loss: 1.1765\n",
      "Epoch [36/100], Loss: 1.1738\n",
      "Epoch [37/100], Loss: 1.1731\n",
      "Epoch [38/100], Loss: 1.1732\n",
      "Epoch [39/100], Loss: 1.1724\n",
      "Epoch [40/100], Loss: 1.1722\n",
      "Epoch [41/100], Loss: 1.1718\n",
      "Epoch [42/100], Loss: 1.1696\n",
      "Epoch [43/100], Loss: 1.1697\n",
      "Epoch [44/100], Loss: 1.1705\n",
      "Epoch [45/100], Loss: 1.1697\n",
      "Epoch [46/100], Loss: 1.1674\n",
      "Epoch [47/100], Loss: 1.1669\n",
      "Epoch [48/100], Loss: 1.1653\n",
      "Epoch [49/100], Loss: 1.1686\n",
      "Epoch [50/100], Loss: 1.1663\n",
      "Epoch [51/100], Loss: 1.1649\n",
      "Epoch [52/100], Loss: 1.1665\n",
      "Epoch [53/100], Loss: 1.1642\n",
      "Epoch [54/100], Loss: 1.1642\n",
      "Epoch [55/100], Loss: 1.1650\n",
      "Epoch [56/100], Loss: 1.1635\n",
      "Epoch [57/100], Loss: 1.1626\n",
      "Epoch [58/100], Loss: 1.1645\n",
      "Epoch [59/100], Loss: 1.1636\n",
      "Epoch [60/100], Loss: 1.1626\n",
      "Epoch [61/100], Loss: 1.1630\n",
      "Epoch [62/100], Loss: 1.1619\n",
      "Epoch [63/100], Loss: 1.1632\n",
      "Epoch [64/100], Loss: 1.1616\n",
      "Epoch [65/100], Loss: 1.1615\n",
      "Epoch [66/100], Loss: 1.1614\n",
      "Epoch [67/100], Loss: 1.1592\n",
      "Epoch [68/100], Loss: 1.1614\n",
      "Epoch [69/100], Loss: 1.1583\n",
      "Epoch [70/100], Loss: 1.1603\n",
      "Epoch [71/100], Loss: 1.1597\n",
      "Epoch [72/100], Loss: 1.1594\n",
      "Epoch [73/100], Loss: 1.1584\n",
      "Epoch [74/100], Loss: 1.1581\n",
      "Epoch [75/100], Loss: 1.1591\n",
      "Epoch [76/100], Loss: 1.1570\n",
      "Epoch [77/100], Loss: 1.1585\n",
      "Epoch [78/100], Loss: 1.1576\n",
      "Epoch [79/100], Loss: 1.1585\n",
      "Epoch [80/100], Loss: 1.1578\n",
      "Epoch [81/100], Loss: 1.1574\n",
      "Epoch [82/100], Loss: 1.1564\n",
      "Epoch [83/100], Loss: 1.1589\n",
      "Epoch [84/100], Loss: 1.1581\n",
      "Epoch [85/100], Loss: 1.1573\n",
      "Epoch [86/100], Loss: 1.1569\n",
      "Epoch [87/100], Loss: 1.1557\n",
      "Epoch [88/100], Loss: 1.1570\n",
      "Epoch [89/100], Loss: 1.1569\n",
      "Epoch [90/100], Loss: 1.1560\n",
      "Epoch [91/100], Loss: 1.1550\n",
      "Epoch [92/100], Loss: 1.1571\n",
      "Epoch [93/100], Loss: 1.1573\n",
      "Epoch [94/100], Loss: 1.1564\n",
      "Epoch [95/100], Loss: 1.1567\n",
      "Epoch [96/100], Loss: 1.1535\n",
      "Epoch [97/100], Loss: 1.1564\n",
      "Epoch [98/100], Loss: 1.1557\n",
      "Epoch [99/100], Loss: 1.1560\n",
      "Epoch [100/100], Loss: 1.1547\n",
      "Subset 125000, Epoch [100/100], Loss: 1.1547\n",
      "Test Accuracy Logit Lipschitz: 55.97%\n",
      "Epoch [1/100], Loss: 1.5136\n",
      "Epoch [2/100], Loss: 1.3067\n",
      "Epoch [3/100], Loss: 1.2723\n",
      "Epoch [4/100], Loss: 1.2548\n",
      "Epoch [5/100], Loss: 1.2428\n",
      "Epoch [6/100], Loss: 1.2340\n",
      "Epoch [7/100], Loss: 1.2272\n",
      "Epoch [8/100], Loss: 1.2225\n",
      "Epoch [9/100], Loss: 1.2162\n",
      "Epoch [10/100], Loss: 1.2117\n",
      "Epoch [11/100], Loss: 1.2094\n",
      "Epoch [12/100], Loss: 1.2054\n",
      "Epoch [13/100], Loss: 1.2037\n",
      "Epoch [14/100], Loss: 1.2017\n",
      "Epoch [15/100], Loss: 1.1977\n",
      "Epoch [16/100], Loss: 1.1957\n",
      "Epoch [17/100], Loss: 1.1950\n",
      "Epoch [18/100], Loss: 1.1918\n",
      "Epoch [19/100], Loss: 1.1899\n",
      "Epoch [20/100], Loss: 1.1904\n",
      "Epoch [21/100], Loss: 1.1881\n",
      "Epoch [22/100], Loss: 1.1862\n",
      "Epoch [23/100], Loss: 1.1844\n",
      "Epoch [24/100], Loss: 1.1842\n",
      "Epoch [25/100], Loss: 1.1822\n",
      "Epoch [26/100], Loss: 1.1802\n",
      "Epoch [27/100], Loss: 1.1796\n",
      "Epoch [28/100], Loss: 1.1787\n",
      "Epoch [29/100], Loss: 1.1779\n",
      "Epoch [30/100], Loss: 1.1781\n",
      "Epoch [31/100], Loss: 1.1758\n",
      "Epoch [32/100], Loss: 1.1768\n",
      "Epoch [33/100], Loss: 1.1731\n",
      "Epoch [34/100], Loss: 1.1734\n",
      "Epoch [35/100], Loss: 1.1720\n",
      "Epoch [36/100], Loss: 1.1722\n",
      "Epoch [37/100], Loss: 1.1725\n",
      "Epoch [38/100], Loss: 1.1718\n",
      "Epoch [39/100], Loss: 1.1710\n",
      "Epoch [40/100], Loss: 1.1704\n",
      "Epoch [41/100], Loss: 1.1699\n",
      "Epoch [42/100], Loss: 1.1684\n",
      "Epoch [43/100], Loss: 1.1682\n",
      "Epoch [44/100], Loss: 1.1677\n",
      "Epoch [45/100], Loss: 1.1672\n",
      "Epoch [46/100], Loss: 1.1674\n",
      "Epoch [47/100], Loss: 1.1663\n",
      "Epoch [48/100], Loss: 1.1651\n",
      "Epoch [49/100], Loss: 1.1662\n",
      "Epoch [50/100], Loss: 1.1656\n",
      "Epoch [51/100], Loss: 1.1653\n",
      "Epoch [52/100], Loss: 1.1655\n",
      "Epoch [53/100], Loss: 1.1631\n",
      "Epoch [54/100], Loss: 1.1640\n",
      "Epoch [55/100], Loss: 1.1647\n",
      "Epoch [56/100], Loss: 1.1633\n",
      "Epoch [57/100], Loss: 1.1633\n",
      "Epoch [58/100], Loss: 1.1622\n",
      "Epoch [59/100], Loss: 1.1616\n",
      "Epoch [60/100], Loss: 1.1620\n",
      "Epoch [61/100], Loss: 1.1607\n",
      "Epoch [62/100], Loss: 1.1610\n",
      "Epoch [63/100], Loss: 1.1621\n",
      "Epoch [64/100], Loss: 1.1604\n",
      "Epoch [65/100], Loss: 1.1602\n",
      "Epoch [66/100], Loss: 1.1602\n",
      "Epoch [67/100], Loss: 1.1607\n",
      "Epoch [68/100], Loss: 1.1589\n",
      "Epoch [69/100], Loss: 1.1587\n",
      "Epoch [70/100], Loss: 1.1601\n",
      "Epoch [71/100], Loss: 1.1590\n",
      "Epoch [72/100], Loss: 1.1591\n",
      "Epoch [73/100], Loss: 1.1598\n",
      "Epoch [74/100], Loss: 1.1568\n",
      "Epoch [75/100], Loss: 1.1568\n",
      "Epoch [76/100], Loss: 1.1572\n",
      "Epoch [77/100], Loss: 1.1563\n",
      "Epoch [78/100], Loss: 1.1583\n",
      "Epoch [79/100], Loss: 1.1571\n",
      "Epoch [80/100], Loss: 1.1560\n",
      "Epoch [81/100], Loss: 1.1564\n",
      "Epoch [82/100], Loss: 1.1572\n",
      "Epoch [83/100], Loss: 1.1565\n",
      "Epoch [84/100], Loss: 1.1557\n",
      "Epoch [85/100], Loss: 1.1556\n",
      "Epoch [86/100], Loss: 1.1572\n",
      "Epoch [87/100], Loss: 1.1574\n",
      "Stopping early at epoch 87 (Loss improvement < 0.0001 for 3 epochs)\n",
      "Subset 125000, Epoch [87/100], Loss: 1.1574\n",
      "Test Accuracy Logit Lipschitz: 56.18%\n",
      "Epoch [1/100], Loss: 1.5224\n",
      "Epoch [2/100], Loss: 1.3145\n",
      "Epoch [3/100], Loss: 1.2820\n",
      "Epoch [4/100], Loss: 1.2651\n",
      "Epoch [5/100], Loss: 1.2525\n",
      "Epoch [6/100], Loss: 1.2431\n",
      "Epoch [7/100], Loss: 1.2365\n",
      "Epoch [8/100], Loss: 1.2309\n",
      "Epoch [9/100], Loss: 1.2271\n",
      "Epoch [10/100], Loss: 1.2211\n",
      "Epoch [11/100], Loss: 1.2184\n",
      "Epoch [12/100], Loss: 1.2153\n",
      "Epoch [13/100], Loss: 1.2118\n",
      "Epoch [14/100], Loss: 1.2102\n",
      "Epoch [15/100], Loss: 1.2078\n",
      "Epoch [16/100], Loss: 1.2056\n",
      "Epoch [17/100], Loss: 1.2026\n",
      "Epoch [18/100], Loss: 1.1999\n",
      "Epoch [19/100], Loss: 1.2004\n",
      "Epoch [20/100], Loss: 1.1977\n",
      "Epoch [21/100], Loss: 1.1972\n",
      "Epoch [22/100], Loss: 1.1954\n",
      "Epoch [23/100], Loss: 1.1926\n",
      "Epoch [24/100], Loss: 1.1931\n",
      "Epoch [25/100], Loss: 1.1920\n",
      "Epoch [26/100], Loss: 1.1912\n",
      "Epoch [27/100], Loss: 1.1889\n",
      "Epoch [28/100], Loss: 1.1878\n",
      "Epoch [29/100], Loss: 1.1867\n",
      "Epoch [30/100], Loss: 1.1834\n",
      "Epoch [31/100], Loss: 1.1857\n",
      "Epoch [32/100], Loss: 1.1831\n",
      "Epoch [33/100], Loss: 1.1843\n",
      "Epoch [34/100], Loss: 1.1840\n",
      "Epoch [35/100], Loss: 1.1818\n",
      "Epoch [36/100], Loss: 1.1809\n",
      "Epoch [37/100], Loss: 1.1793\n",
      "Epoch [38/100], Loss: 1.1787\n",
      "Epoch [39/100], Loss: 1.1797\n",
      "Epoch [40/100], Loss: 1.1778\n",
      "Epoch [41/100], Loss: 1.1790\n",
      "Epoch [42/100], Loss: 1.1771\n",
      "Epoch [43/100], Loss: 1.1764\n",
      "Epoch [44/100], Loss: 1.1758\n",
      "Epoch [45/100], Loss: 1.1767\n",
      "Epoch [46/100], Loss: 1.1764\n",
      "Epoch [47/100], Loss: 1.1759\n",
      "Epoch [48/100], Loss: 1.1746\n",
      "Epoch [49/100], Loss: 1.1753\n",
      "Epoch [50/100], Loss: 1.1742\n",
      "Epoch [51/100], Loss: 1.1730\n",
      "Epoch [52/100], Loss: 1.1736\n",
      "Epoch [53/100], Loss: 1.1733\n",
      "Epoch [54/100], Loss: 1.1706\n",
      "Epoch [55/100], Loss: 1.1716\n",
      "Epoch [56/100], Loss: 1.1725\n",
      "Epoch [57/100], Loss: 1.1724\n",
      "Stopping early at epoch 57 (Loss improvement < 0.0001 for 3 epochs)\n",
      "Subset 125000, Epoch [57/100], Loss: 1.1724\n",
      "Test Accuracy Logit Lipschitz: 55.77%\n",
      "Epoch [1/100], Loss: 1.5205\n",
      "Epoch [2/100], Loss: 1.3103\n",
      "Epoch [3/100], Loss: 1.2782\n",
      "Epoch [4/100], Loss: 1.2591\n",
      "Epoch [5/100], Loss: 1.2472\n",
      "Epoch [6/100], Loss: 1.2391\n",
      "Epoch [7/100], Loss: 1.2316\n",
      "Epoch [8/100], Loss: 1.2260\n",
      "Epoch [9/100], Loss: 1.2198\n",
      "Epoch [10/100], Loss: 1.2176\n",
      "Epoch [11/100], Loss: 1.2141\n",
      "Epoch [12/100], Loss: 1.2097\n",
      "Epoch [13/100], Loss: 1.2084\n",
      "Epoch [14/100], Loss: 1.2025\n",
      "Epoch [15/100], Loss: 1.2038\n",
      "Epoch [16/100], Loss: 1.1998\n",
      "Epoch [17/100], Loss: 1.1975\n",
      "Epoch [18/100], Loss: 1.1948\n",
      "Epoch [19/100], Loss: 1.1933\n",
      "Epoch [20/100], Loss: 1.1917\n",
      "Epoch [21/100], Loss: 1.1922\n",
      "Epoch [22/100], Loss: 1.1891\n",
      "Epoch [23/100], Loss: 1.1888\n",
      "Epoch [24/100], Loss: 1.1866\n",
      "Epoch [25/100], Loss: 1.1864\n",
      "Epoch [26/100], Loss: 1.1855\n",
      "Epoch [27/100], Loss: 1.1828\n",
      "Epoch [28/100], Loss: 1.1827\n",
      "Epoch [29/100], Loss: 1.1829\n",
      "Epoch [30/100], Loss: 1.1807\n",
      "Epoch [31/100], Loss: 1.1799\n",
      "Epoch [32/100], Loss: 1.1800\n",
      "Epoch [33/100], Loss: 1.1781\n",
      "Epoch [34/100], Loss: 1.1779\n",
      "Epoch [35/100], Loss: 1.1784\n",
      "Epoch [36/100], Loss: 1.1753\n",
      "Epoch [37/100], Loss: 1.1746\n",
      "Epoch [38/100], Loss: 1.1734\n",
      "Epoch [39/100], Loss: 1.1736\n",
      "Epoch [40/100], Loss: 1.1730\n",
      "Epoch [41/100], Loss: 1.1741\n",
      "Epoch [42/100], Loss: 1.1723\n",
      "Epoch [43/100], Loss: 1.1714\n",
      "Epoch [44/100], Loss: 1.1706\n",
      "Epoch [45/100], Loss: 1.1711\n",
      "Epoch [46/100], Loss: 1.1703\n",
      "Epoch [47/100], Loss: 1.1702\n",
      "Epoch [48/100], Loss: 1.1708\n",
      "Epoch [49/100], Loss: 1.1697\n",
      "Epoch [50/100], Loss: 1.1684\n",
      "Epoch [51/100], Loss: 1.1680\n",
      "Epoch [52/100], Loss: 1.1666\n",
      "Epoch [53/100], Loss: 1.1663\n",
      "Epoch [54/100], Loss: 1.1660\n",
      "Epoch [55/100], Loss: 1.1680\n",
      "Epoch [56/100], Loss: 1.1670\n",
      "Epoch [57/100], Loss: 1.1654\n",
      "Epoch [58/100], Loss: 1.1657\n",
      "Epoch [59/100], Loss: 1.1656\n",
      "Epoch [60/100], Loss: 1.1640\n",
      "Epoch [61/100], Loss: 1.1655\n",
      "Epoch [62/100], Loss: 1.1658\n",
      "Epoch [63/100], Loss: 1.1646\n",
      "Epoch [64/100], Loss: 1.1627\n",
      "Epoch [65/100], Loss: 1.1633\n",
      "Epoch [66/100], Loss: 1.1634\n",
      "Epoch [67/100], Loss: 1.1644\n",
      "Stopping early at epoch 67 (Loss improvement < 0.0001 for 3 epochs)\n",
      "Subset 125000, Epoch [67/100], Loss: 1.1644\n",
      "Test Accuracy Logit Lipschitz: 55.92%\n",
      "Epoch [1/100], Loss: 1.5186\n",
      "Epoch [2/100], Loss: 1.3094\n",
      "Epoch [3/100], Loss: 1.2758\n",
      "Epoch [4/100], Loss: 1.2594\n",
      "Epoch [5/100], Loss: 1.2478\n",
      "Epoch [6/100], Loss: 1.2398\n",
      "Epoch [7/100], Loss: 1.2301\n",
      "Epoch [8/100], Loss: 1.2254\n",
      "Epoch [9/100], Loss: 1.2198\n",
      "Epoch [10/100], Loss: 1.2160\n",
      "Epoch [11/100], Loss: 1.2139\n",
      "Epoch [12/100], Loss: 1.2088\n",
      "Epoch [13/100], Loss: 1.2074\n",
      "Epoch [14/100], Loss: 1.2039\n",
      "Epoch [15/100], Loss: 1.2022\n",
      "Epoch [16/100], Loss: 1.1989\n",
      "Epoch [17/100], Loss: 1.1968\n",
      "Epoch [18/100], Loss: 1.1964\n",
      "Epoch [19/100], Loss: 1.1963\n",
      "Epoch [20/100], Loss: 1.1929\n",
      "Epoch [21/100], Loss: 1.1921\n",
      "Epoch [22/100], Loss: 1.1883\n",
      "Epoch [23/100], Loss: 1.1889\n",
      "Epoch [24/100], Loss: 1.1860\n",
      "Epoch [25/100], Loss: 1.1860\n",
      "Epoch [26/100], Loss: 1.1860\n",
      "Epoch [27/100], Loss: 1.1830\n",
      "Epoch [28/100], Loss: 1.1821\n",
      "Epoch [29/100], Loss: 1.1817\n",
      "Epoch [30/100], Loss: 1.1812\n",
      "Epoch [31/100], Loss: 1.1792\n",
      "Epoch [32/100], Loss: 1.1782\n",
      "Epoch [33/100], Loss: 1.1794\n",
      "Epoch [34/100], Loss: 1.1771\n",
      "Epoch [35/100], Loss: 1.1774\n",
      "Epoch [36/100], Loss: 1.1760\n",
      "Epoch [37/100], Loss: 1.1754\n",
      "Epoch [38/100], Loss: 1.1730\n",
      "Epoch [39/100], Loss: 1.1725\n",
      "Epoch [40/100], Loss: 1.1727\n",
      "Epoch [41/100], Loss: 1.1724\n",
      "Epoch [42/100], Loss: 1.1701\n",
      "Epoch [43/100], Loss: 1.1724\n",
      "Epoch [44/100], Loss: 1.1704\n",
      "Epoch [45/100], Loss: 1.1707\n",
      "Epoch [46/100], Loss: 1.1702\n",
      "Epoch [47/100], Loss: 1.1694\n",
      "Epoch [48/100], Loss: 1.1682\n",
      "Epoch [49/100], Loss: 1.1690\n",
      "Epoch [50/100], Loss: 1.1692\n",
      "Epoch [51/100], Loss: 1.1682\n",
      "Epoch [52/100], Loss: 1.1681\n",
      "Epoch [53/100], Loss: 1.1669\n",
      "Epoch [54/100], Loss: 1.1656\n",
      "Epoch [55/100], Loss: 1.1645\n",
      "Epoch [56/100], Loss: 1.1643\n",
      "Epoch [57/100], Loss: 1.1663\n",
      "Epoch [58/100], Loss: 1.1643\n",
      "Epoch [59/100], Loss: 1.1642\n",
      "Epoch [60/100], Loss: 1.1630\n",
      "Epoch [61/100], Loss: 1.1654\n",
      "Epoch [62/100], Loss: 1.1650\n",
      "Epoch [63/100], Loss: 1.1638\n",
      "Epoch [64/100], Loss: 1.1624\n",
      "Epoch [65/100], Loss: 1.1634\n",
      "Epoch [66/100], Loss: 1.1627\n",
      "Epoch [67/100], Loss: 1.1634\n",
      "Epoch [68/100], Loss: 1.1635\n",
      "Epoch [69/100], Loss: 1.1612\n",
      "Epoch [70/100], Loss: 1.1602\n",
      "Epoch [71/100], Loss: 1.1602\n",
      "Epoch [72/100], Loss: 1.1634\n",
      "Epoch [73/100], Loss: 1.1611\n",
      "Epoch [74/100], Loss: 1.1617\n",
      "Epoch [75/100], Loss: 1.1606\n",
      "Epoch [76/100], Loss: 1.1592\n",
      "Epoch [77/100], Loss: 1.1591\n",
      "Epoch [78/100], Loss: 1.1607\n",
      "Epoch [79/100], Loss: 1.1603\n",
      "Epoch [80/100], Loss: 1.1598\n",
      "Epoch [81/100], Loss: 1.1582\n",
      "Epoch [82/100], Loss: 1.1596\n",
      "Epoch [83/100], Loss: 1.1599\n",
      "Epoch [84/100], Loss: 1.1591\n",
      "Epoch [85/100], Loss: 1.1601\n",
      "Epoch [86/100], Loss: 1.1579\n",
      "Epoch [87/100], Loss: 1.1581\n",
      "Epoch [88/100], Loss: 1.1586\n",
      "Epoch [89/100], Loss: 1.1579\n",
      "Epoch [90/100], Loss: 1.1580\n",
      "Epoch [91/100], Loss: 1.1566\n",
      "Epoch [92/100], Loss: 1.1580\n",
      "Epoch [93/100], Loss: 1.1605\n",
      "Epoch [94/100], Loss: 1.1574\n",
      "Epoch [95/100], Loss: 1.1577\n",
      "Epoch [96/100], Loss: 1.1578\n",
      "Epoch [97/100], Loss: 1.1579\n",
      "Stopping early at epoch 97 (Loss improvement < 0.0001 for 3 epochs)\n",
      "Subset 125000, Epoch [97/100], Loss: 1.1579\n",
      "Test Accuracy Logit Lipschitz: 55.24%\n",
      "Epoch [1/100], Loss: 1.5194\n",
      "Epoch [2/100], Loss: 1.3119\n",
      "Epoch [3/100], Loss: 1.2786\n",
      "Epoch [4/100], Loss: 1.2610\n",
      "Epoch [5/100], Loss: 1.2483\n",
      "Epoch [6/100], Loss: 1.2409\n",
      "Epoch [7/100], Loss: 1.2329\n",
      "Epoch [8/100], Loss: 1.2282\n",
      "Epoch [9/100], Loss: 1.2229\n",
      "Epoch [10/100], Loss: 1.2184\n",
      "Epoch [11/100], Loss: 1.2145\n",
      "Epoch [12/100], Loss: 1.2115\n",
      "Epoch [13/100], Loss: 1.2092\n",
      "Epoch [14/100], Loss: 1.2051\n",
      "Epoch [15/100], Loss: 1.2037\n",
      "Epoch [16/100], Loss: 1.2026\n",
      "Epoch [17/100], Loss: 1.2000\n",
      "Epoch [18/100], Loss: 1.1993\n",
      "Epoch [19/100], Loss: 1.1973\n",
      "Epoch [20/100], Loss: 1.1937\n",
      "Epoch [21/100], Loss: 1.1940\n",
      "Epoch [22/100], Loss: 1.1917\n",
      "Epoch [23/100], Loss: 1.1895\n",
      "Epoch [24/100], Loss: 1.1877\n",
      "Epoch [25/100], Loss: 1.1875\n",
      "Epoch [26/100], Loss: 1.1869\n",
      "Epoch [27/100], Loss: 1.1857\n",
      "Epoch [28/100], Loss: 1.1844\n",
      "Epoch [29/100], Loss: 1.1838\n",
      "Epoch [30/100], Loss: 1.1826\n",
      "Epoch [31/100], Loss: 1.1824\n",
      "Epoch [32/100], Loss: 1.1828\n",
      "Epoch [33/100], Loss: 1.1809\n",
      "Epoch [34/100], Loss: 1.1801\n",
      "Epoch [35/100], Loss: 1.1789\n",
      "Epoch [36/100], Loss: 1.1787\n",
      "Epoch [37/100], Loss: 1.1784\n",
      "Epoch [38/100], Loss: 1.1770\n",
      "Epoch [39/100], Loss: 1.1758\n",
      "Epoch [40/100], Loss: 1.1748\n",
      "Epoch [41/100], Loss: 1.1768\n",
      "Epoch [42/100], Loss: 1.1761\n",
      "Epoch [43/100], Loss: 1.1746\n",
      "Epoch [44/100], Loss: 1.1733\n",
      "Epoch [45/100], Loss: 1.1739\n",
      "Epoch [46/100], Loss: 1.1736\n",
      "Epoch [47/100], Loss: 1.1721\n",
      "Epoch [48/100], Loss: 1.1706\n",
      "Epoch [49/100], Loss: 1.1718\n",
      "Epoch [50/100], Loss: 1.1703\n",
      "Epoch [51/100], Loss: 1.1726\n",
      "Epoch [52/100], Loss: 1.1697\n",
      "Epoch [53/100], Loss: 1.1716\n",
      "Epoch [54/100], Loss: 1.1696\n",
      "Epoch [55/100], Loss: 1.1694\n",
      "Epoch [56/100], Loss: 1.1682\n",
      "Epoch [57/100], Loss: 1.1681\n",
      "Epoch [58/100], Loss: 1.1676\n",
      "Epoch [59/100], Loss: 1.1670\n",
      "Epoch [60/100], Loss: 1.1671\n",
      "Epoch [61/100], Loss: 1.1683\n",
      "Epoch [62/100], Loss: 1.1654\n",
      "Epoch [63/100], Loss: 1.1649\n",
      "Epoch [64/100], Loss: 1.1650\n",
      "Epoch [65/100], Loss: 1.1657\n",
      "Epoch [66/100], Loss: 1.1669\n",
      "Stopping early at epoch 66 (Loss improvement < 0.0001 for 3 epochs)\n",
      "Subset 125000, Epoch [66/100], Loss: 1.1669\n",
      "Test Accuracy Logit Lipschitz: 56.40%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.6231\n",
      "Epoch [2/100], Loss: 1.3364\n",
      "Epoch [3/100], Loss: 1.2909\n",
      "Epoch [4/100], Loss: 1.2646\n",
      "Epoch [5/100], Loss: 1.2489\n",
      "Epoch [6/100], Loss: 1.2353\n",
      "Epoch [7/100], Loss: 1.2256\n",
      "Epoch [8/100], Loss: 1.2181\n",
      "Epoch [9/100], Loss: 1.2106\n",
      "Epoch [10/100], Loss: 1.2050\n",
      "Epoch [11/100], Loss: 1.1975\n",
      "Epoch [12/100], Loss: 1.1946\n",
      "Epoch [13/100], Loss: 1.1912\n",
      "Epoch [14/100], Loss: 1.1856\n",
      "Epoch [15/100], Loss: 1.1818\n",
      "Epoch [16/100], Loss: 1.1779\n",
      "Epoch [17/100], Loss: 1.1754\n",
      "Epoch [18/100], Loss: 1.1718\n",
      "Epoch [19/100], Loss: 1.1687\n",
      "Epoch [20/100], Loss: 1.1677\n",
      "Epoch [21/100], Loss: 1.1666\n",
      "Epoch [22/100], Loss: 1.1604\n",
      "Epoch [23/100], Loss: 1.1586\n",
      "Epoch [24/100], Loss: 1.1588\n",
      "Epoch [25/100], Loss: 1.1548\n",
      "Epoch [26/100], Loss: 1.1538\n",
      "Epoch [27/100], Loss: 1.1517\n",
      "Epoch [28/100], Loss: 1.1514\n",
      "Epoch [29/100], Loss: 1.1467\n",
      "Epoch [30/100], Loss: 1.1463\n",
      "Epoch [31/100], Loss: 1.1441\n",
      "Epoch [32/100], Loss: 1.1462\n",
      "Epoch [33/100], Loss: 1.1437\n",
      "Epoch [34/100], Loss: 1.1424\n",
      "Epoch [35/100], Loss: 1.1386\n",
      "Epoch [36/100], Loss: 1.1380\n",
      "Epoch [37/100], Loss: 1.1370\n",
      "Epoch [38/100], Loss: 1.1342\n",
      "Epoch [39/100], Loss: 1.1351\n",
      "Epoch [40/100], Loss: 1.1350\n",
      "Epoch [41/100], Loss: 1.1356\n",
      "Stopping early at epoch 41 (Loss improvement < 0.0001 for 3 epochs)\n",
      "Subset 75000, Epoch [41/100], Loss: 1.1356\n",
      "Test Accuracy Logit Lipschitz: 55.87%\n",
      "Epoch [1/100], Loss: 1.6186\n",
      "Epoch [2/100], Loss: 1.3369\n",
      "Epoch [3/100], Loss: 1.2898\n",
      "Epoch [4/100], Loss: 1.2635\n",
      "Epoch [5/100], Loss: 1.2472\n",
      "Epoch [6/100], Loss: 1.2335\n",
      "Epoch [7/100], Loss: 1.2230\n",
      "Epoch [8/100], Loss: 1.2139\n",
      "Epoch [9/100], Loss: 1.2104\n",
      "Epoch [10/100], Loss: 1.2025\n",
      "Epoch [11/100], Loss: 1.1987\n",
      "Epoch [12/100], Loss: 1.1927\n",
      "Epoch [13/100], Loss: 1.1894\n",
      "Epoch [14/100], Loss: 1.1827\n",
      "Epoch [15/100], Loss: 1.1784\n",
      "Epoch [16/100], Loss: 1.1725\n",
      "Epoch [17/100], Loss: 1.1711\n",
      "Epoch [18/100], Loss: 1.1700\n",
      "Epoch [19/100], Loss: 1.1640\n",
      "Epoch [20/100], Loss: 1.1639\n",
      "Epoch [21/100], Loss: 1.1600\n",
      "Epoch [22/100], Loss: 1.1608\n",
      "Epoch [23/100], Loss: 1.1575\n",
      "Epoch [24/100], Loss: 1.1555\n",
      "Epoch [25/100], Loss: 1.1537\n",
      "Epoch [26/100], Loss: 1.1512\n",
      "Epoch [27/100], Loss: 1.1488\n",
      "Epoch [28/100], Loss: 1.1465\n",
      "Epoch [29/100], Loss: 1.1460\n",
      "Epoch [30/100], Loss: 1.1426\n",
      "Epoch [31/100], Loss: 1.1421\n",
      "Epoch [32/100], Loss: 1.1423\n",
      "Epoch [33/100], Loss: 1.1390\n",
      "Epoch [34/100], Loss: 1.1386\n",
      "Epoch [35/100], Loss: 1.1368\n",
      "Epoch [36/100], Loss: 1.1357\n",
      "Epoch [37/100], Loss: 1.1344\n",
      "Epoch [38/100], Loss: 1.1318\n",
      "Epoch [39/100], Loss: 1.1332\n",
      "Epoch [40/100], Loss: 1.1341\n",
      "Epoch [41/100], Loss: 1.1328\n",
      "Epoch [42/100], Loss: 1.1273\n",
      "Epoch [43/100], Loss: 1.1285\n",
      "Epoch [44/100], Loss: 1.1306\n",
      "Epoch [45/100], Loss: 1.1251\n",
      "Epoch [46/100], Loss: 1.1265\n",
      "Epoch [47/100], Loss: 1.1247\n",
      "Epoch [48/100], Loss: 1.1226\n",
      "Epoch [49/100], Loss: 1.1235\n",
      "Epoch [50/100], Loss: 1.1214\n",
      "Epoch [51/100], Loss: 1.1225\n",
      "Epoch [52/100], Loss: 1.1206\n",
      "Epoch [53/100], Loss: 1.1192\n",
      "Epoch [54/100], Loss: 1.1167\n",
      "Epoch [55/100], Loss: 1.1177\n",
      "Epoch [56/100], Loss: 1.1177\n",
      "Epoch [57/100], Loss: 1.1154\n",
      "Epoch [58/100], Loss: 1.1162\n",
      "Epoch [59/100], Loss: 1.1155\n",
      "Epoch [60/100], Loss: 1.1151\n",
      "Epoch [61/100], Loss: 1.1147\n",
      "Epoch [62/100], Loss: 1.1152\n",
      "Epoch [63/100], Loss: 1.1147\n",
      "Epoch [64/100], Loss: 1.1131\n",
      "Epoch [65/100], Loss: 1.1138\n",
      "Epoch [66/100], Loss: 1.1122\n",
      "Epoch [67/100], Loss: 1.1106\n",
      "Epoch [68/100], Loss: 1.1099\n",
      "Epoch [69/100], Loss: 1.1110\n",
      "Epoch [70/100], Loss: 1.1117\n",
      "Epoch [71/100], Loss: 1.1079\n",
      "Epoch [72/100], Loss: 1.1080\n",
      "Epoch [73/100], Loss: 1.1094\n",
      "Epoch [74/100], Loss: 1.1063\n",
      "Epoch [75/100], Loss: 1.1086\n",
      "Epoch [76/100], Loss: 1.1082\n",
      "Epoch [77/100], Loss: 1.1077\n",
      "Epoch [78/100], Loss: 1.1059\n",
      "Epoch [79/100], Loss: 1.1050\n",
      "Epoch [80/100], Loss: 1.1047\n",
      "Epoch [81/100], Loss: 1.1028\n",
      "Epoch [82/100], Loss: 1.1033\n",
      "Epoch [83/100], Loss: 1.1044\n",
      "Epoch [84/100], Loss: 1.1019\n",
      "Epoch [85/100], Loss: 1.1028\n",
      "Epoch [86/100], Loss: 1.1027\n",
      "Epoch [87/100], Loss: 1.1019\n",
      "Epoch [88/100], Loss: 1.1003\n",
      "Epoch [89/100], Loss: 1.1045\n",
      "Epoch [90/100], Loss: 1.1010\n",
      "Epoch [91/100], Loss: 1.1008\n",
      "Epoch [92/100], Loss: 1.1022\n",
      "Epoch [93/100], Loss: 1.1007\n",
      "Epoch [94/100], Loss: 1.0989\n",
      "Epoch [95/100], Loss: 1.0998\n",
      "Epoch [96/100], Loss: 1.0984\n",
      "Epoch [97/100], Loss: 1.0968\n",
      "Epoch [98/100], Loss: 1.0985\n",
      "Epoch [99/100], Loss: 1.0976\n",
      "Epoch [100/100], Loss: 1.0988\n",
      "Subset 75000, Epoch [100/100], Loss: 1.0988\n",
      "Test Accuracy Logit Lipschitz: 54.44%\n",
      "Epoch [1/100], Loss: 1.6239\n",
      "Epoch [2/100], Loss: 1.3394\n",
      "Epoch [3/100], Loss: 1.2975\n",
      "Epoch [4/100], Loss: 1.2704\n",
      "Epoch [5/100], Loss: 1.2539\n",
      "Epoch [6/100], Loss: 1.2400\n",
      "Epoch [7/100], Loss: 1.2299\n",
      "Epoch [8/100], Loss: 1.2224\n",
      "Epoch [9/100], Loss: 1.2153\n",
      "Epoch [10/100], Loss: 1.2084\n",
      "Epoch [11/100], Loss: 1.2028\n",
      "Epoch [12/100], Loss: 1.1979\n",
      "Epoch [13/100], Loss: 1.1935\n",
      "Epoch [14/100], Loss: 1.1899\n",
      "Epoch [15/100], Loss: 1.1865\n",
      "Epoch [16/100], Loss: 1.1822\n",
      "Epoch [17/100], Loss: 1.1768\n",
      "Epoch [18/100], Loss: 1.1777\n",
      "Epoch [19/100], Loss: 1.1738\n",
      "Epoch [20/100], Loss: 1.1723\n",
      "Epoch [21/100], Loss: 1.1684\n",
      "Epoch [22/100], Loss: 1.1660\n",
      "Epoch [23/100], Loss: 1.1634\n",
      "Epoch [24/100], Loss: 1.1612\n",
      "Epoch [25/100], Loss: 1.1597\n",
      "Epoch [26/100], Loss: 1.1580\n",
      "Epoch [27/100], Loss: 1.1557\n",
      "Epoch [28/100], Loss: 1.1533\n",
      "Epoch [29/100], Loss: 1.1505\n",
      "Epoch [30/100], Loss: 1.1528\n",
      "Epoch [31/100], Loss: 1.1470\n",
      "Epoch [32/100], Loss: 1.1482\n",
      "Epoch [33/100], Loss: 1.1469\n",
      "Epoch [34/100], Loss: 1.1456\n",
      "Epoch [35/100], Loss: 1.1428\n",
      "Epoch [36/100], Loss: 1.1408\n",
      "Epoch [37/100], Loss: 1.1398\n",
      "Epoch [38/100], Loss: 1.1404\n",
      "Epoch [39/100], Loss: 1.1378\n",
      "Epoch [40/100], Loss: 1.1364\n",
      "Epoch [41/100], Loss: 1.1371\n",
      "Epoch [42/100], Loss: 1.1326\n",
      "Epoch [43/100], Loss: 1.1320\n",
      "Epoch [44/100], Loss: 1.1327\n",
      "Epoch [45/100], Loss: 1.1320\n",
      "Epoch [46/100], Loss: 1.1307\n",
      "Epoch [47/100], Loss: 1.1305\n",
      "Epoch [48/100], Loss: 1.1310\n",
      "Epoch [49/100], Loss: 1.1284\n",
      "Epoch [50/100], Loss: 1.1269\n",
      "Epoch [51/100], Loss: 1.1274\n",
      "Epoch [52/100], Loss: 1.1266\n",
      "Epoch [53/100], Loss: 1.1248\n",
      "Epoch [54/100], Loss: 1.1244\n",
      "Epoch [55/100], Loss: 1.1230\n",
      "Epoch [56/100], Loss: 1.1226\n",
      "Epoch [57/100], Loss: 1.1229\n",
      "Epoch [58/100], Loss: 1.1214\n",
      "Epoch [59/100], Loss: 1.1201\n",
      "Epoch [60/100], Loss: 1.1202\n",
      "Epoch [61/100], Loss: 1.1179\n",
      "Epoch [62/100], Loss: 1.1210\n",
      "Epoch [63/100], Loss: 1.1174\n",
      "Epoch [64/100], Loss: 1.1165\n",
      "Epoch [65/100], Loss: 1.1175\n",
      "Epoch [66/100], Loss: 1.1173\n",
      "Epoch [67/100], Loss: 1.1178\n",
      "Epoch [68/100], Loss: 1.1145\n",
      "Epoch [69/100], Loss: 1.1150\n",
      "Epoch [70/100], Loss: 1.1163\n",
      "Epoch [71/100], Loss: 1.1141\n",
      "Epoch [72/100], Loss: 1.1146\n",
      "Epoch [73/100], Loss: 1.1139\n",
      "Epoch [74/100], Loss: 1.1120\n",
      "Epoch [75/100], Loss: 1.1139\n",
      "Epoch [76/100], Loss: 1.1127\n",
      "Epoch [77/100], Loss: 1.1098\n",
      "Epoch [78/100], Loss: 1.1112\n",
      "Epoch [79/100], Loss: 1.1114\n",
      "Epoch [80/100], Loss: 1.1106\n",
      "Epoch [81/100], Loss: 1.1121\n",
      "Epoch [82/100], Loss: 1.1114\n",
      "Epoch [83/100], Loss: 1.1104\n",
      "Epoch [84/100], Loss: 1.1097\n",
      "Epoch [85/100], Loss: 1.1090\n",
      "Epoch [86/100], Loss: 1.1076\n",
      "Epoch [87/100], Loss: 1.1087\n",
      "Epoch [88/100], Loss: 1.1054\n",
      "Epoch [89/100], Loss: 1.1053\n",
      "Epoch [90/100], Loss: 1.1052\n",
      "Epoch [91/100], Loss: 1.1056\n",
      "Epoch [92/100], Loss: 1.1065\n",
      "Epoch [93/100], Loss: 1.1063\n",
      "Epoch [94/100], Loss: 1.1056\n",
      "Epoch [95/100], Loss: 1.1056\n",
      "Epoch [96/100], Loss: 1.1037\n",
      "Epoch [97/100], Loss: 1.1034\n",
      "Epoch [98/100], Loss: 1.1036\n",
      "Epoch [99/100], Loss: 1.1050\n",
      "Epoch [100/100], Loss: 1.1018\n",
      "Subset 75000, Epoch [100/100], Loss: 1.1018\n",
      "Test Accuracy Logit Lipschitz: 54.41%\n",
      "Epoch [1/100], Loss: 1.6334\n",
      "Epoch [2/100], Loss: 1.3472\n",
      "Epoch [3/100], Loss: 1.3003\n",
      "Epoch [4/100], Loss: 1.2754\n",
      "Epoch [5/100], Loss: 1.2575\n",
      "Epoch [6/100], Loss: 1.2454\n",
      "Epoch [7/100], Loss: 1.2327\n",
      "Epoch [8/100], Loss: 1.2240\n",
      "Epoch [9/100], Loss: 1.2160\n",
      "Epoch [10/100], Loss: 1.2106\n",
      "Epoch [11/100], Loss: 1.2063\n",
      "Epoch [12/100], Loss: 1.2023\n",
      "Epoch [13/100], Loss: 1.1963\n",
      "Epoch [14/100], Loss: 1.1921\n",
      "Epoch [15/100], Loss: 1.1884\n",
      "Epoch [16/100], Loss: 1.1838\n",
      "Epoch [17/100], Loss: 1.1798\n",
      "Epoch [18/100], Loss: 1.1783\n",
      "Epoch [19/100], Loss: 1.1764\n",
      "Epoch [20/100], Loss: 1.1731\n",
      "Epoch [21/100], Loss: 1.1686\n",
      "Epoch [22/100], Loss: 1.1672\n",
      "Epoch [23/100], Loss: 1.1662\n",
      "Epoch [24/100], Loss: 1.1641\n",
      "Epoch [25/100], Loss: 1.1615\n",
      "Epoch [26/100], Loss: 1.1595\n",
      "Epoch [27/100], Loss: 1.1595\n",
      "Epoch [28/100], Loss: 1.1558\n",
      "Epoch [29/100], Loss: 1.1553\n",
      "Epoch [30/100], Loss: 1.1522\n",
      "Epoch [31/100], Loss: 1.1515\n",
      "Epoch [32/100], Loss: 1.1498\n",
      "Epoch [33/100], Loss: 1.1482\n",
      "Epoch [34/100], Loss: 1.1474\n",
      "Epoch [35/100], Loss: 1.1449\n",
      "Epoch [36/100], Loss: 1.1451\n",
      "Epoch [37/100], Loss: 1.1425\n",
      "Epoch [38/100], Loss: 1.1428\n",
      "Epoch [39/100], Loss: 1.1414\n",
      "Epoch [40/100], Loss: 1.1387\n",
      "Epoch [41/100], Loss: 1.1390\n",
      "Epoch [42/100], Loss: 1.1369\n",
      "Epoch [43/100], Loss: 1.1369\n",
      "Epoch [44/100], Loss: 1.1340\n",
      "Epoch [45/100], Loss: 1.1314\n",
      "Epoch [46/100], Loss: 1.1347\n",
      "Epoch [47/100], Loss: 1.1328\n",
      "Epoch [48/100], Loss: 1.1317\n",
      "Epoch [49/100], Loss: 1.1315\n",
      "Epoch [50/100], Loss: 1.1302\n",
      "Epoch [51/100], Loss: 1.1306\n",
      "Epoch [52/100], Loss: 1.1283\n",
      "Epoch [53/100], Loss: 1.1288\n",
      "Epoch [54/100], Loss: 1.1275\n",
      "Epoch [55/100], Loss: 1.1272\n",
      "Epoch [56/100], Loss: 1.1244\n",
      "Epoch [57/100], Loss: 1.1247\n",
      "Epoch [58/100], Loss: 1.1251\n",
      "Epoch [59/100], Loss: 1.1239\n",
      "Epoch [60/100], Loss: 1.1239\n",
      "Epoch [61/100], Loss: 1.1209\n",
      "Epoch [62/100], Loss: 1.1225\n",
      "Epoch [63/100], Loss: 1.1208\n",
      "Epoch [64/100], Loss: 1.1193\n",
      "Epoch [65/100], Loss: 1.1205\n",
      "Epoch [66/100], Loss: 1.1186\n",
      "Epoch [67/100], Loss: 1.1187\n",
      "Epoch [68/100], Loss: 1.1176\n",
      "Epoch [69/100], Loss: 1.1187\n",
      "Epoch [70/100], Loss: 1.1168\n",
      "Epoch [71/100], Loss: 1.1176\n",
      "Epoch [72/100], Loss: 1.1163\n",
      "Epoch [73/100], Loss: 1.1150\n",
      "Epoch [74/100], Loss: 1.1147\n",
      "Epoch [75/100], Loss: 1.1131\n",
      "Epoch [76/100], Loss: 1.1154\n",
      "Epoch [77/100], Loss: 1.1121\n",
      "Epoch [78/100], Loss: 1.1158\n",
      "Epoch [79/100], Loss: 1.1124\n",
      "Epoch [80/100], Loss: 1.1137\n",
      "Epoch [81/100], Loss: 1.1128\n",
      "Epoch [82/100], Loss: 1.1111\n",
      "Epoch [83/100], Loss: 1.1121\n",
      "Epoch [84/100], Loss: 1.1111\n",
      "Epoch [85/100], Loss: 1.1100\n",
      "Epoch [86/100], Loss: 1.1124\n",
      "Epoch [87/100], Loss: 1.1103\n",
      "Epoch [88/100], Loss: 1.1090\n",
      "Epoch [89/100], Loss: 1.1106\n",
      "Epoch [90/100], Loss: 1.1088\n",
      "Epoch [91/100], Loss: 1.1095\n",
      "Epoch [92/100], Loss: 1.1072\n",
      "Epoch [93/100], Loss: 1.1052\n",
      "Epoch [94/100], Loss: 1.1067\n",
      "Epoch [95/100], Loss: 1.1060\n",
      "Epoch [96/100], Loss: 1.1053\n",
      "Epoch [97/100], Loss: 1.1073\n",
      "Epoch [98/100], Loss: 1.1069\n",
      "Epoch [99/100], Loss: 1.1068\n",
      "Epoch [100/100], Loss: 1.1059\n",
      "Subset 75000, Epoch [100/100], Loss: 1.1059\n",
      "Test Accuracy Logit Lipschitz: 54.72%\n",
      "Epoch [1/100], Loss: 1.6188\n",
      "Epoch [2/100], Loss: 1.3308\n",
      "Epoch [3/100], Loss: 1.2866\n",
      "Epoch [4/100], Loss: 1.2616\n",
      "Epoch [5/100], Loss: 1.2419\n",
      "Epoch [6/100], Loss: 1.2306\n",
      "Epoch [7/100], Loss: 1.2212\n",
      "Epoch [8/100], Loss: 1.2123\n",
      "Epoch [9/100], Loss: 1.2073\n",
      "Epoch [10/100], Loss: 1.1999\n",
      "Epoch [11/100], Loss: 1.1940\n",
      "Epoch [12/100], Loss: 1.1907\n",
      "Epoch [13/100], Loss: 1.1851\n",
      "Epoch [14/100], Loss: 1.1802\n",
      "Epoch [15/100], Loss: 1.1789\n",
      "Epoch [16/100], Loss: 1.1717\n",
      "Epoch [17/100], Loss: 1.1694\n",
      "Epoch [18/100], Loss: 1.1654\n",
      "Epoch [19/100], Loss: 1.1649\n",
      "Epoch [20/100], Loss: 1.1608\n",
      "Epoch [21/100], Loss: 1.1591\n",
      "Epoch [22/100], Loss: 1.1554\n",
      "Epoch [23/100], Loss: 1.1537\n",
      "Epoch [24/100], Loss: 1.1516\n",
      "Epoch [25/100], Loss: 1.1507\n",
      "Epoch [26/100], Loss: 1.1462\n",
      "Epoch [27/100], Loss: 1.1487\n",
      "Epoch [28/100], Loss: 1.1447\n",
      "Epoch [29/100], Loss: 1.1455\n",
      "Epoch [30/100], Loss: 1.1419\n",
      "Epoch [31/100], Loss: 1.1415\n",
      "Epoch [32/100], Loss: 1.1390\n",
      "Epoch [33/100], Loss: 1.1363\n",
      "Epoch [34/100], Loss: 1.1368\n",
      "Epoch [35/100], Loss: 1.1336\n",
      "Epoch [36/100], Loss: 1.1321\n",
      "Epoch [37/100], Loss: 1.1304\n",
      "Epoch [38/100], Loss: 1.1331\n",
      "Epoch [39/100], Loss: 1.1314\n",
      "Epoch [40/100], Loss: 1.1276\n",
      "Epoch [41/100], Loss: 1.1259\n",
      "Epoch [42/100], Loss: 1.1265\n",
      "Epoch [43/100], Loss: 1.1266\n",
      "Epoch [44/100], Loss: 1.1255\n",
      "Epoch [45/100], Loss: 1.1230\n",
      "Epoch [46/100], Loss: 1.1224\n",
      "Epoch [47/100], Loss: 1.1222\n",
      "Epoch [48/100], Loss: 1.1206\n",
      "Epoch [49/100], Loss: 1.1207\n",
      "Epoch [50/100], Loss: 1.1169\n",
      "Epoch [51/100], Loss: 1.1188\n",
      "Epoch [52/100], Loss: 1.1179\n",
      "Epoch [53/100], Loss: 1.1167\n",
      "Epoch [54/100], Loss: 1.1154\n",
      "Epoch [55/100], Loss: 1.1154\n",
      "Epoch [56/100], Loss: 1.1148\n",
      "Epoch [57/100], Loss: 1.1127\n",
      "Epoch [58/100], Loss: 1.1132\n",
      "Epoch [59/100], Loss: 1.1135\n",
      "Epoch [60/100], Loss: 1.1114\n",
      "Epoch [61/100], Loss: 1.1128\n",
      "Epoch [62/100], Loss: 1.1111\n",
      "Epoch [63/100], Loss: 1.1119\n",
      "Epoch [64/100], Loss: 1.1105\n",
      "Epoch [65/100], Loss: 1.1093\n",
      "Epoch [66/100], Loss: 1.1082\n",
      "Epoch [67/100], Loss: 1.1093\n",
      "Epoch [68/100], Loss: 1.1073\n",
      "Epoch [69/100], Loss: 1.1073\n",
      "Epoch [70/100], Loss: 1.1077\n",
      "Epoch [71/100], Loss: 1.1062\n",
      "Epoch [72/100], Loss: 1.1052\n",
      "Epoch [73/100], Loss: 1.1072\n",
      "Epoch [74/100], Loss: 1.1041\n",
      "Epoch [75/100], Loss: 1.1048\n",
      "Epoch [76/100], Loss: 1.1036\n",
      "Epoch [77/100], Loss: 1.1057\n",
      "Epoch [78/100], Loss: 1.1033\n",
      "Epoch [79/100], Loss: 1.1018\n",
      "Epoch [80/100], Loss: 1.1022\n",
      "Epoch [81/100], Loss: 1.1006\n",
      "Epoch [82/100], Loss: 1.1006\n",
      "Epoch [83/100], Loss: 1.1010\n",
      "Epoch [84/100], Loss: 1.0997\n",
      "Epoch [85/100], Loss: 1.1002\n",
      "Epoch [86/100], Loss: 1.1005\n",
      "Epoch [87/100], Loss: 1.1002\n",
      "Epoch [88/100], Loss: 1.0986\n",
      "Epoch [89/100], Loss: 1.0993\n",
      "Epoch [90/100], Loss: 1.0985\n",
      "Epoch [91/100], Loss: 1.0979\n",
      "Epoch [92/100], Loss: 1.0986\n",
      "Epoch [93/100], Loss: 1.0970\n",
      "Epoch [94/100], Loss: 1.0959\n",
      "Epoch [95/100], Loss: 1.0966\n",
      "Epoch [96/100], Loss: 1.0978\n",
      "Epoch [97/100], Loss: 1.0973\n",
      "Epoch [98/100], Loss: 1.0968\n",
      "Epoch [99/100], Loss: 1.0964\n",
      "Epoch [100/100], Loss: 1.0943\n",
      "Subset 75000, Epoch [100/100], Loss: 1.0943\n",
      "Test Accuracy Logit Lipschitz: 54.54%\n",
      "Epoch [1/100], Loss: 1.6159\n",
      "Epoch [2/100], Loss: 1.3314\n",
      "Epoch [3/100], Loss: 1.2875\n",
      "Epoch [4/100], Loss: 1.2615\n",
      "Epoch [5/100], Loss: 1.2438\n",
      "Epoch [6/100], Loss: 1.2296\n",
      "Epoch [7/100], Loss: 1.2192\n",
      "Epoch [8/100], Loss: 1.2097\n",
      "Epoch [9/100], Loss: 1.2053\n",
      "Epoch [10/100], Loss: 1.1959\n",
      "Epoch [11/100], Loss: 1.1934\n",
      "Epoch [12/100], Loss: 1.1860\n",
      "Epoch [13/100], Loss: 1.1850\n",
      "Epoch [14/100], Loss: 1.1788\n",
      "Epoch [15/100], Loss: 1.1763\n",
      "Epoch [16/100], Loss: 1.1722\n",
      "Epoch [17/100], Loss: 1.1674\n",
      "Epoch [18/100], Loss: 1.1653\n",
      "Epoch [19/100], Loss: 1.1650\n",
      "Epoch [20/100], Loss: 1.1600\n",
      "Epoch [21/100], Loss: 1.1570\n",
      "Epoch [22/100], Loss: 1.1553\n",
      "Epoch [23/100], Loss: 1.1547\n",
      "Epoch [24/100], Loss: 1.1504\n",
      "Epoch [25/100], Loss: 1.1479\n",
      "Epoch [26/100], Loss: 1.1457\n",
      "Epoch [27/100], Loss: 1.1436\n",
      "Epoch [28/100], Loss: 1.1411\n",
      "Epoch [29/100], Loss: 1.1413\n",
      "Epoch [30/100], Loss: 1.1407\n",
      "Epoch [31/100], Loss: 1.1397\n",
      "Epoch [32/100], Loss: 1.1368\n",
      "Epoch [33/100], Loss: 1.1354\n",
      "Epoch [34/100], Loss: 1.1362\n",
      "Epoch [35/100], Loss: 1.1326\n",
      "Epoch [36/100], Loss: 1.1316\n",
      "Epoch [37/100], Loss: 1.1310\n",
      "Epoch [38/100], Loss: 1.1302\n",
      "Epoch [39/100], Loss: 1.1281\n",
      "Epoch [40/100], Loss: 1.1249\n",
      "Epoch [41/100], Loss: 1.1262\n",
      "Epoch [42/100], Loss: 1.1243\n",
      "Epoch [43/100], Loss: 1.1222\n",
      "Epoch [44/100], Loss: 1.1230\n",
      "Epoch [45/100], Loss: 1.1226\n",
      "Epoch [46/100], Loss: 1.1213\n",
      "Epoch [47/100], Loss: 1.1193\n",
      "Epoch [48/100], Loss: 1.1199\n",
      "Epoch [49/100], Loss: 1.1195\n",
      "Epoch [50/100], Loss: 1.1194\n",
      "Epoch [51/100], Loss: 1.1177\n",
      "Epoch [52/100], Loss: 1.1163\n",
      "Epoch [53/100], Loss: 1.1168\n",
      "Epoch [54/100], Loss: 1.1129\n",
      "Epoch [55/100], Loss: 1.1127\n",
      "Epoch [56/100], Loss: 1.1126\n",
      "Epoch [57/100], Loss: 1.1119\n",
      "Epoch [58/100], Loss: 1.1108\n",
      "Epoch [59/100], Loss: 1.1103\n",
      "Epoch [60/100], Loss: 1.1107\n",
      "Epoch [61/100], Loss: 1.1088\n",
      "Epoch [62/100], Loss: 1.1100\n",
      "Epoch [63/100], Loss: 1.1103\n",
      "Epoch [64/100], Loss: 1.1079\n",
      "Epoch [65/100], Loss: 1.1070\n",
      "Epoch [66/100], Loss: 1.1082\n",
      "Epoch [67/100], Loss: 1.1076\n",
      "Epoch [68/100], Loss: 1.1063\n",
      "Epoch [69/100], Loss: 1.1056\n",
      "Epoch [70/100], Loss: 1.1036\n",
      "Epoch [71/100], Loss: 1.1053\n",
      "Epoch [72/100], Loss: 1.1055\n",
      "Epoch [73/100], Loss: 1.1041\n",
      "Epoch [74/100], Loss: 1.1020\n",
      "Epoch [75/100], Loss: 1.1027\n",
      "Epoch [76/100], Loss: 1.1045\n",
      "Epoch [77/100], Loss: 1.1002\n",
      "Epoch [78/100], Loss: 1.1014\n",
      "Epoch [79/100], Loss: 1.1013\n",
      "Epoch [80/100], Loss: 1.1004\n",
      "Epoch [81/100], Loss: 1.0991\n",
      "Epoch [82/100], Loss: 1.1011\n",
      "Epoch [83/100], Loss: 1.0993\n",
      "Epoch [84/100], Loss: 1.0986\n",
      "Epoch [85/100], Loss: 1.0995\n",
      "Epoch [86/100], Loss: 1.0987\n",
      "Epoch [87/100], Loss: 1.0986\n",
      "Epoch [88/100], Loss: 1.0982\n",
      "Epoch [89/100], Loss: 1.0970\n",
      "Epoch [90/100], Loss: 1.0984\n",
      "Epoch [91/100], Loss: 1.0988\n",
      "Epoch [92/100], Loss: 1.0981\n",
      "Epoch [93/100], Loss: 1.0966\n",
      "Epoch [94/100], Loss: 1.0979\n",
      "Epoch [95/100], Loss: 1.0950\n",
      "Epoch [96/100], Loss: 1.0960\n",
      "Epoch [97/100], Loss: 1.0952\n",
      "Epoch [98/100], Loss: 1.0921\n",
      "Epoch [99/100], Loss: 1.0915\n",
      "Epoch [100/100], Loss: 1.0926\n",
      "Subset 75000, Epoch [100/100], Loss: 1.0926\n",
      "Test Accuracy Logit Lipschitz: 55.18%\n",
      "Epoch [1/100], Loss: 1.6224\n",
      "Epoch [2/100], Loss: 1.3428\n",
      "Epoch [3/100], Loss: 1.2942\n",
      "Epoch [4/100], Loss: 1.2703\n",
      "Epoch [5/100], Loss: 1.2517\n",
      "Epoch [6/100], Loss: 1.2420\n",
      "Epoch [7/100], Loss: 1.2294\n",
      "Epoch [8/100], Loss: 1.2214\n",
      "Epoch [9/100], Loss: 1.2122\n",
      "Epoch [10/100], Loss: 1.2073\n",
      "Epoch [11/100], Loss: 1.2030\n",
      "Epoch [12/100], Loss: 1.1969\n",
      "Epoch [13/100], Loss: 1.1913\n",
      "Epoch [14/100], Loss: 1.1902\n",
      "Epoch [15/100], Loss: 1.1855\n",
      "Epoch [16/100], Loss: 1.1809\n",
      "Epoch [17/100], Loss: 1.1763\n",
      "Epoch [18/100], Loss: 1.1743\n",
      "Epoch [19/100], Loss: 1.1707\n",
      "Epoch [20/100], Loss: 1.1676\n",
      "Epoch [21/100], Loss: 1.1668\n",
      "Epoch [22/100], Loss: 1.1655\n",
      "Epoch [23/100], Loss: 1.1638\n",
      "Epoch [24/100], Loss: 1.1611\n",
      "Epoch [25/100], Loss: 1.1586\n",
      "Epoch [26/100], Loss: 1.1567\n",
      "Epoch [27/100], Loss: 1.1544\n",
      "Epoch [28/100], Loss: 1.1515\n",
      "Epoch [29/100], Loss: 1.1498\n",
      "Epoch [30/100], Loss: 1.1495\n",
      "Epoch [31/100], Loss: 1.1463\n",
      "Epoch [32/100], Loss: 1.1475\n",
      "Epoch [33/100], Loss: 1.1456\n",
      "Epoch [34/100], Loss: 1.1434\n",
      "Epoch [35/100], Loss: 1.1429\n",
      "Epoch [36/100], Loss: 1.1397\n",
      "Epoch [37/100], Loss: 1.1399\n",
      "Epoch [38/100], Loss: 1.1388\n",
      "Epoch [39/100], Loss: 1.1362\n",
      "Epoch [40/100], Loss: 1.1360\n",
      "Epoch [41/100], Loss: 1.1349\n",
      "Epoch [42/100], Loss: 1.1365\n",
      "Epoch [43/100], Loss: 1.1347\n",
      "Epoch [44/100], Loss: 1.1338\n",
      "Epoch [45/100], Loss: 1.1316\n",
      "Epoch [46/100], Loss: 1.1292\n",
      "Epoch [47/100], Loss: 1.1312\n",
      "Epoch [48/100], Loss: 1.1279\n",
      "Epoch [49/100], Loss: 1.1261\n",
      "Epoch [50/100], Loss: 1.1285\n",
      "Epoch [51/100], Loss: 1.1255\n",
      "Epoch [52/100], Loss: 1.1270\n",
      "Epoch [53/100], Loss: 1.1250\n",
      "Epoch [54/100], Loss: 1.1222\n",
      "Epoch [55/100], Loss: 1.1229\n",
      "Epoch [56/100], Loss: 1.1239\n",
      "Epoch [57/100], Loss: 1.1204\n",
      "Epoch [58/100], Loss: 1.1201\n",
      "Epoch [59/100], Loss: 1.1208\n",
      "Epoch [60/100], Loss: 1.1196\n",
      "Epoch [61/100], Loss: 1.1190\n",
      "Epoch [62/100], Loss: 1.1182\n",
      "Epoch [63/100], Loss: 1.1190\n",
      "Epoch [64/100], Loss: 1.1168\n",
      "Epoch [65/100], Loss: 1.1157\n",
      "Epoch [66/100], Loss: 1.1155\n",
      "Epoch [67/100], Loss: 1.1136\n",
      "Epoch [68/100], Loss: 1.1151\n",
      "Epoch [69/100], Loss: 1.1135\n",
      "Epoch [70/100], Loss: 1.1145\n",
      "Epoch [71/100], Loss: 1.1140\n",
      "Epoch [72/100], Loss: 1.1122\n",
      "Epoch [73/100], Loss: 1.1105\n",
      "Epoch [74/100], Loss: 1.1104\n",
      "Epoch [75/100], Loss: 1.1125\n",
      "Epoch [76/100], Loss: 1.1114\n",
      "Epoch [77/100], Loss: 1.1117\n",
      "Epoch [78/100], Loss: 1.1117\n",
      "Epoch [79/100], Loss: 1.1087\n",
      "Epoch [80/100], Loss: 1.1067\n",
      "Epoch [81/100], Loss: 1.1077\n",
      "Epoch [82/100], Loss: 1.1097\n",
      "Epoch [83/100], Loss: 1.1070\n",
      "Epoch [84/100], Loss: 1.1077\n",
      "Epoch [85/100], Loss: 1.1078\n",
      "Epoch [86/100], Loss: 1.1068\n",
      "Epoch [87/100], Loss: 1.1062\n",
      "Epoch [88/100], Loss: 1.1041\n",
      "Epoch [89/100], Loss: 1.1057\n",
      "Epoch [90/100], Loss: 1.1051\n",
      "Epoch [91/100], Loss: 1.1036\n",
      "Epoch [92/100], Loss: 1.1051\n",
      "Epoch [93/100], Loss: 1.1052\n",
      "Epoch [94/100], Loss: 1.1046\n",
      "Epoch [95/100], Loss: 1.1047\n",
      "Epoch [96/100], Loss: 1.1028\n",
      "Epoch [97/100], Loss: 1.1030\n",
      "Epoch [98/100], Loss: 1.1040\n",
      "Epoch [99/100], Loss: 1.1030\n",
      "Epoch [100/100], Loss: 1.1018\n",
      "Subset 75000, Epoch [100/100], Loss: 1.1018\n",
      "Test Accuracy Logit Lipschitz: 54.81%\n",
      "Epoch [1/100], Loss: 1.6228\n",
      "Epoch [2/100], Loss: 1.3416\n",
      "Epoch [3/100], Loss: 1.2956\n",
      "Epoch [4/100], Loss: 1.2664\n",
      "Epoch [5/100], Loss: 1.2496\n",
      "Epoch [6/100], Loss: 1.2366\n",
      "Epoch [7/100], Loss: 1.2278\n",
      "Epoch [8/100], Loss: 1.2190\n",
      "Epoch [9/100], Loss: 1.2118\n",
      "Epoch [10/100], Loss: 1.2060\n",
      "Epoch [11/100], Loss: 1.1991\n",
      "Epoch [12/100], Loss: 1.1945\n",
      "Epoch [13/100], Loss: 1.1906\n",
      "Epoch [14/100], Loss: 1.1873\n",
      "Epoch [15/100], Loss: 1.1822\n",
      "Epoch [16/100], Loss: 1.1787\n",
      "Epoch [17/100], Loss: 1.1756\n",
      "Epoch [18/100], Loss: 1.1747\n",
      "Epoch [19/100], Loss: 1.1712\n",
      "Epoch [20/100], Loss: 1.1689\n",
      "Epoch [21/100], Loss: 1.1669\n",
      "Epoch [22/100], Loss: 1.1644\n",
      "Epoch [23/100], Loss: 1.1611\n",
      "Epoch [24/100], Loss: 1.1592\n",
      "Epoch [25/100], Loss: 1.1564\n",
      "Epoch [26/100], Loss: 1.1551\n",
      "Epoch [27/100], Loss: 1.1548\n",
      "Epoch [28/100], Loss: 1.1510\n",
      "Epoch [29/100], Loss: 1.1510\n",
      "Epoch [30/100], Loss: 1.1476\n",
      "Epoch [31/100], Loss: 1.1457\n",
      "Epoch [32/100], Loss: 1.1460\n",
      "Epoch [33/100], Loss: 1.1441\n",
      "Epoch [34/100], Loss: 1.1424\n",
      "Epoch [35/100], Loss: 1.1408\n",
      "Epoch [36/100], Loss: 1.1400\n",
      "Epoch [37/100], Loss: 1.1397\n",
      "Epoch [38/100], Loss: 1.1388\n",
      "Epoch [39/100], Loss: 1.1366\n",
      "Epoch [40/100], Loss: 1.1333\n",
      "Epoch [41/100], Loss: 1.1352\n",
      "Epoch [42/100], Loss: 1.1330\n",
      "Epoch [43/100], Loss: 1.1318\n",
      "Epoch [44/100], Loss: 1.1326\n",
      "Epoch [45/100], Loss: 1.1319\n",
      "Epoch [46/100], Loss: 1.1318\n",
      "Epoch [47/100], Loss: 1.1290\n",
      "Epoch [48/100], Loss: 1.1294\n",
      "Epoch [49/100], Loss: 1.1264\n",
      "Epoch [50/100], Loss: 1.1261\n",
      "Epoch [51/100], Loss: 1.1261\n",
      "Epoch [52/100], Loss: 1.1249\n",
      "Epoch [53/100], Loss: 1.1252\n",
      "Epoch [54/100], Loss: 1.1247\n",
      "Epoch [55/100], Loss: 1.1241\n",
      "Epoch [56/100], Loss: 1.1207\n",
      "Epoch [57/100], Loss: 1.1211\n",
      "Epoch [58/100], Loss: 1.1205\n",
      "Epoch [59/100], Loss: 1.1211\n",
      "Epoch [60/100], Loss: 1.1217\n",
      "Epoch [61/100], Loss: 1.1200\n",
      "Epoch [62/100], Loss: 1.1188\n",
      "Epoch [63/100], Loss: 1.1174\n",
      "Epoch [64/100], Loss: 1.1158\n",
      "Epoch [65/100], Loss: 1.1182\n",
      "Epoch [66/100], Loss: 1.1168\n",
      "Epoch [67/100], Loss: 1.1143\n",
      "Epoch [68/100], Loss: 1.1139\n",
      "Epoch [69/100], Loss: 1.1123\n",
      "Epoch [70/100], Loss: 1.1142\n",
      "Epoch [71/100], Loss: 1.1146\n",
      "Epoch [72/100], Loss: 1.1134\n",
      "Epoch [73/100], Loss: 1.1115\n",
      "Epoch [74/100], Loss: 1.1126\n",
      "Epoch [75/100], Loss: 1.1110\n",
      "Epoch [76/100], Loss: 1.1111\n",
      "Epoch [77/100], Loss: 1.1100\n",
      "Epoch [78/100], Loss: 1.1129\n",
      "Epoch [79/100], Loss: 1.1090\n",
      "Epoch [80/100], Loss: 1.1115\n",
      "Epoch [81/100], Loss: 1.1100\n",
      "Epoch [82/100], Loss: 1.1080\n",
      "Epoch [83/100], Loss: 1.1080\n",
      "Epoch [84/100], Loss: 1.1057\n",
      "Epoch [85/100], Loss: 1.1067\n",
      "Epoch [86/100], Loss: 1.1062\n",
      "Epoch [87/100], Loss: 1.1069\n",
      "Epoch [88/100], Loss: 1.1084\n",
      "Epoch [89/100], Loss: 1.1047\n",
      "Epoch [90/100], Loss: 1.1080\n",
      "Epoch [91/100], Loss: 1.1051\n",
      "Epoch [92/100], Loss: 1.1072\n",
      "Epoch [93/100], Loss: 1.1052\n",
      "Epoch [94/100], Loss: 1.1028\n",
      "Epoch [95/100], Loss: 1.1043\n",
      "Epoch [96/100], Loss: 1.1058\n",
      "Epoch [97/100], Loss: 1.1039\n",
      "Epoch [98/100], Loss: 1.1041\n",
      "Epoch [99/100], Loss: 1.1019\n",
      "Epoch [100/100], Loss: 1.1033\n",
      "Subset 75000, Epoch [100/100], Loss: 1.1033\n",
      "Test Accuracy Logit Lipschitz: 54.80%\n",
      "Epoch [1/100], Loss: 1.6090\n",
      "Epoch [2/100], Loss: 1.3313\n",
      "Epoch [3/100], Loss: 1.2864\n",
      "Epoch [4/100], Loss: 1.2599\n",
      "Epoch [5/100], Loss: 1.2442\n",
      "Epoch [6/100], Loss: 1.2300\n",
      "Epoch [7/100], Loss: 1.2189\n",
      "Epoch [8/100], Loss: 1.2101\n",
      "Epoch [9/100], Loss: 1.2052\n",
      "Epoch [10/100], Loss: 1.1970\n",
      "Epoch [11/100], Loss: 1.1935\n",
      "Epoch [12/100], Loss: 1.1868\n",
      "Epoch [13/100], Loss: 1.1845\n",
      "Epoch [14/100], Loss: 1.1809\n",
      "Epoch [15/100], Loss: 1.1752\n",
      "Epoch [16/100], Loss: 1.1727\n",
      "Epoch [17/100], Loss: 1.1710\n",
      "Epoch [18/100], Loss: 1.1660\n",
      "Epoch [19/100], Loss: 1.1640\n",
      "Epoch [20/100], Loss: 1.1616\n",
      "Epoch [21/100], Loss: 1.1575\n",
      "Epoch [22/100], Loss: 1.1575\n",
      "Epoch [23/100], Loss: 1.1536\n",
      "Epoch [24/100], Loss: 1.1526\n",
      "Epoch [25/100], Loss: 1.1524\n",
      "Epoch [26/100], Loss: 1.1471\n",
      "Epoch [27/100], Loss: 1.1482\n",
      "Epoch [28/100], Loss: 1.1464\n",
      "Epoch [29/100], Loss: 1.1414\n",
      "Epoch [30/100], Loss: 1.1409\n",
      "Epoch [31/100], Loss: 1.1405\n",
      "Epoch [32/100], Loss: 1.1392\n",
      "Epoch [33/100], Loss: 1.1373\n",
      "Epoch [34/100], Loss: 1.1357\n",
      "Epoch [35/100], Loss: 1.1339\n",
      "Epoch [36/100], Loss: 1.1338\n",
      "Epoch [37/100], Loss: 1.1343\n",
      "Epoch [38/100], Loss: 1.1306\n",
      "Epoch [39/100], Loss: 1.1316\n",
      "Epoch [40/100], Loss: 1.1273\n",
      "Epoch [41/100], Loss: 1.1272\n",
      "Epoch [42/100], Loss: 1.1257\n",
      "Epoch [43/100], Loss: 1.1241\n",
      "Epoch [44/100], Loss: 1.1234\n",
      "Epoch [45/100], Loss: 1.1220\n",
      "Epoch [46/100], Loss: 1.1218\n",
      "Epoch [47/100], Loss: 1.1207\n",
      "Epoch [48/100], Loss: 1.1209\n",
      "Epoch [49/100], Loss: 1.1195\n",
      "Epoch [50/100], Loss: 1.1218\n",
      "Epoch [51/100], Loss: 1.1190\n",
      "Epoch [52/100], Loss: 1.1189\n",
      "Epoch [53/100], Loss: 1.1182\n",
      "Epoch [54/100], Loss: 1.1170\n",
      "Epoch [55/100], Loss: 1.1161\n",
      "Epoch [56/100], Loss: 1.1153\n",
      "Epoch [57/100], Loss: 1.1159\n",
      "Epoch [58/100], Loss: 1.1154\n",
      "Epoch [59/100], Loss: 1.1143\n",
      "Epoch [60/100], Loss: 1.1136\n",
      "Epoch [61/100], Loss: 1.1103\n",
      "Epoch [62/100], Loss: 1.1092\n",
      "Epoch [63/100], Loss: 1.1105\n",
      "Epoch [64/100], Loss: 1.1116\n",
      "Epoch [65/100], Loss: 1.1089\n",
      "Epoch [66/100], Loss: 1.1094\n",
      "Epoch [67/100], Loss: 1.1086\n",
      "Epoch [68/100], Loss: 1.1074\n",
      "Epoch [69/100], Loss: 1.1092\n",
      "Epoch [70/100], Loss: 1.1062\n",
      "Epoch [71/100], Loss: 1.1071\n",
      "Epoch [72/100], Loss: 1.1037\n",
      "Epoch [73/100], Loss: 1.1079\n",
      "Epoch [74/100], Loss: 1.1057\n",
      "Epoch [75/100], Loss: 1.1041\n",
      "Epoch [76/100], Loss: 1.1042\n",
      "Epoch [77/100], Loss: 1.1060\n",
      "Epoch [78/100], Loss: 1.1028\n",
      "Epoch [79/100], Loss: 1.1026\n",
      "Epoch [80/100], Loss: 1.1018\n",
      "Epoch [81/100], Loss: 1.1032\n",
      "Epoch [82/100], Loss: 1.1013\n",
      "Epoch [83/100], Loss: 1.1030\n",
      "Epoch [84/100], Loss: 1.0996\n",
      "Epoch [85/100], Loss: 1.0983\n",
      "Epoch [86/100], Loss: 1.1010\n",
      "Epoch [87/100], Loss: 1.0980\n",
      "Epoch [88/100], Loss: 1.1005\n",
      "Epoch [89/100], Loss: 1.0996\n",
      "Epoch [90/100], Loss: 1.1022\n",
      "Epoch [91/100], Loss: 1.0986\n",
      "Epoch [92/100], Loss: 1.0977\n",
      "Epoch [93/100], Loss: 1.0968\n",
      "Epoch [94/100], Loss: 1.0982\n",
      "Epoch [95/100], Loss: 1.0966\n",
      "Epoch [96/100], Loss: 1.0970\n",
      "Epoch [97/100], Loss: 1.0965\n",
      "Epoch [98/100], Loss: 1.0957\n",
      "Epoch [99/100], Loss: 1.0936\n",
      "Epoch [100/100], Loss: 1.0967\n",
      "Subset 75000, Epoch [100/100], Loss: 1.0967\n",
      "Test Accuracy Logit Lipschitz: 55.22%\n",
      "Epoch [1/100], Loss: 1.6232\n",
      "Epoch [2/100], Loss: 1.3398\n",
      "Epoch [3/100], Loss: 1.2932\n",
      "Epoch [4/100], Loss: 1.2665\n",
      "Epoch [5/100], Loss: 1.2491\n",
      "Epoch [6/100], Loss: 1.2355\n",
      "Epoch [7/100], Loss: 1.2274\n",
      "Epoch [8/100], Loss: 1.2190\n",
      "Epoch [9/100], Loss: 1.2099\n",
      "Epoch [10/100], Loss: 1.2014\n",
      "Epoch [11/100], Loss: 1.2001\n",
      "Epoch [12/100], Loss: 1.1944\n",
      "Epoch [13/100], Loss: 1.1898\n",
      "Epoch [14/100], Loss: 1.1863\n",
      "Epoch [15/100], Loss: 1.1812\n",
      "Epoch [16/100], Loss: 1.1756\n",
      "Epoch [17/100], Loss: 1.1734\n",
      "Epoch [18/100], Loss: 1.1704\n",
      "Epoch [19/100], Loss: 1.1695\n",
      "Epoch [20/100], Loss: 1.1668\n",
      "Epoch [21/100], Loss: 1.1645\n",
      "Epoch [22/100], Loss: 1.1606\n",
      "Epoch [23/100], Loss: 1.1572\n",
      "Epoch [24/100], Loss: 1.1563\n",
      "Epoch [25/100], Loss: 1.1556\n",
      "Epoch [26/100], Loss: 1.1519\n",
      "Epoch [27/100], Loss: 1.1519\n",
      "Epoch [28/100], Loss: 1.1482\n",
      "Epoch [29/100], Loss: 1.1468\n",
      "Epoch [30/100], Loss: 1.1432\n",
      "Epoch [31/100], Loss: 1.1435\n",
      "Epoch [32/100], Loss: 1.1408\n",
      "Epoch [33/100], Loss: 1.1426\n",
      "Epoch [34/100], Loss: 1.1398\n",
      "Epoch [35/100], Loss: 1.1382\n",
      "Epoch [36/100], Loss: 1.1369\n",
      "Epoch [37/100], Loss: 1.1345\n",
      "Epoch [38/100], Loss: 1.1342\n",
      "Epoch [39/100], Loss: 1.1321\n",
      "Epoch [40/100], Loss: 1.1323\n",
      "Epoch [41/100], Loss: 1.1310\n",
      "Epoch [42/100], Loss: 1.1303\n",
      "Epoch [43/100], Loss: 1.1269\n",
      "Epoch [44/100], Loss: 1.1280\n",
      "Epoch [45/100], Loss: 1.1265\n",
      "Epoch [46/100], Loss: 1.1263\n",
      "Epoch [47/100], Loss: 1.1260\n",
      "Epoch [48/100], Loss: 1.1224\n",
      "Epoch [49/100], Loss: 1.1226\n",
      "Epoch [50/100], Loss: 1.1229\n",
      "Epoch [51/100], Loss: 1.1219\n",
      "Epoch [52/100], Loss: 1.1200\n",
      "Epoch [53/100], Loss: 1.1208\n",
      "Epoch [54/100], Loss: 1.1206\n",
      "Epoch [55/100], Loss: 1.1193\n",
      "Epoch [56/100], Loss: 1.1168\n",
      "Epoch [57/100], Loss: 1.1169\n",
      "Epoch [58/100], Loss: 1.1185\n",
      "Epoch [59/100], Loss: 1.1172\n",
      "Epoch [60/100], Loss: 1.1164\n",
      "Epoch [61/100], Loss: 1.1146\n",
      "Epoch [62/100], Loss: 1.1158\n",
      "Epoch [63/100], Loss: 1.1124\n",
      "Epoch [64/100], Loss: 1.1121\n",
      "Epoch [65/100], Loss: 1.1096\n",
      "Epoch [66/100], Loss: 1.1114\n",
      "Epoch [67/100], Loss: 1.1124\n",
      "Epoch [68/100], Loss: 1.1077\n",
      "Epoch [69/100], Loss: 1.1109\n",
      "Epoch [70/100], Loss: 1.1096\n",
      "Epoch [71/100], Loss: 1.1095\n",
      "Epoch [72/100], Loss: 1.1077\n",
      "Epoch [73/100], Loss: 1.1082\n",
      "Epoch [74/100], Loss: 1.1077\n",
      "Epoch [75/100], Loss: 1.1071\n",
      "Epoch [76/100], Loss: 1.1051\n",
      "Epoch [77/100], Loss: 1.1090\n",
      "Epoch [78/100], Loss: 1.1043\n",
      "Epoch [79/100], Loss: 1.1055\n",
      "Epoch [80/100], Loss: 1.1034\n",
      "Epoch [81/100], Loss: 1.1050\n",
      "Epoch [82/100], Loss: 1.1036\n",
      "Epoch [83/100], Loss: 1.1035\n",
      "Epoch [84/100], Loss: 1.1048\n",
      "Epoch [85/100], Loss: 1.1032\n",
      "Epoch [86/100], Loss: 1.1028\n",
      "Epoch [87/100], Loss: 1.1025\n",
      "Epoch [88/100], Loss: 1.1012\n",
      "Epoch [89/100], Loss: 1.1011\n",
      "Epoch [90/100], Loss: 1.1022\n",
      "Epoch [91/100], Loss: 1.0991\n",
      "Epoch [92/100], Loss: 1.0995\n",
      "Epoch [93/100], Loss: 1.0991\n",
      "Epoch [94/100], Loss: 1.0991\n",
      "Epoch [95/100], Loss: 1.1005\n",
      "Epoch [96/100], Loss: 1.1012\n",
      "Stopping early at epoch 96 (Loss improvement < 0.0001 for 3 epochs)\n",
      "Subset 75000, Epoch [96/100], Loss: 1.1012\n",
      "Test Accuracy Logit Lipschitz: 55.13%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.7171\n",
      "Epoch [2/100], Loss: 1.3622\n",
      "Epoch [3/100], Loss: 1.3017\n",
      "Epoch [4/100], Loss: 1.2672\n",
      "Epoch [5/100], Loss: 1.2492\n",
      "Epoch [6/100], Loss: 1.2301\n",
      "Epoch [7/100], Loss: 1.2180\n",
      "Epoch [8/100], Loss: 1.2071\n",
      "Epoch [9/100], Loss: 1.1985\n",
      "Epoch [10/100], Loss: 1.1890\n",
      "Epoch [11/100], Loss: 1.1820\n",
      "Epoch [12/100], Loss: 1.1745\n",
      "Epoch [13/100], Loss: 1.1697\n",
      "Epoch [14/100], Loss: 1.1615\n",
      "Epoch [15/100], Loss: 1.1567\n",
      "Epoch [16/100], Loss: 1.1526\n",
      "Epoch [17/100], Loss: 1.1485\n",
      "Epoch [18/100], Loss: 1.1438\n",
      "Epoch [19/100], Loss: 1.1396\n",
      "Epoch [20/100], Loss: 1.1376\n",
      "Epoch [21/100], Loss: 1.1326\n",
      "Epoch [22/100], Loss: 1.1275\n",
      "Epoch [23/100], Loss: 1.1258\n",
      "Epoch [24/100], Loss: 1.1239\n",
      "Epoch [25/100], Loss: 1.1210\n",
      "Epoch [26/100], Loss: 1.1174\n",
      "Epoch [27/100], Loss: 1.1177\n",
      "Epoch [28/100], Loss: 1.1122\n",
      "Epoch [29/100], Loss: 1.1107\n",
      "Epoch [30/100], Loss: 1.1084\n",
      "Epoch [31/100], Loss: 1.1070\n",
      "Epoch [32/100], Loss: 1.1055\n",
      "Epoch [33/100], Loss: 1.0996\n",
      "Epoch [34/100], Loss: 1.1004\n",
      "Epoch [35/100], Loss: 1.0979\n",
      "Epoch [36/100], Loss: 1.0983\n",
      "Epoch [37/100], Loss: 1.0934\n",
      "Epoch [38/100], Loss: 1.0888\n",
      "Epoch [39/100], Loss: 1.0908\n",
      "Epoch [40/100], Loss: 1.0856\n",
      "Epoch [41/100], Loss: 1.0890\n",
      "Epoch [42/100], Loss: 1.0863\n",
      "Epoch [43/100], Loss: 1.0841\n",
      "Epoch [44/100], Loss: 1.0795\n",
      "Epoch [45/100], Loss: 1.0819\n",
      "Epoch [46/100], Loss: 1.0775\n",
      "Epoch [47/100], Loss: 1.0770\n",
      "Epoch [48/100], Loss: 1.0744\n",
      "Epoch [49/100], Loss: 1.0766\n",
      "Epoch [50/100], Loss: 1.0738\n",
      "Epoch [51/100], Loss: 1.0735\n",
      "Epoch [52/100], Loss: 1.0714\n",
      "Epoch [53/100], Loss: 1.0704\n",
      "Epoch [54/100], Loss: 1.0692\n",
      "Epoch [55/100], Loss: 1.0679\n",
      "Epoch [56/100], Loss: 1.0658\n",
      "Epoch [57/100], Loss: 1.0646\n",
      "Epoch [58/100], Loss: 1.0661\n",
      "Epoch [59/100], Loss: 1.0618\n",
      "Epoch [60/100], Loss: 1.0641\n",
      "Epoch [61/100], Loss: 1.0630\n",
      "Epoch [62/100], Loss: 1.0603\n",
      "Epoch [63/100], Loss: 1.0599\n",
      "Epoch [64/100], Loss: 1.0593\n",
      "Epoch [65/100], Loss: 1.0583\n",
      "Epoch [66/100], Loss: 1.0552\n",
      "Epoch [67/100], Loss: 1.0545\n",
      "Epoch [68/100], Loss: 1.0547\n",
      "Epoch [69/100], Loss: 1.0542\n",
      "Epoch [70/100], Loss: 1.0525\n",
      "Epoch [71/100], Loss: 1.0510\n",
      "Epoch [72/100], Loss: 1.0513\n",
      "Epoch [73/100], Loss: 1.0500\n",
      "Epoch [74/100], Loss: 1.0500\n",
      "Epoch [75/100], Loss: 1.0481\n",
      "Epoch [76/100], Loss: 1.0501\n",
      "Epoch [77/100], Loss: 1.0478\n",
      "Epoch [78/100], Loss: 1.0479\n",
      "Epoch [79/100], Loss: 1.0465\n",
      "Epoch [80/100], Loss: 1.0441\n",
      "Epoch [81/100], Loss: 1.0441\n",
      "Epoch [82/100], Loss: 1.0419\n",
      "Epoch [83/100], Loss: 1.0428\n",
      "Epoch [84/100], Loss: 1.0418\n",
      "Epoch [85/100], Loss: 1.0415\n",
      "Epoch [86/100], Loss: 1.0421\n",
      "Epoch [87/100], Loss: 1.0410\n",
      "Epoch [88/100], Loss: 1.0397\n",
      "Epoch [89/100], Loss: 1.0375\n",
      "Epoch [90/100], Loss: 1.0399\n",
      "Epoch [91/100], Loss: 1.0381\n",
      "Epoch [92/100], Loss: 1.0375\n",
      "Epoch [93/100], Loss: 1.0358\n",
      "Epoch [94/100], Loss: 1.0363\n",
      "Epoch [95/100], Loss: 1.0355\n",
      "Epoch [96/100], Loss: 1.0346\n",
      "Epoch [97/100], Loss: 1.0339\n",
      "Epoch [98/100], Loss: 1.0352\n",
      "Epoch [99/100], Loss: 1.0362\n",
      "Epoch [100/100], Loss: 1.0329\n",
      "Subset 50000, Epoch [100/100], Loss: 1.0329\n",
      "Test Accuracy Logit Lipschitz: 54.34%\n",
      "Epoch [1/100], Loss: 1.7280\n",
      "Epoch [2/100], Loss: 1.3731\n",
      "Epoch [3/100], Loss: 1.3111\n",
      "Epoch [4/100], Loss: 1.2802\n",
      "Epoch [5/100], Loss: 1.2565\n",
      "Epoch [6/100], Loss: 1.2427\n",
      "Epoch [7/100], Loss: 1.2279\n",
      "Epoch [8/100], Loss: 1.2181\n",
      "Epoch [9/100], Loss: 1.2074\n",
      "Epoch [10/100], Loss: 1.1992\n",
      "Epoch [11/100], Loss: 1.1921\n",
      "Epoch [12/100], Loss: 1.1864\n",
      "Epoch [13/100], Loss: 1.1787\n",
      "Epoch [14/100], Loss: 1.1710\n",
      "Epoch [15/100], Loss: 1.1654\n",
      "Epoch [16/100], Loss: 1.1646\n",
      "Epoch [17/100], Loss: 1.1593\n",
      "Epoch [18/100], Loss: 1.1539\n",
      "Epoch [19/100], Loss: 1.1510\n",
      "Epoch [20/100], Loss: 1.1477\n",
      "Epoch [21/100], Loss: 1.1386\n",
      "Epoch [22/100], Loss: 1.1407\n",
      "Epoch [23/100], Loss: 1.1340\n",
      "Epoch [24/100], Loss: 1.1350\n",
      "Epoch [25/100], Loss: 1.1295\n",
      "Epoch [26/100], Loss: 1.1308\n",
      "Epoch [27/100], Loss: 1.1265\n",
      "Epoch [28/100], Loss: 1.1212\n",
      "Epoch [29/100], Loss: 1.1198\n",
      "Epoch [30/100], Loss: 1.1164\n",
      "Epoch [31/100], Loss: 1.1151\n",
      "Epoch [32/100], Loss: 1.1137\n",
      "Epoch [33/100], Loss: 1.1120\n",
      "Epoch [34/100], Loss: 1.1100\n",
      "Epoch [35/100], Loss: 1.1050\n",
      "Epoch [36/100], Loss: 1.1029\n",
      "Epoch [37/100], Loss: 1.1052\n",
      "Epoch [38/100], Loss: 1.1028\n",
      "Epoch [39/100], Loss: 1.0971\n",
      "Epoch [40/100], Loss: 1.0983\n",
      "Epoch [41/100], Loss: 1.0958\n",
      "Epoch [42/100], Loss: 1.0947\n",
      "Epoch [43/100], Loss: 1.0926\n",
      "Epoch [44/100], Loss: 1.0907\n",
      "Epoch [45/100], Loss: 1.0904\n",
      "Epoch [46/100], Loss: 1.0870\n",
      "Epoch [47/100], Loss: 1.0855\n",
      "Epoch [48/100], Loss: 1.0865\n",
      "Epoch [49/100], Loss: 1.0850\n",
      "Epoch [50/100], Loss: 1.0835\n",
      "Epoch [51/100], Loss: 1.0802\n",
      "Epoch [52/100], Loss: 1.0772\n",
      "Epoch [53/100], Loss: 1.0787\n",
      "Epoch [54/100], Loss: 1.0802\n",
      "Epoch [55/100], Loss: 1.0783\n",
      "Epoch [56/100], Loss: 1.0751\n",
      "Epoch [57/100], Loss: 1.0738\n",
      "Epoch [58/100], Loss: 1.0742\n",
      "Epoch [59/100], Loss: 1.0697\n",
      "Epoch [60/100], Loss: 1.0731\n",
      "Epoch [61/100], Loss: 1.0696\n",
      "Epoch [62/100], Loss: 1.0715\n",
      "Epoch [63/100], Loss: 1.0686\n",
      "Epoch [64/100], Loss: 1.0678\n",
      "Epoch [65/100], Loss: 1.0665\n",
      "Epoch [66/100], Loss: 1.0654\n",
      "Epoch [67/100], Loss: 1.0641\n",
      "Epoch [68/100], Loss: 1.0621\n",
      "Epoch [69/100], Loss: 1.0641\n",
      "Epoch [70/100], Loss: 1.0618\n",
      "Epoch [71/100], Loss: 1.0602\n",
      "Epoch [72/100], Loss: 1.0622\n",
      "Epoch [73/100], Loss: 1.0600\n",
      "Epoch [74/100], Loss: 1.0609\n",
      "Epoch [75/100], Loss: 1.0568\n",
      "Epoch [76/100], Loss: 1.0554\n",
      "Epoch [77/100], Loss: 1.0547\n",
      "Epoch [78/100], Loss: 1.0545\n",
      "Epoch [79/100], Loss: 1.0550\n",
      "Epoch [80/100], Loss: 1.0547\n",
      "Epoch [81/100], Loss: 1.0537\n",
      "Epoch [82/100], Loss: 1.0546\n",
      "Epoch [83/100], Loss: 1.0515\n",
      "Epoch [84/100], Loss: 1.0526\n",
      "Epoch [85/100], Loss: 1.0495\n",
      "Epoch [86/100], Loss: 1.0524\n",
      "Epoch [87/100], Loss: 1.0478\n",
      "Epoch [88/100], Loss: 1.0487\n",
      "Epoch [89/100], Loss: 1.0487\n",
      "Epoch [90/100], Loss: 1.0472\n",
      "Epoch [91/100], Loss: 1.0479\n",
      "Epoch [92/100], Loss: 1.0458\n",
      "Epoch [93/100], Loss: 1.0457\n",
      "Epoch [94/100], Loss: 1.0453\n",
      "Epoch [95/100], Loss: 1.0462\n",
      "Epoch [96/100], Loss: 1.0438\n",
      "Epoch [97/100], Loss: 1.0433\n",
      "Epoch [98/100], Loss: 1.0450\n",
      "Epoch [99/100], Loss: 1.0422\n",
      "Epoch [100/100], Loss: 1.0428\n",
      "Subset 50000, Epoch [100/100], Loss: 1.0428\n",
      "Test Accuracy Logit Lipschitz: 54.14%\n",
      "Epoch [1/100], Loss: 1.7329\n",
      "Epoch [2/100], Loss: 1.3753\n",
      "Epoch [3/100], Loss: 1.3175\n",
      "Epoch [4/100], Loss: 1.2824\n",
      "Epoch [5/100], Loss: 1.2625\n",
      "Epoch [6/100], Loss: 1.2446\n",
      "Epoch [7/100], Loss: 1.2309\n",
      "Epoch [8/100], Loss: 1.2223\n",
      "Epoch [9/100], Loss: 1.2069\n",
      "Epoch [10/100], Loss: 1.2019\n",
      "Epoch [11/100], Loss: 1.1953\n",
      "Epoch [12/100], Loss: 1.1897\n",
      "Epoch [13/100], Loss: 1.1844\n",
      "Epoch [14/100], Loss: 1.1785\n",
      "Epoch [15/100], Loss: 1.1713\n",
      "Epoch [16/100], Loss: 1.1663\n",
      "Epoch [17/100], Loss: 1.1615\n",
      "Epoch [18/100], Loss: 1.1575\n",
      "Epoch [19/100], Loss: 1.1553\n",
      "Epoch [20/100], Loss: 1.1516\n",
      "Epoch [21/100], Loss: 1.1510\n",
      "Epoch [22/100], Loss: 1.1447\n",
      "Epoch [23/100], Loss: 1.1400\n",
      "Epoch [24/100], Loss: 1.1398\n",
      "Epoch [25/100], Loss: 1.1365\n",
      "Epoch [26/100], Loss: 1.1311\n",
      "Epoch [27/100], Loss: 1.1288\n",
      "Epoch [28/100], Loss: 1.1249\n",
      "Epoch [29/100], Loss: 1.1221\n",
      "Epoch [30/100], Loss: 1.1236\n",
      "Epoch [31/100], Loss: 1.1217\n",
      "Epoch [32/100], Loss: 1.1183\n",
      "Epoch [33/100], Loss: 1.1173\n",
      "Epoch [34/100], Loss: 1.1125\n",
      "Epoch [35/100], Loss: 1.1109\n",
      "Epoch [36/100], Loss: 1.1077\n",
      "Epoch [37/100], Loss: 1.1067\n",
      "Epoch [38/100], Loss: 1.1086\n",
      "Epoch [39/100], Loss: 1.1041\n",
      "Epoch [40/100], Loss: 1.1014\n",
      "Epoch [41/100], Loss: 1.0994\n",
      "Epoch [42/100], Loss: 1.0980\n",
      "Epoch [43/100], Loss: 1.0985\n",
      "Epoch [44/100], Loss: 1.0947\n",
      "Epoch [45/100], Loss: 1.0929\n",
      "Epoch [46/100], Loss: 1.0926\n",
      "Epoch [47/100], Loss: 1.0903\n",
      "Epoch [48/100], Loss: 1.0904\n",
      "Epoch [49/100], Loss: 1.0907\n",
      "Epoch [50/100], Loss: 1.0873\n",
      "Epoch [51/100], Loss: 1.0873\n",
      "Epoch [52/100], Loss: 1.0891\n",
      "Epoch [53/100], Loss: 1.0858\n",
      "Epoch [54/100], Loss: 1.0839\n",
      "Epoch [55/100], Loss: 1.0829\n",
      "Epoch [56/100], Loss: 1.0833\n",
      "Epoch [57/100], Loss: 1.0787\n",
      "Epoch [58/100], Loss: 1.0814\n",
      "Epoch [59/100], Loss: 1.0790\n",
      "Epoch [60/100], Loss: 1.0787\n",
      "Epoch [61/100], Loss: 1.0755\n",
      "Epoch [62/100], Loss: 1.0771\n",
      "Epoch [63/100], Loss: 1.0737\n",
      "Epoch [64/100], Loss: 1.0738\n",
      "Epoch [65/100], Loss: 1.0738\n",
      "Epoch [66/100], Loss: 1.0710\n",
      "Epoch [67/100], Loss: 1.0695\n",
      "Epoch [68/100], Loss: 1.0707\n",
      "Epoch [69/100], Loss: 1.0688\n",
      "Epoch [70/100], Loss: 1.0683\n",
      "Epoch [71/100], Loss: 1.0666\n",
      "Epoch [72/100], Loss: 1.0665\n",
      "Epoch [73/100], Loss: 1.0632\n",
      "Epoch [74/100], Loss: 1.0635\n",
      "Epoch [75/100], Loss: 1.0659\n",
      "Epoch [76/100], Loss: 1.0629\n",
      "Epoch [77/100], Loss: 1.0629\n",
      "Epoch [78/100], Loss: 1.0636\n",
      "Epoch [79/100], Loss: 1.0614\n",
      "Epoch [80/100], Loss: 1.0601\n",
      "Epoch [81/100], Loss: 1.0587\n",
      "Epoch [82/100], Loss: 1.0597\n",
      "Epoch [83/100], Loss: 1.0572\n",
      "Epoch [84/100], Loss: 1.0572\n",
      "Epoch [85/100], Loss: 1.0609\n",
      "Epoch [86/100], Loss: 1.0568\n",
      "Epoch [87/100], Loss: 1.0553\n",
      "Epoch [88/100], Loss: 1.0545\n",
      "Epoch [89/100], Loss: 1.0554\n",
      "Epoch [90/100], Loss: 1.0525\n",
      "Epoch [91/100], Loss: 1.0510\n",
      "Epoch [92/100], Loss: 1.0532\n",
      "Epoch [93/100], Loss: 1.0513\n",
      "Epoch [94/100], Loss: 1.0515\n",
      "Epoch [95/100], Loss: 1.0513\n",
      "Epoch [96/100], Loss: 1.0488\n",
      "Epoch [97/100], Loss: 1.0477\n",
      "Epoch [98/100], Loss: 1.0511\n",
      "Epoch [99/100], Loss: 1.0518\n",
      "Epoch [100/100], Loss: 1.0496\n",
      "Subset 50000, Epoch [100/100], Loss: 1.0496\n",
      "Test Accuracy Logit Lipschitz: 54.27%\n",
      "Epoch [1/100], Loss: 1.7173\n",
      "Epoch [2/100], Loss: 1.3587\n",
      "Epoch [3/100], Loss: 1.3018\n",
      "Epoch [4/100], Loss: 1.2665\n",
      "Epoch [5/100], Loss: 1.2467\n",
      "Epoch [6/100], Loss: 1.2257\n",
      "Epoch [7/100], Loss: 1.2145\n",
      "Epoch [8/100], Loss: 1.2027\n",
      "Epoch [9/100], Loss: 1.1921\n",
      "Epoch [10/100], Loss: 1.1874\n",
      "Epoch [11/100], Loss: 1.1783\n",
      "Epoch [12/100], Loss: 1.1704\n",
      "Epoch [13/100], Loss: 1.1644\n",
      "Epoch [14/100], Loss: 1.1600\n",
      "Epoch [15/100], Loss: 1.1538\n",
      "Epoch [16/100], Loss: 1.1501\n",
      "Epoch [17/100], Loss: 1.1423\n",
      "Epoch [18/100], Loss: 1.1386\n",
      "Epoch [19/100], Loss: 1.1372\n",
      "Epoch [20/100], Loss: 1.1324\n",
      "Epoch [21/100], Loss: 1.1281\n",
      "Epoch [22/100], Loss: 1.1263\n",
      "Epoch [23/100], Loss: 1.1242\n",
      "Epoch [24/100], Loss: 1.1170\n",
      "Epoch [25/100], Loss: 1.1150\n",
      "Epoch [26/100], Loss: 1.1155\n",
      "Epoch [27/100], Loss: 1.1111\n",
      "Epoch [28/100], Loss: 1.1096\n",
      "Epoch [29/100], Loss: 1.1083\n",
      "Epoch [30/100], Loss: 1.1043\n",
      "Epoch [31/100], Loss: 1.1034\n",
      "Epoch [32/100], Loss: 1.0972\n",
      "Epoch [33/100], Loss: 1.0977\n",
      "Epoch [34/100], Loss: 1.0938\n",
      "Epoch [35/100], Loss: 1.0931\n",
      "Epoch [36/100], Loss: 1.0935\n",
      "Epoch [37/100], Loss: 1.0892\n",
      "Epoch [38/100], Loss: 1.0856\n",
      "Epoch [39/100], Loss: 1.0870\n",
      "Epoch [40/100], Loss: 1.0844\n",
      "Epoch [41/100], Loss: 1.0834\n",
      "Epoch [42/100], Loss: 1.0816\n",
      "Epoch [43/100], Loss: 1.0810\n",
      "Epoch [44/100], Loss: 1.0798\n",
      "Epoch [45/100], Loss: 1.0767\n",
      "Epoch [46/100], Loss: 1.0762\n",
      "Epoch [47/100], Loss: 1.0744\n",
      "Epoch [48/100], Loss: 1.0712\n",
      "Epoch [49/100], Loss: 1.0727\n",
      "Epoch [50/100], Loss: 1.0704\n",
      "Epoch [51/100], Loss: 1.0680\n",
      "Epoch [52/100], Loss: 1.0671\n",
      "Epoch [53/100], Loss: 1.0644\n",
      "Epoch [54/100], Loss: 1.0658\n",
      "Epoch [55/100], Loss: 1.0624\n",
      "Epoch [56/100], Loss: 1.0628\n",
      "Epoch [57/100], Loss: 1.0627\n",
      "Epoch [58/100], Loss: 1.0603\n",
      "Epoch [59/100], Loss: 1.0585\n",
      "Epoch [60/100], Loss: 1.0564\n",
      "Epoch [61/100], Loss: 1.0590\n",
      "Epoch [62/100], Loss: 1.0574\n",
      "Epoch [63/100], Loss: 1.0566\n",
      "Epoch [64/100], Loss: 1.0542\n",
      "Epoch [65/100], Loss: 1.0550\n",
      "Epoch [66/100], Loss: 1.0518\n",
      "Epoch [67/100], Loss: 1.0527\n",
      "Epoch [68/100], Loss: 1.0521\n",
      "Epoch [69/100], Loss: 1.0477\n",
      "Epoch [70/100], Loss: 1.0498\n",
      "Epoch [71/100], Loss: 1.0500\n",
      "Epoch [72/100], Loss: 1.0445\n",
      "Epoch [73/100], Loss: 1.0476\n",
      "Epoch [74/100], Loss: 1.0448\n",
      "Epoch [75/100], Loss: 1.0455\n",
      "Epoch [76/100], Loss: 1.0418\n",
      "Epoch [77/100], Loss: 1.0428\n",
      "Epoch [78/100], Loss: 1.0435\n",
      "Epoch [79/100], Loss: 1.0422\n",
      "Epoch [80/100], Loss: 1.0397\n",
      "Epoch [81/100], Loss: 1.0428\n",
      "Epoch [82/100], Loss: 1.0397\n",
      "Epoch [83/100], Loss: 1.0411\n",
      "Epoch [84/100], Loss: 1.0385\n",
      "Epoch [85/100], Loss: 1.0380\n",
      "Epoch [86/100], Loss: 1.0363\n",
      "Epoch [87/100], Loss: 1.0384\n",
      "Epoch [88/100], Loss: 1.0363\n",
      "Epoch [89/100], Loss: 1.0349\n",
      "Epoch [90/100], Loss: 1.0338\n",
      "Epoch [91/100], Loss: 1.0362\n",
      "Epoch [92/100], Loss: 1.0356\n",
      "Epoch [93/100], Loss: 1.0319\n",
      "Epoch [94/100], Loss: 1.0333\n",
      "Epoch [95/100], Loss: 1.0327\n",
      "Epoch [96/100], Loss: 1.0338\n",
      "Epoch [97/100], Loss: 1.0283\n",
      "Epoch [98/100], Loss: 1.0310\n",
      "Epoch [99/100], Loss: 1.0312\n",
      "Epoch [100/100], Loss: 1.0306\n",
      "Subset 50000, Epoch [100/100], Loss: 1.0306\n",
      "Test Accuracy Logit Lipschitz: 53.92%\n",
      "Epoch [1/100], Loss: 1.7179\n",
      "Epoch [2/100], Loss: 1.3575\n",
      "Epoch [3/100], Loss: 1.2979\n",
      "Epoch [4/100], Loss: 1.2650\n",
      "Epoch [5/100], Loss: 1.2404\n",
      "Epoch [6/100], Loss: 1.2269\n",
      "Epoch [7/100], Loss: 1.2131\n",
      "Epoch [8/100], Loss: 1.2008\n",
      "Epoch [9/100], Loss: 1.1902\n",
      "Epoch [10/100], Loss: 1.1847\n",
      "Epoch [11/100], Loss: 1.1799\n",
      "Epoch [12/100], Loss: 1.1702\n",
      "Epoch [13/100], Loss: 1.1614\n",
      "Epoch [14/100], Loss: 1.1567\n",
      "Epoch [15/100], Loss: 1.1501\n",
      "Epoch [16/100], Loss: 1.1493\n",
      "Epoch [17/100], Loss: 1.1420\n",
      "Epoch [18/100], Loss: 1.1404\n",
      "Epoch [19/100], Loss: 1.1327\n",
      "Epoch [20/100], Loss: 1.1316\n",
      "Epoch [21/100], Loss: 1.1258\n",
      "Epoch [22/100], Loss: 1.1186\n",
      "Epoch [23/100], Loss: 1.1203\n",
      "Epoch [24/100], Loss: 1.1154\n",
      "Epoch [25/100], Loss: 1.1132\n",
      "Epoch [26/100], Loss: 1.1114\n",
      "Epoch [27/100], Loss: 1.1101\n",
      "Epoch [28/100], Loss: 1.1063\n",
      "Epoch [29/100], Loss: 1.1080\n",
      "Epoch [30/100], Loss: 1.1016\n",
      "Epoch [31/100], Loss: 1.0981\n",
      "Epoch [32/100], Loss: 1.0970\n",
      "Epoch [33/100], Loss: 1.0948\n",
      "Epoch [34/100], Loss: 1.0918\n",
      "Epoch [35/100], Loss: 1.0925\n",
      "Epoch [36/100], Loss: 1.0878\n",
      "Epoch [37/100], Loss: 1.0889\n",
      "Epoch [38/100], Loss: 1.0861\n",
      "Epoch [39/100], Loss: 1.0830\n",
      "Epoch [40/100], Loss: 1.0834\n",
      "Epoch [41/100], Loss: 1.0807\n",
      "Epoch [42/100], Loss: 1.0787\n",
      "Epoch [43/100], Loss: 1.0773\n",
      "Epoch [44/100], Loss: 1.0785\n",
      "Epoch [45/100], Loss: 1.0748\n",
      "Epoch [46/100], Loss: 1.0709\n",
      "Epoch [47/100], Loss: 1.0718\n",
      "Epoch [48/100], Loss: 1.0713\n",
      "Epoch [49/100], Loss: 1.0710\n",
      "Epoch [50/100], Loss: 1.0699\n",
      "Epoch [51/100], Loss: 1.0654\n",
      "Epoch [52/100], Loss: 1.0649\n",
      "Epoch [53/100], Loss: 1.0649\n",
      "Epoch [54/100], Loss: 1.0633\n",
      "Epoch [55/100], Loss: 1.0613\n",
      "Epoch [56/100], Loss: 1.0596\n",
      "Epoch [57/100], Loss: 1.0597\n",
      "Epoch [58/100], Loss: 1.0591\n",
      "Epoch [59/100], Loss: 1.0570\n",
      "Epoch [60/100], Loss: 1.0574\n",
      "Epoch [61/100], Loss: 1.0544\n",
      "Epoch [62/100], Loss: 1.0542\n",
      "Epoch [63/100], Loss: 1.0519\n",
      "Epoch [64/100], Loss: 1.0535\n",
      "Epoch [65/100], Loss: 1.0533\n",
      "Epoch [66/100], Loss: 1.0508\n",
      "Epoch [67/100], Loss: 1.0481\n",
      "Epoch [68/100], Loss: 1.0491\n",
      "Epoch [69/100], Loss: 1.0501\n",
      "Epoch [70/100], Loss: 1.0474\n",
      "Epoch [71/100], Loss: 1.0456\n",
      "Epoch [72/100], Loss: 1.0455\n",
      "Epoch [73/100], Loss: 1.0432\n",
      "Epoch [74/100], Loss: 1.0451\n",
      "Epoch [75/100], Loss: 1.0422\n",
      "Epoch [76/100], Loss: 1.0412\n",
      "Epoch [77/100], Loss: 1.0411\n",
      "Epoch [78/100], Loss: 1.0447\n",
      "Epoch [79/100], Loss: 1.0385\n",
      "Epoch [80/100], Loss: 1.0402\n",
      "Epoch [81/100], Loss: 1.0393\n",
      "Epoch [82/100], Loss: 1.0398\n",
      "Epoch [83/100], Loss: 1.0408\n",
      "Epoch [84/100], Loss: 1.0362\n",
      "Epoch [85/100], Loss: 1.0367\n",
      "Epoch [86/100], Loss: 1.0356\n",
      "Epoch [87/100], Loss: 1.0348\n",
      "Epoch [88/100], Loss: 1.0328\n",
      "Epoch [89/100], Loss: 1.0336\n",
      "Epoch [90/100], Loss: 1.0329\n",
      "Epoch [91/100], Loss: 1.0312\n",
      "Epoch [92/100], Loss: 1.0324\n",
      "Epoch [93/100], Loss: 1.0321\n",
      "Epoch [94/100], Loss: 1.0319\n",
      "Epoch [95/100], Loss: 1.0322\n",
      "Epoch [96/100], Loss: 1.0296\n",
      "Epoch [97/100], Loss: 1.0289\n",
      "Epoch [98/100], Loss: 1.0284\n",
      "Epoch [99/100], Loss: 1.0300\n",
      "Epoch [100/100], Loss: 1.0282\n",
      "Subset 50000, Epoch [100/100], Loss: 1.0282\n",
      "Test Accuracy Logit Lipschitz: 54.29%\n",
      "Epoch [1/100], Loss: 1.7206\n",
      "Epoch [2/100], Loss: 1.3672\n",
      "Epoch [3/100], Loss: 1.3056\n",
      "Epoch [4/100], Loss: 1.2743\n",
      "Epoch [5/100], Loss: 1.2490\n",
      "Epoch [6/100], Loss: 1.2338\n",
      "Epoch [7/100], Loss: 1.2198\n",
      "Epoch [8/100], Loss: 1.2090\n",
      "Epoch [9/100], Loss: 1.1979\n",
      "Epoch [10/100], Loss: 1.1900\n",
      "Epoch [11/100], Loss: 1.1825\n",
      "Epoch [12/100], Loss: 1.1741\n",
      "Epoch [13/100], Loss: 1.1686\n",
      "Epoch [14/100], Loss: 1.1627\n",
      "Epoch [15/100], Loss: 1.1591\n",
      "Epoch [16/100], Loss: 1.1542\n",
      "Epoch [17/100], Loss: 1.1509\n",
      "Epoch [18/100], Loss: 1.1444\n",
      "Epoch [19/100], Loss: 1.1418\n",
      "Epoch [20/100], Loss: 1.1373\n",
      "Epoch [21/100], Loss: 1.1325\n",
      "Epoch [22/100], Loss: 1.1292\n",
      "Epoch [23/100], Loss: 1.1256\n",
      "Epoch [24/100], Loss: 1.1249\n",
      "Epoch [25/100], Loss: 1.1185\n",
      "Epoch [26/100], Loss: 1.1191\n",
      "Epoch [27/100], Loss: 1.1134\n",
      "Epoch [28/100], Loss: 1.1107\n",
      "Epoch [29/100], Loss: 1.1100\n",
      "Epoch [30/100], Loss: 1.1045\n",
      "Epoch [31/100], Loss: 1.1043\n",
      "Epoch [32/100], Loss: 1.1016\n",
      "Epoch [33/100], Loss: 1.1012\n",
      "Epoch [34/100], Loss: 1.0978\n",
      "Epoch [35/100], Loss: 1.0977\n",
      "Epoch [36/100], Loss: 1.0944\n",
      "Epoch [37/100], Loss: 1.0942\n",
      "Epoch [38/100], Loss: 1.0917\n",
      "Epoch [39/100], Loss: 1.0905\n",
      "Epoch [40/100], Loss: 1.0871\n",
      "Epoch [41/100], Loss: 1.0850\n",
      "Epoch [42/100], Loss: 1.0832\n",
      "Epoch [43/100], Loss: 1.0815\n",
      "Epoch [44/100], Loss: 1.0822\n",
      "Epoch [45/100], Loss: 1.0798\n",
      "Epoch [46/100], Loss: 1.0799\n",
      "Epoch [47/100], Loss: 1.0765\n",
      "Epoch [48/100], Loss: 1.0741\n",
      "Epoch [49/100], Loss: 1.0750\n",
      "Epoch [50/100], Loss: 1.0704\n",
      "Epoch [51/100], Loss: 1.0720\n",
      "Epoch [52/100], Loss: 1.0702\n",
      "Epoch [53/100], Loss: 1.0712\n",
      "Epoch [54/100], Loss: 1.0693\n",
      "Epoch [55/100], Loss: 1.0670\n",
      "Epoch [56/100], Loss: 1.0637\n",
      "Epoch [57/100], Loss: 1.0641\n",
      "Epoch [58/100], Loss: 1.0632\n",
      "Epoch [59/100], Loss: 1.0652\n",
      "Epoch [60/100], Loss: 1.0610\n",
      "Epoch [61/100], Loss: 1.0605\n",
      "Epoch [62/100], Loss: 1.0579\n",
      "Epoch [63/100], Loss: 1.0592\n",
      "Epoch [64/100], Loss: 1.0596\n",
      "Epoch [65/100], Loss: 1.0563\n",
      "Epoch [66/100], Loss: 1.0575\n",
      "Epoch [67/100], Loss: 1.0558\n",
      "Epoch [68/100], Loss: 1.0538\n",
      "Epoch [69/100], Loss: 1.0514\n",
      "Epoch [70/100], Loss: 1.0497\n",
      "Epoch [71/100], Loss: 1.0516\n",
      "Epoch [72/100], Loss: 1.0509\n",
      "Epoch [73/100], Loss: 1.0502\n",
      "Epoch [74/100], Loss: 1.0481\n",
      "Epoch [75/100], Loss: 1.0484\n",
      "Epoch [76/100], Loss: 1.0464\n",
      "Epoch [77/100], Loss: 1.0462\n",
      "Epoch [78/100], Loss: 1.0449\n",
      "Epoch [79/100], Loss: 1.0439\n",
      "Epoch [80/100], Loss: 1.0440\n",
      "Epoch [81/100], Loss: 1.0438\n",
      "Epoch [82/100], Loss: 1.0410\n",
      "Epoch [83/100], Loss: 1.0413\n",
      "Epoch [84/100], Loss: 1.0407\n",
      "Epoch [85/100], Loss: 1.0406\n",
      "Epoch [86/100], Loss: 1.0394\n",
      "Epoch [87/100], Loss: 1.0390\n",
      "Epoch [88/100], Loss: 1.0382\n",
      "Epoch [89/100], Loss: 1.0402\n",
      "Epoch [90/100], Loss: 1.0357\n",
      "Epoch [91/100], Loss: 1.0351\n",
      "Epoch [92/100], Loss: 1.0343\n",
      "Epoch [93/100], Loss: 1.0372\n",
      "Epoch [94/100], Loss: 1.0336\n",
      "Epoch [95/100], Loss: 1.0339\n",
      "Epoch [96/100], Loss: 1.0343\n",
      "Epoch [97/100], Loss: 1.0318\n",
      "Epoch [98/100], Loss: 1.0316\n",
      "Epoch [99/100], Loss: 1.0337\n",
      "Epoch [100/100], Loss: 1.0299\n",
      "Subset 50000, Epoch [100/100], Loss: 1.0299\n",
      "Test Accuracy Logit Lipschitz: 54.00%\n",
      "Epoch [1/100], Loss: 1.7234\n",
      "Epoch [2/100], Loss: 1.3676\n",
      "Epoch [3/100], Loss: 1.3060\n",
      "Epoch [4/100], Loss: 1.2744\n",
      "Epoch [5/100], Loss: 1.2512\n",
      "Epoch [6/100], Loss: 1.2329\n",
      "Epoch [7/100], Loss: 1.2206\n",
      "Epoch [8/100], Loss: 1.2104\n",
      "Epoch [9/100], Loss: 1.1990\n",
      "Epoch [10/100], Loss: 1.1898\n",
      "Epoch [11/100], Loss: 1.1842\n",
      "Epoch [12/100], Loss: 1.1752\n",
      "Epoch [13/100], Loss: 1.1713\n",
      "Epoch [14/100], Loss: 1.1629\n",
      "Epoch [15/100], Loss: 1.1624\n",
      "Epoch [16/100], Loss: 1.1538\n",
      "Epoch [17/100], Loss: 1.1490\n",
      "Epoch [18/100], Loss: 1.1463\n",
      "Epoch [19/100], Loss: 1.1418\n",
      "Epoch [20/100], Loss: 1.1377\n",
      "Epoch [21/100], Loss: 1.1336\n",
      "Epoch [22/100], Loss: 1.1337\n",
      "Epoch [23/100], Loss: 1.1275\n",
      "Epoch [24/100], Loss: 1.1241\n",
      "Epoch [25/100], Loss: 1.1193\n",
      "Epoch [26/100], Loss: 1.1182\n",
      "Epoch [27/100], Loss: 1.1155\n",
      "Epoch [28/100], Loss: 1.1137\n",
      "Epoch [29/100], Loss: 1.1113\n",
      "Epoch [30/100], Loss: 1.1119\n",
      "Epoch [31/100], Loss: 1.1066\n",
      "Epoch [32/100], Loss: 1.1045\n",
      "Epoch [33/100], Loss: 1.1021\n",
      "Epoch [34/100], Loss: 1.0972\n",
      "Epoch [35/100], Loss: 1.0980\n",
      "Epoch [36/100], Loss: 1.0948\n",
      "Epoch [37/100], Loss: 1.0920\n",
      "Epoch [38/100], Loss: 1.0922\n",
      "Epoch [39/100], Loss: 1.0909\n",
      "Epoch [40/100], Loss: 1.0908\n",
      "Epoch [41/100], Loss: 1.0869\n",
      "Epoch [42/100], Loss: 1.0858\n",
      "Epoch [43/100], Loss: 1.0831\n",
      "Epoch [44/100], Loss: 1.0816\n",
      "Epoch [45/100], Loss: 1.0815\n",
      "Epoch [46/100], Loss: 1.0797\n",
      "Epoch [47/100], Loss: 1.0798\n",
      "Epoch [48/100], Loss: 1.0768\n",
      "Epoch [49/100], Loss: 1.0769\n",
      "Epoch [50/100], Loss: 1.0755\n",
      "Epoch [51/100], Loss: 1.0735\n",
      "Epoch [52/100], Loss: 1.0736\n",
      "Epoch [53/100], Loss: 1.0689\n",
      "Epoch [54/100], Loss: 1.0688\n",
      "Epoch [55/100], Loss: 1.0699\n",
      "Epoch [56/100], Loss: 1.0676\n",
      "Epoch [57/100], Loss: 1.0658\n",
      "Epoch [58/100], Loss: 1.0642\n",
      "Epoch [59/100], Loss: 1.0640\n",
      "Epoch [60/100], Loss: 1.0622\n",
      "Epoch [61/100], Loss: 1.0618\n",
      "Epoch [62/100], Loss: 1.0623\n",
      "Epoch [63/100], Loss: 1.0597\n",
      "Epoch [64/100], Loss: 1.0611\n",
      "Epoch [65/100], Loss: 1.0581\n",
      "Epoch [66/100], Loss: 1.0573\n",
      "Epoch [67/100], Loss: 1.0571\n",
      "Epoch [68/100], Loss: 1.0560\n",
      "Epoch [69/100], Loss: 1.0536\n",
      "Epoch [70/100], Loss: 1.0531\n",
      "Epoch [71/100], Loss: 1.0518\n",
      "Epoch [72/100], Loss: 1.0534\n",
      "Epoch [73/100], Loss: 1.0490\n",
      "Epoch [74/100], Loss: 1.0515\n",
      "Epoch [75/100], Loss: 1.0504\n",
      "Epoch [76/100], Loss: 1.0489\n",
      "Epoch [77/100], Loss: 1.0478\n",
      "Epoch [78/100], Loss: 1.0491\n",
      "Epoch [79/100], Loss: 1.0467\n",
      "Epoch [80/100], Loss: 1.0466\n",
      "Epoch [81/100], Loss: 1.0455\n",
      "Epoch [82/100], Loss: 1.0446\n",
      "Epoch [83/100], Loss: 1.0475\n",
      "Epoch [84/100], Loss: 1.0434\n",
      "Epoch [85/100], Loss: 1.0451\n",
      "Epoch [86/100], Loss: 1.0424\n",
      "Epoch [87/100], Loss: 1.0415\n",
      "Epoch [88/100], Loss: 1.0411\n",
      "Epoch [89/100], Loss: 1.0424\n",
      "Epoch [90/100], Loss: 1.0412\n",
      "Epoch [91/100], Loss: 1.0394\n",
      "Epoch [92/100], Loss: 1.0409\n",
      "Epoch [93/100], Loss: 1.0372\n",
      "Epoch [94/100], Loss: 1.0389\n",
      "Epoch [95/100], Loss: 1.0401\n",
      "Epoch [96/100], Loss: 1.0366\n",
      "Epoch [97/100], Loss: 1.0338\n",
      "Epoch [98/100], Loss: 1.0357\n",
      "Epoch [99/100], Loss: 1.0359\n",
      "Epoch [100/100], Loss: 1.0328\n",
      "Subset 50000, Epoch [100/100], Loss: 1.0328\n",
      "Test Accuracy Logit Lipschitz: 53.98%\n",
      "Epoch [1/100], Loss: 1.7291\n",
      "Epoch [2/100], Loss: 1.3755\n",
      "Epoch [3/100], Loss: 1.3136\n",
      "Epoch [4/100], Loss: 1.2796\n",
      "Epoch [5/100], Loss: 1.2589\n",
      "Epoch [6/100], Loss: 1.2420\n",
      "Epoch [7/100], Loss: 1.2280\n",
      "Epoch [8/100], Loss: 1.2137\n",
      "Epoch [9/100], Loss: 1.2035\n",
      "Epoch [10/100], Loss: 1.1950\n",
      "Epoch [11/100], Loss: 1.1887\n",
      "Epoch [12/100], Loss: 1.1805\n",
      "Epoch [13/100], Loss: 1.1742\n",
      "Epoch [14/100], Loss: 1.1685\n",
      "Epoch [15/100], Loss: 1.1641\n",
      "Epoch [16/100], Loss: 1.1581\n",
      "Epoch [17/100], Loss: 1.1568\n",
      "Epoch [18/100], Loss: 1.1496\n",
      "Epoch [19/100], Loss: 1.1463\n",
      "Epoch [20/100], Loss: 1.1448\n",
      "Epoch [21/100], Loss: 1.1416\n",
      "Epoch [22/100], Loss: 1.1376\n",
      "Epoch [23/100], Loss: 1.1343\n",
      "Epoch [24/100], Loss: 1.1268\n",
      "Epoch [25/100], Loss: 1.1262\n",
      "Epoch [26/100], Loss: 1.1220\n",
      "Epoch [27/100], Loss: 1.1231\n",
      "Epoch [28/100], Loss: 1.1181\n",
      "Epoch [29/100], Loss: 1.1171\n",
      "Epoch [30/100], Loss: 1.1146\n",
      "Epoch [31/100], Loss: 1.1124\n",
      "Epoch [32/100], Loss: 1.1100\n",
      "Epoch [33/100], Loss: 1.1071\n",
      "Epoch [34/100], Loss: 1.1045\n",
      "Epoch [35/100], Loss: 1.1012\n",
      "Epoch [36/100], Loss: 1.0993\n",
      "Epoch [37/100], Loss: 1.0996\n",
      "Epoch [38/100], Loss: 1.0952\n",
      "Epoch [39/100], Loss: 1.0972\n",
      "Epoch [40/100], Loss: 1.0922\n",
      "Epoch [41/100], Loss: 1.0902\n",
      "Epoch [42/100], Loss: 1.0913\n",
      "Epoch [43/100], Loss: 1.0910\n",
      "Epoch [44/100], Loss: 1.0889\n",
      "Epoch [45/100], Loss: 1.0893\n",
      "Epoch [46/100], Loss: 1.0830\n",
      "Epoch [47/100], Loss: 1.0826\n",
      "Epoch [48/100], Loss: 1.0810\n",
      "Epoch [49/100], Loss: 1.0807\n",
      "Epoch [50/100], Loss: 1.0792\n",
      "Epoch [51/100], Loss: 1.0795\n",
      "Epoch [52/100], Loss: 1.0774\n",
      "Epoch [53/100], Loss: 1.0737\n",
      "Epoch [54/100], Loss: 1.0751\n",
      "Epoch [55/100], Loss: 1.0727\n",
      "Epoch [56/100], Loss: 1.0722\n",
      "Epoch [57/100], Loss: 1.0702\n",
      "Epoch [58/100], Loss: 1.0693\n",
      "Epoch [59/100], Loss: 1.0710\n",
      "Epoch [60/100], Loss: 1.0690\n",
      "Epoch [61/100], Loss: 1.0673\n",
      "Epoch [62/100], Loss: 1.0651\n",
      "Epoch [63/100], Loss: 1.0639\n",
      "Epoch [64/100], Loss: 1.0657\n",
      "Epoch [65/100], Loss: 1.0613\n",
      "Epoch [66/100], Loss: 1.0629\n",
      "Epoch [67/100], Loss: 1.0618\n",
      "Epoch [68/100], Loss: 1.0565\n",
      "Epoch [69/100], Loss: 1.0595\n",
      "Epoch [70/100], Loss: 1.0587\n",
      "Epoch [71/100], Loss: 1.0579\n",
      "Epoch [72/100], Loss: 1.0556\n",
      "Epoch [73/100], Loss: 1.0542\n",
      "Epoch [74/100], Loss: 1.0581\n",
      "Epoch [75/100], Loss: 1.0569\n",
      "Epoch [76/100], Loss: 1.0526\n",
      "Epoch [77/100], Loss: 1.0534\n",
      "Epoch [78/100], Loss: 1.0534\n",
      "Epoch [79/100], Loss: 1.0490\n",
      "Epoch [80/100], Loss: 1.0502\n",
      "Epoch [81/100], Loss: 1.0484\n",
      "Epoch [82/100], Loss: 1.0470\n",
      "Epoch [83/100], Loss: 1.0513\n",
      "Epoch [84/100], Loss: 1.0482\n",
      "Epoch [85/100], Loss: 1.0465\n",
      "Epoch [86/100], Loss: 1.0477\n",
      "Epoch [87/100], Loss: 1.0463\n",
      "Epoch [88/100], Loss: 1.0442\n",
      "Epoch [89/100], Loss: 1.0453\n",
      "Epoch [90/100], Loss: 1.0459\n",
      "Epoch [91/100], Loss: 1.0420\n",
      "Epoch [92/100], Loss: 1.0434\n",
      "Epoch [93/100], Loss: 1.0412\n",
      "Epoch [94/100], Loss: 1.0425\n",
      "Epoch [95/100], Loss: 1.0412\n",
      "Epoch [96/100], Loss: 1.0420\n",
      "Epoch [97/100], Loss: 1.0405\n",
      "Epoch [98/100], Loss: 1.0374\n",
      "Epoch [99/100], Loss: 1.0380\n",
      "Epoch [100/100], Loss: 1.0387\n",
      "Subset 50000, Epoch [100/100], Loss: 1.0387\n",
      "Test Accuracy Logit Lipschitz: 53.99%\n",
      "Epoch [1/100], Loss: 1.7152\n",
      "Epoch [2/100], Loss: 1.3582\n",
      "Epoch [3/100], Loss: 1.3003\n",
      "Epoch [4/100], Loss: 1.2632\n",
      "Epoch [5/100], Loss: 1.2451\n",
      "Epoch [6/100], Loss: 1.2268\n",
      "Epoch [7/100], Loss: 1.2109\n",
      "Epoch [8/100], Loss: 1.2006\n",
      "Epoch [9/100], Loss: 1.1925\n",
      "Epoch [10/100], Loss: 1.1874\n",
      "Epoch [11/100], Loss: 1.1762\n",
      "Epoch [12/100], Loss: 1.1709\n",
      "Epoch [13/100], Loss: 1.1669\n",
      "Epoch [14/100], Loss: 1.1624\n",
      "Epoch [15/100], Loss: 1.1550\n",
      "Epoch [16/100], Loss: 1.1499\n",
      "Epoch [17/100], Loss: 1.1446\n",
      "Epoch [18/100], Loss: 1.1415\n",
      "Epoch [19/100], Loss: 1.1354\n",
      "Epoch [20/100], Loss: 1.1318\n",
      "Epoch [21/100], Loss: 1.1297\n",
      "Epoch [22/100], Loss: 1.1266\n",
      "Epoch [23/100], Loss: 1.1222\n",
      "Epoch [24/100], Loss: 1.1203\n",
      "Epoch [25/100], Loss: 1.1181\n",
      "Epoch [26/100], Loss: 1.1132\n",
      "Epoch [27/100], Loss: 1.1107\n",
      "Epoch [28/100], Loss: 1.1088\n",
      "Epoch [29/100], Loss: 1.1082\n",
      "Epoch [30/100], Loss: 1.1039\n",
      "Epoch [31/100], Loss: 1.1013\n",
      "Epoch [32/100], Loss: 1.0994\n",
      "Epoch [33/100], Loss: 1.1007\n",
      "Epoch [34/100], Loss: 1.0962\n",
      "Epoch [35/100], Loss: 1.0941\n",
      "Epoch [36/100], Loss: 1.0921\n",
      "Epoch [37/100], Loss: 1.0888\n",
      "Epoch [38/100], Loss: 1.0881\n",
      "Epoch [39/100], Loss: 1.0851\n",
      "Epoch [40/100], Loss: 1.0823\n",
      "Epoch [41/100], Loss: 1.0832\n",
      "Epoch [42/100], Loss: 1.0814\n",
      "Epoch [43/100], Loss: 1.0812\n",
      "Epoch [44/100], Loss: 1.0780\n",
      "Epoch [45/100], Loss: 1.0755\n",
      "Epoch [46/100], Loss: 1.0736\n",
      "Epoch [47/100], Loss: 1.0760\n",
      "Epoch [48/100], Loss: 1.0720\n",
      "Epoch [49/100], Loss: 1.0724\n",
      "Epoch [50/100], Loss: 1.0709\n",
      "Epoch [51/100], Loss: 1.0698\n",
      "Epoch [52/100], Loss: 1.0688\n",
      "Epoch [53/100], Loss: 1.0649\n",
      "Epoch [54/100], Loss: 1.0637\n",
      "Epoch [55/100], Loss: 1.0640\n",
      "Epoch [56/100], Loss: 1.0637\n",
      "Epoch [57/100], Loss: 1.0645\n",
      "Epoch [58/100], Loss: 1.0608\n",
      "Epoch [59/100], Loss: 1.0610\n",
      "Epoch [60/100], Loss: 1.0589\n",
      "Epoch [61/100], Loss: 1.0560\n",
      "Epoch [62/100], Loss: 1.0584\n",
      "Epoch [63/100], Loss: 1.0557\n",
      "Epoch [64/100], Loss: 1.0546\n",
      "Epoch [65/100], Loss: 1.0578\n",
      "Epoch [66/100], Loss: 1.0532\n",
      "Epoch [67/100], Loss: 1.0508\n",
      "Epoch [68/100], Loss: 1.0530\n",
      "Epoch [69/100], Loss: 1.0506\n",
      "Epoch [70/100], Loss: 1.0487\n",
      "Epoch [71/100], Loss: 1.0472\n",
      "Epoch [72/100], Loss: 1.0453\n",
      "Epoch [73/100], Loss: 1.0472\n",
      "Epoch [74/100], Loss: 1.0464\n",
      "Epoch [75/100], Loss: 1.0483\n",
      "Epoch [76/100], Loss: 1.0472\n",
      "Epoch [77/100], Loss: 1.0423\n",
      "Epoch [78/100], Loss: 1.0421\n",
      "Epoch [79/100], Loss: 1.0421\n",
      "Epoch [80/100], Loss: 1.0422\n",
      "Epoch [81/100], Loss: 1.0414\n",
      "Epoch [82/100], Loss: 1.0403\n",
      "Epoch [83/100], Loss: 1.0411\n",
      "Epoch [84/100], Loss: 1.0405\n",
      "Epoch [85/100], Loss: 1.0411\n",
      "Epoch [86/100], Loss: 1.0383\n",
      "Epoch [87/100], Loss: 1.0390\n",
      "Epoch [88/100], Loss: 1.0370\n",
      "Epoch [89/100], Loss: 1.0372\n",
      "Epoch [90/100], Loss: 1.0334\n",
      "Epoch [91/100], Loss: 1.0350\n",
      "Epoch [92/100], Loss: 1.0337\n",
      "Epoch [93/100], Loss: 1.0341\n",
      "Epoch [94/100], Loss: 1.0354\n",
      "Epoch [95/100], Loss: 1.0325\n",
      "Epoch [96/100], Loss: 1.0293\n",
      "Epoch [97/100], Loss: 1.0301\n",
      "Epoch [98/100], Loss: 1.0313\n",
      "Epoch [99/100], Loss: 1.0301\n",
      "Epoch [100/100], Loss: 1.0297\n",
      "Subset 50000, Epoch [100/100], Loss: 1.0297\n",
      "Test Accuracy Logit Lipschitz: 54.93%\n",
      "Epoch [1/100], Loss: 1.7282\n",
      "Epoch [2/100], Loss: 1.3747\n",
      "Epoch [3/100], Loss: 1.3110\n",
      "Epoch [4/100], Loss: 1.2774\n",
      "Epoch [5/100], Loss: 1.2579\n",
      "Epoch [6/100], Loss: 1.2388\n",
      "Epoch [7/100], Loss: 1.2254\n",
      "Epoch [8/100], Loss: 1.2116\n",
      "Epoch [9/100], Loss: 1.2051\n",
      "Epoch [10/100], Loss: 1.1966\n",
      "Epoch [11/100], Loss: 1.1897\n",
      "Epoch [12/100], Loss: 1.1811\n",
      "Epoch [13/100], Loss: 1.1733\n",
      "Epoch [14/100], Loss: 1.1716\n",
      "Epoch [15/100], Loss: 1.1668\n",
      "Epoch [16/100], Loss: 1.1601\n",
      "Epoch [17/100], Loss: 1.1548\n",
      "Epoch [18/100], Loss: 1.1521\n",
      "Epoch [19/100], Loss: 1.1483\n",
      "Epoch [20/100], Loss: 1.1462\n",
      "Epoch [21/100], Loss: 1.1407\n",
      "Epoch [22/100], Loss: 1.1363\n",
      "Epoch [23/100], Loss: 1.1312\n",
      "Epoch [24/100], Loss: 1.1279\n",
      "Epoch [25/100], Loss: 1.1267\n",
      "Epoch [26/100], Loss: 1.1260\n",
      "Epoch [27/100], Loss: 1.1210\n",
      "Epoch [28/100], Loss: 1.1197\n",
      "Epoch [29/100], Loss: 1.1148\n",
      "Epoch [30/100], Loss: 1.1132\n",
      "Epoch [31/100], Loss: 1.1095\n",
      "Epoch [32/100], Loss: 1.1089\n",
      "Epoch [33/100], Loss: 1.1064\n",
      "Epoch [34/100], Loss: 1.1063\n",
      "Epoch [35/100], Loss: 1.1030\n",
      "Epoch [36/100], Loss: 1.1029\n",
      "Epoch [37/100], Loss: 1.1000\n",
      "Epoch [38/100], Loss: 1.0981\n",
      "Epoch [39/100], Loss: 1.0949\n",
      "Epoch [40/100], Loss: 1.0904\n",
      "Epoch [41/100], Loss: 1.0914\n",
      "Epoch [42/100], Loss: 1.0894\n",
      "Epoch [43/100], Loss: 1.0881\n",
      "Epoch [44/100], Loss: 1.0887\n",
      "Epoch [45/100], Loss: 1.0839\n",
      "Epoch [46/100], Loss: 1.0867\n",
      "Epoch [47/100], Loss: 1.0828\n",
      "Epoch [48/100], Loss: 1.0833\n",
      "Epoch [49/100], Loss: 1.0804\n",
      "Epoch [50/100], Loss: 1.0789\n",
      "Epoch [51/100], Loss: 1.0807\n",
      "Epoch [52/100], Loss: 1.0757\n",
      "Epoch [53/100], Loss: 1.0756\n",
      "Epoch [54/100], Loss: 1.0749\n",
      "Epoch [55/100], Loss: 1.0732\n",
      "Epoch [56/100], Loss: 1.0706\n",
      "Epoch [57/100], Loss: 1.0686\n",
      "Epoch [58/100], Loss: 1.0720\n",
      "Epoch [59/100], Loss: 1.0706\n",
      "Epoch [60/100], Loss: 1.0656\n",
      "Epoch [61/100], Loss: 1.0653\n",
      "Epoch [62/100], Loss: 1.0666\n",
      "Epoch [63/100], Loss: 1.0652\n",
      "Epoch [64/100], Loss: 1.0622\n",
      "Epoch [65/100], Loss: 1.0599\n",
      "Epoch [66/100], Loss: 1.0611\n",
      "Epoch [67/100], Loss: 1.0601\n",
      "Epoch [68/100], Loss: 1.0572\n",
      "Epoch [69/100], Loss: 1.0563\n",
      "Epoch [70/100], Loss: 1.0578\n",
      "Epoch [71/100], Loss: 1.0587\n",
      "Epoch [72/100], Loss: 1.0557\n",
      "Epoch [73/100], Loss: 1.0572\n",
      "Epoch [74/100], Loss: 1.0540\n",
      "Epoch [75/100], Loss: 1.0532\n",
      "Epoch [76/100], Loss: 1.0541\n",
      "Epoch [77/100], Loss: 1.0533\n",
      "Epoch [78/100], Loss: 1.0544\n",
      "Epoch [79/100], Loss: 1.0507\n",
      "Epoch [80/100], Loss: 1.0515\n",
      "Epoch [81/100], Loss: 1.0497\n",
      "Epoch [82/100], Loss: 1.0462\n",
      "Epoch [83/100], Loss: 1.0465\n",
      "Epoch [84/100], Loss: 1.0458\n",
      "Epoch [85/100], Loss: 1.0483\n",
      "Epoch [86/100], Loss: 1.0449\n",
      "Epoch [87/100], Loss: 1.0452\n",
      "Epoch [88/100], Loss: 1.0452\n",
      "Epoch [89/100], Loss: 1.0432\n",
      "Epoch [90/100], Loss: 1.0426\n",
      "Epoch [91/100], Loss: 1.0413\n",
      "Epoch [92/100], Loss: 1.0416\n",
      "Epoch [93/100], Loss: 1.0387\n",
      "Epoch [94/100], Loss: 1.0391\n",
      "Epoch [95/100], Loss: 1.0403\n",
      "Epoch [96/100], Loss: 1.0403\n",
      "Stopping early at epoch 96 (Loss improvement < 0.0001 for 3 epochs)\n",
      "Subset 50000, Epoch [96/100], Loss: 1.0403\n",
      "Test Accuracy Logit Lipschitz: 53.87%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 2.4577\n",
      "Epoch [2/100], Loss: 1.6505\n",
      "Epoch [3/100], Loss: 1.4681\n",
      "Epoch [4/100], Loss: 1.3900\n",
      "Epoch [5/100], Loss: 1.3255\n",
      "Epoch [6/100], Loss: 1.2808\n",
      "Epoch [7/100], Loss: 1.2369\n",
      "Epoch [8/100], Loss: 1.2041\n",
      "Epoch [9/100], Loss: 1.1776\n",
      "Epoch [10/100], Loss: 1.1602\n",
      "Epoch [11/100], Loss: 1.1382\n",
      "Epoch [12/100], Loss: 1.1172\n",
      "Epoch [13/100], Loss: 1.0995\n",
      "Epoch [14/100], Loss: 1.0817\n",
      "Epoch [15/100], Loss: 1.0616\n",
      "Epoch [16/100], Loss: 1.0488\n",
      "Epoch [17/100], Loss: 1.0373\n",
      "Epoch [18/100], Loss: 1.0337\n",
      "Epoch [19/100], Loss: 1.0178\n",
      "Epoch [20/100], Loss: 1.0038\n",
      "Epoch [21/100], Loss: 0.9943\n",
      "Epoch [22/100], Loss: 0.9828\n",
      "Epoch [23/100], Loss: 0.9778\n",
      "Epoch [24/100], Loss: 0.9668\n",
      "Epoch [25/100], Loss: 0.9540\n",
      "Epoch [26/100], Loss: 0.9503\n",
      "Epoch [27/100], Loss: 0.9347\n",
      "Epoch [28/100], Loss: 0.9296\n",
      "Epoch [29/100], Loss: 0.9222\n",
      "Epoch [30/100], Loss: 0.9138\n",
      "Epoch [31/100], Loss: 0.9008\n",
      "Epoch [32/100], Loss: 0.8948\n",
      "Epoch [33/100], Loss: 0.8937\n",
      "Epoch [34/100], Loss: 0.8917\n",
      "Epoch [35/100], Loss: 0.8815\n",
      "Epoch [36/100], Loss: 0.8777\n",
      "Epoch [37/100], Loss: 0.8680\n",
      "Epoch [38/100], Loss: 0.8649\n",
      "Epoch [39/100], Loss: 0.8578\n",
      "Epoch [40/100], Loss: 0.8507\n",
      "Epoch [41/100], Loss: 0.8477\n",
      "Epoch [42/100], Loss: 0.8446\n",
      "Epoch [43/100], Loss: 0.8350\n",
      "Epoch [44/100], Loss: 0.8222\n",
      "Epoch [45/100], Loss: 0.8263\n",
      "Epoch [46/100], Loss: 0.8190\n",
      "Epoch [47/100], Loss: 0.8131\n",
      "Epoch [48/100], Loss: 0.8168\n",
      "Epoch [49/100], Loss: 0.8096\n",
      "Epoch [50/100], Loss: 0.8054\n",
      "Epoch [51/100], Loss: 0.7976\n",
      "Epoch [52/100], Loss: 0.7893\n",
      "Epoch [53/100], Loss: 0.7868\n",
      "Epoch [54/100], Loss: 0.7877\n",
      "Epoch [55/100], Loss: 0.7767\n",
      "Epoch [56/100], Loss: 0.7807\n",
      "Epoch [57/100], Loss: 0.7748\n",
      "Epoch [58/100], Loss: 0.7728\n",
      "Epoch [59/100], Loss: 0.7738\n",
      "Epoch [60/100], Loss: 0.7629\n",
      "Epoch [61/100], Loss: 0.7601\n",
      "Epoch [62/100], Loss: 0.7602\n",
      "Epoch [63/100], Loss: 0.7489\n",
      "Epoch [64/100], Loss: 0.7420\n",
      "Epoch [65/100], Loss: 0.7460\n",
      "Epoch [66/100], Loss: 0.7425\n",
      "Epoch [67/100], Loss: 0.7448\n",
      "Epoch [68/100], Loss: 0.7282\n",
      "Epoch [69/100], Loss: 0.7271\n",
      "Epoch [70/100], Loss: 0.7249\n",
      "Epoch [71/100], Loss: 0.7204\n",
      "Epoch [72/100], Loss: 0.7270\n",
      "Epoch [73/100], Loss: 0.7216\n",
      "Epoch [74/100], Loss: 0.7153\n",
      "Epoch [75/100], Loss: 0.7117\n",
      "Epoch [76/100], Loss: 0.7079\n",
      "Epoch [77/100], Loss: 0.7063\n",
      "Epoch [78/100], Loss: 0.6999\n",
      "Epoch [79/100], Loss: 0.6974\n",
      "Epoch [80/100], Loss: 0.6970\n",
      "Epoch [81/100], Loss: 0.6948\n",
      "Epoch [82/100], Loss: 0.6999\n",
      "Epoch [83/100], Loss: 0.7010\n",
      "Epoch [84/100], Loss: 0.6841\n",
      "Epoch [85/100], Loss: 0.6788\n",
      "Epoch [86/100], Loss: 0.6802\n",
      "Epoch [87/100], Loss: 0.6795\n",
      "Epoch [88/100], Loss: 0.6692\n",
      "Epoch [89/100], Loss: 0.6717\n",
      "Epoch [90/100], Loss: 0.6716\n",
      "Epoch [91/100], Loss: 0.6625\n",
      "Epoch [92/100], Loss: 0.6686\n",
      "Epoch [93/100], Loss: 0.6655\n",
      "Epoch [94/100], Loss: 0.6630\n",
      "Epoch [95/100], Loss: 0.6656\n",
      "Epoch [96/100], Loss: 0.6583\n",
      "Epoch [97/100], Loss: 0.6546\n",
      "Epoch [98/100], Loss: 0.6482\n",
      "Epoch [99/100], Loss: 0.6498\n",
      "Epoch [100/100], Loss: 0.6595\n",
      "Subset 10000, Epoch [100/100], Loss: 0.6595\n",
      "Test Accuracy Logit Lipschitz: 49.46%\n",
      "Epoch [1/100], Loss: 2.4175\n",
      "Epoch [2/100], Loss: 1.5888\n",
      "Epoch [3/100], Loss: 1.4327\n",
      "Epoch [4/100], Loss: 1.3443\n",
      "Epoch [5/100], Loss: 1.2888\n",
      "Epoch [6/100], Loss: 1.2446\n",
      "Epoch [7/100], Loss: 1.2016\n",
      "Epoch [8/100], Loss: 1.1621\n",
      "Epoch [9/100], Loss: 1.1472\n",
      "Epoch [10/100], Loss: 1.1189\n",
      "Epoch [11/100], Loss: 1.1003\n",
      "Epoch [12/100], Loss: 1.0773\n",
      "Epoch [13/100], Loss: 1.0686\n",
      "Epoch [14/100], Loss: 1.0386\n",
      "Epoch [15/100], Loss: 1.0363\n",
      "Epoch [16/100], Loss: 1.0163\n",
      "Epoch [17/100], Loss: 1.0009\n",
      "Epoch [18/100], Loss: 0.9848\n",
      "Epoch [19/100], Loss: 0.9687\n",
      "Epoch [20/100], Loss: 0.9648\n",
      "Epoch [21/100], Loss: 0.9507\n",
      "Epoch [22/100], Loss: 0.9440\n",
      "Epoch [23/100], Loss: 0.9336\n",
      "Epoch [24/100], Loss: 0.9304\n",
      "Epoch [25/100], Loss: 0.9106\n",
      "Epoch [26/100], Loss: 0.9031\n",
      "Epoch [27/100], Loss: 0.8994\n",
      "Epoch [28/100], Loss: 0.8845\n",
      "Epoch [29/100], Loss: 0.8785\n",
      "Epoch [30/100], Loss: 0.8707\n",
      "Epoch [31/100], Loss: 0.8683\n",
      "Epoch [32/100], Loss: 0.8609\n",
      "Epoch [33/100], Loss: 0.8505\n",
      "Epoch [34/100], Loss: 0.8446\n",
      "Epoch [35/100], Loss: 0.8397\n",
      "Epoch [36/100], Loss: 0.8354\n",
      "Epoch [37/100], Loss: 0.8282\n",
      "Epoch [38/100], Loss: 0.8230\n",
      "Epoch [39/100], Loss: 0.8170\n",
      "Epoch [40/100], Loss: 0.8064\n",
      "Epoch [41/100], Loss: 0.8062\n",
      "Epoch [42/100], Loss: 0.8059\n",
      "Epoch [43/100], Loss: 0.8042\n",
      "Epoch [44/100], Loss: 0.7951\n",
      "Epoch [45/100], Loss: 0.7860\n",
      "Epoch [46/100], Loss: 0.7742\n",
      "Epoch [47/100], Loss: 0.7725\n",
      "Epoch [48/100], Loss: 0.7649\n",
      "Epoch [49/100], Loss: 0.7677\n",
      "Epoch [50/100], Loss: 0.7618\n",
      "Epoch [51/100], Loss: 0.7664\n",
      "Epoch [52/100], Loss: 0.7504\n",
      "Epoch [53/100], Loss: 0.7487\n",
      "Epoch [54/100], Loss: 0.7527\n",
      "Epoch [55/100], Loss: 0.7345\n",
      "Epoch [56/100], Loss: 0.7329\n",
      "Epoch [57/100], Loss: 0.7282\n",
      "Epoch [58/100], Loss: 0.7202\n",
      "Epoch [59/100], Loss: 0.7254\n",
      "Epoch [60/100], Loss: 0.7255\n",
      "Epoch [61/100], Loss: 0.7193\n",
      "Epoch [62/100], Loss: 0.7118\n",
      "Epoch [63/100], Loss: 0.7099\n",
      "Epoch [64/100], Loss: 0.7077\n",
      "Epoch [65/100], Loss: 0.6979\n",
      "Epoch [66/100], Loss: 0.6991\n",
      "Epoch [67/100], Loss: 0.6955\n",
      "Epoch [68/100], Loss: 0.6957\n",
      "Epoch [69/100], Loss: 0.6919\n",
      "Epoch [70/100], Loss: 0.6926\n",
      "Epoch [71/100], Loss: 0.6823\n",
      "Epoch [72/100], Loss: 0.6789\n",
      "Epoch [73/100], Loss: 0.6781\n",
      "Epoch [74/100], Loss: 0.6732\n",
      "Epoch [75/100], Loss: 0.6694\n",
      "Epoch [76/100], Loss: 0.6668\n",
      "Epoch [77/100], Loss: 0.6641\n",
      "Epoch [78/100], Loss: 0.6594\n",
      "Epoch [79/100], Loss: 0.6583\n",
      "Epoch [80/100], Loss: 0.6551\n",
      "Epoch [81/100], Loss: 0.6515\n",
      "Epoch [82/100], Loss: 0.6491\n",
      "Epoch [83/100], Loss: 0.6472\n",
      "Epoch [84/100], Loss: 0.6463\n",
      "Epoch [85/100], Loss: 0.6368\n",
      "Epoch [86/100], Loss: 0.6482\n",
      "Epoch [87/100], Loss: 0.6365\n",
      "Epoch [88/100], Loss: 0.6354\n",
      "Epoch [89/100], Loss: 0.6304\n",
      "Epoch [90/100], Loss: 0.6288\n",
      "Epoch [91/100], Loss: 0.6240\n",
      "Epoch [92/100], Loss: 0.6210\n",
      "Epoch [93/100], Loss: 0.6184\n",
      "Epoch [94/100], Loss: 0.6182\n",
      "Epoch [95/100], Loss: 0.6233\n",
      "Epoch [96/100], Loss: 0.6152\n",
      "Epoch [97/100], Loss: 0.6164\n",
      "Epoch [98/100], Loss: 0.6164\n",
      "Epoch [99/100], Loss: 0.6100\n",
      "Epoch [100/100], Loss: 0.6044\n",
      "Subset 10000, Epoch [100/100], Loss: 0.6044\n",
      "Test Accuracy Logit Lipschitz: 49.24%\n",
      "Epoch [1/100], Loss: 2.4584\n",
      "Epoch [2/100], Loss: 1.6337\n",
      "Epoch [3/100], Loss: 1.4693\n",
      "Epoch [4/100], Loss: 1.3688\n",
      "Epoch [5/100], Loss: 1.3069\n",
      "Epoch [6/100], Loss: 1.2636\n",
      "Epoch [7/100], Loss: 1.2286\n",
      "Epoch [8/100], Loss: 1.1947\n",
      "Epoch [9/100], Loss: 1.1648\n",
      "Epoch [10/100], Loss: 1.1448\n",
      "Epoch [11/100], Loss: 1.1202\n",
      "Epoch [12/100], Loss: 1.1008\n",
      "Epoch [13/100], Loss: 1.0829\n",
      "Epoch [14/100], Loss: 1.0636\n",
      "Epoch [15/100], Loss: 1.0495\n",
      "Epoch [16/100], Loss: 1.0429\n",
      "Epoch [17/100], Loss: 1.0227\n",
      "Epoch [18/100], Loss: 1.0119\n",
      "Epoch [19/100], Loss: 1.0123\n",
      "Epoch [20/100], Loss: 0.9942\n",
      "Epoch [21/100], Loss: 0.9745\n",
      "Epoch [22/100], Loss: 0.9698\n",
      "Epoch [23/100], Loss: 0.9643\n",
      "Epoch [24/100], Loss: 0.9466\n",
      "Epoch [25/100], Loss: 0.9384\n",
      "Epoch [26/100], Loss: 0.9355\n",
      "Epoch [27/100], Loss: 0.9234\n",
      "Epoch [28/100], Loss: 0.9173\n",
      "Epoch [29/100], Loss: 0.9091\n",
      "Epoch [30/100], Loss: 0.9092\n",
      "Epoch [31/100], Loss: 0.9007\n",
      "Epoch [32/100], Loss: 0.8932\n",
      "Epoch [33/100], Loss: 0.8815\n",
      "Epoch [34/100], Loss: 0.8733\n",
      "Epoch [35/100], Loss: 0.8634\n",
      "Epoch [36/100], Loss: 0.8603\n",
      "Epoch [37/100], Loss: 0.8529\n",
      "Epoch [38/100], Loss: 0.8466\n",
      "Epoch [39/100], Loss: 0.8506\n",
      "Epoch [40/100], Loss: 0.8476\n",
      "Epoch [41/100], Loss: 0.8320\n",
      "Epoch [42/100], Loss: 0.8326\n",
      "Epoch [43/100], Loss: 0.8242\n",
      "Epoch [44/100], Loss: 0.8165\n",
      "Epoch [45/100], Loss: 0.8125\n",
      "Epoch [46/100], Loss: 0.8074\n",
      "Epoch [47/100], Loss: 0.7991\n",
      "Epoch [48/100], Loss: 0.7941\n",
      "Epoch [49/100], Loss: 0.7962\n",
      "Epoch [50/100], Loss: 0.7926\n",
      "Epoch [51/100], Loss: 0.7933\n",
      "Epoch [52/100], Loss: 0.7801\n",
      "Epoch [53/100], Loss: 0.7812\n",
      "Epoch [54/100], Loss: 0.7702\n",
      "Epoch [55/100], Loss: 0.7680\n",
      "Epoch [56/100], Loss: 0.7732\n",
      "Epoch [57/100], Loss: 0.7627\n",
      "Epoch [58/100], Loss: 0.7548\n",
      "Epoch [59/100], Loss: 0.7640\n",
      "Epoch [60/100], Loss: 0.7550\n",
      "Epoch [61/100], Loss: 0.7455\n",
      "Epoch [62/100], Loss: 0.7383\n",
      "Epoch [63/100], Loss: 0.7438\n",
      "Epoch [64/100], Loss: 0.7352\n",
      "Epoch [65/100], Loss: 0.7343\n",
      "Epoch [66/100], Loss: 0.7324\n",
      "Epoch [67/100], Loss: 0.7267\n",
      "Epoch [68/100], Loss: 0.7206\n",
      "Epoch [69/100], Loss: 0.7205\n",
      "Epoch [70/100], Loss: 0.7119\n",
      "Epoch [71/100], Loss: 0.7107\n",
      "Epoch [72/100], Loss: 0.7063\n",
      "Epoch [73/100], Loss: 0.7096\n",
      "Epoch [74/100], Loss: 0.7068\n",
      "Epoch [75/100], Loss: 0.7000\n",
      "Epoch [76/100], Loss: 0.6979\n",
      "Epoch [77/100], Loss: 0.6984\n",
      "Epoch [78/100], Loss: 0.6955\n",
      "Epoch [79/100], Loss: 0.6879\n",
      "Epoch [80/100], Loss: 0.6912\n",
      "Epoch [81/100], Loss: 0.6872\n",
      "Epoch [82/100], Loss: 0.6803\n",
      "Epoch [83/100], Loss: 0.6845\n",
      "Epoch [84/100], Loss: 0.6751\n",
      "Epoch [85/100], Loss: 0.6723\n",
      "Epoch [86/100], Loss: 0.6697\n",
      "Epoch [87/100], Loss: 0.6727\n",
      "Epoch [88/100], Loss: 0.6602\n",
      "Epoch [89/100], Loss: 0.6692\n",
      "Epoch [90/100], Loss: 0.6595\n",
      "Epoch [91/100], Loss: 0.6596\n",
      "Epoch [92/100], Loss: 0.6558\n",
      "Epoch [93/100], Loss: 0.6498\n",
      "Epoch [94/100], Loss: 0.6448\n",
      "Epoch [95/100], Loss: 0.6480\n",
      "Epoch [96/100], Loss: 0.6488\n",
      "Epoch [97/100], Loss: 0.6430\n",
      "Epoch [98/100], Loss: 0.6365\n",
      "Epoch [99/100], Loss: 0.6391\n",
      "Epoch [100/100], Loss: 0.6365\n",
      "Subset 10000, Epoch [100/100], Loss: 0.6365\n",
      "Test Accuracy Logit Lipschitz: 49.74%\n",
      "Epoch [1/100], Loss: 2.4238\n",
      "Epoch [2/100], Loss: 1.6103\n",
      "Epoch [3/100], Loss: 1.4462\n",
      "Epoch [4/100], Loss: 1.3469\n",
      "Epoch [5/100], Loss: 1.3045\n",
      "Epoch [6/100], Loss: 1.2525\n",
      "Epoch [7/100], Loss: 1.2002\n",
      "Epoch [8/100], Loss: 1.1724\n",
      "Epoch [9/100], Loss: 1.1567\n",
      "Epoch [10/100], Loss: 1.1233\n",
      "Epoch [11/100], Loss: 1.1036\n",
      "Epoch [12/100], Loss: 1.0832\n",
      "Epoch [13/100], Loss: 1.0675\n",
      "Epoch [14/100], Loss: 1.0598\n",
      "Epoch [15/100], Loss: 1.0333\n",
      "Epoch [16/100], Loss: 1.0153\n",
      "Epoch [17/100], Loss: 1.0061\n",
      "Epoch [18/100], Loss: 0.9991\n",
      "Epoch [19/100], Loss: 0.9819\n",
      "Epoch [20/100], Loss: 0.9794\n",
      "Epoch [21/100], Loss: 0.9590\n",
      "Epoch [22/100], Loss: 0.9449\n",
      "Epoch [23/100], Loss: 0.9492\n",
      "Epoch [24/100], Loss: 0.9486\n",
      "Epoch [25/100], Loss: 0.9261\n",
      "Epoch [26/100], Loss: 0.9185\n",
      "Epoch [27/100], Loss: 0.9108\n",
      "Epoch [28/100], Loss: 0.8985\n",
      "Epoch [29/100], Loss: 0.8958\n",
      "Epoch [30/100], Loss: 0.8862\n",
      "Epoch [31/100], Loss: 0.8854\n",
      "Epoch [32/100], Loss: 0.8785\n",
      "Epoch [33/100], Loss: 0.8631\n",
      "Epoch [34/100], Loss: 0.8636\n",
      "Epoch [35/100], Loss: 0.8492\n",
      "Epoch [36/100], Loss: 0.8446\n",
      "Epoch [37/100], Loss: 0.8334\n",
      "Epoch [38/100], Loss: 0.8269\n",
      "Epoch [39/100], Loss: 0.8327\n",
      "Epoch [40/100], Loss: 0.8206\n",
      "Epoch [41/100], Loss: 0.8177\n",
      "Epoch [42/100], Loss: 0.8089\n",
      "Epoch [43/100], Loss: 0.8028\n",
      "Epoch [44/100], Loss: 0.7995\n",
      "Epoch [45/100], Loss: 0.7908\n",
      "Epoch [46/100], Loss: 0.7948\n",
      "Epoch [47/100], Loss: 0.7897\n",
      "Epoch [48/100], Loss: 0.7793\n",
      "Epoch [49/100], Loss: 0.7816\n",
      "Epoch [50/100], Loss: 0.7750\n",
      "Epoch [51/100], Loss: 0.7691\n",
      "Epoch [52/100], Loss: 0.7615\n",
      "Epoch [53/100], Loss: 0.7598\n",
      "Epoch [54/100], Loss: 0.7535\n",
      "Epoch [55/100], Loss: 0.7473\n",
      "Epoch [56/100], Loss: 0.7417\n",
      "Epoch [57/100], Loss: 0.7391\n",
      "Epoch [58/100], Loss: 0.7425\n",
      "Epoch [59/100], Loss: 0.7384\n",
      "Epoch [60/100], Loss: 0.7256\n",
      "Epoch [61/100], Loss: 0.7328\n",
      "Epoch [62/100], Loss: 0.7302\n",
      "Epoch [63/100], Loss: 0.7243\n",
      "Epoch [64/100], Loss: 0.7139\n",
      "Epoch [65/100], Loss: 0.7172\n",
      "Epoch [66/100], Loss: 0.7157\n",
      "Epoch [67/100], Loss: 0.7100\n",
      "Epoch [68/100], Loss: 0.7073\n",
      "Epoch [69/100], Loss: 0.6987\n",
      "Epoch [70/100], Loss: 0.7123\n",
      "Epoch [71/100], Loss: 0.6962\n",
      "Epoch [72/100], Loss: 0.6941\n",
      "Epoch [73/100], Loss: 0.6880\n",
      "Epoch [74/100], Loss: 0.6791\n",
      "Epoch [75/100], Loss: 0.6773\n",
      "Epoch [76/100], Loss: 0.6846\n",
      "Epoch [77/100], Loss: 0.6779\n",
      "Epoch [78/100], Loss: 0.6736\n",
      "Epoch [79/100], Loss: 0.6790\n",
      "Epoch [80/100], Loss: 0.6726\n",
      "Epoch [81/100], Loss: 0.6689\n",
      "Epoch [82/100], Loss: 0.6617\n",
      "Epoch [83/100], Loss: 0.6553\n",
      "Epoch [84/100], Loss: 0.6503\n",
      "Epoch [85/100], Loss: 0.6514\n",
      "Epoch [86/100], Loss: 0.6486\n",
      "Epoch [87/100], Loss: 0.6462\n",
      "Epoch [88/100], Loss: 0.6483\n",
      "Epoch [89/100], Loss: 0.6463\n",
      "Epoch [90/100], Loss: 0.6406\n",
      "Epoch [91/100], Loss: 0.6432\n",
      "Epoch [92/100], Loss: 0.6417\n",
      "Epoch [93/100], Loss: 0.6383\n",
      "Epoch [94/100], Loss: 0.6392\n",
      "Epoch [95/100], Loss: 0.6277\n",
      "Epoch [96/100], Loss: 0.6297\n",
      "Epoch [97/100], Loss: 0.6333\n",
      "Epoch [98/100], Loss: 0.6355\n",
      "Stopping early at epoch 98 (Loss improvement < 0.0001 for 3 epochs)\n",
      "Subset 10000, Epoch [98/100], Loss: 0.6355\n",
      "Test Accuracy Logit Lipschitz: 50.05%\n",
      "Epoch [1/100], Loss: 2.4258\n",
      "Epoch [2/100], Loss: 1.5983\n",
      "Epoch [3/100], Loss: 1.4386\n",
      "Epoch [4/100], Loss: 1.3499\n",
      "Epoch [5/100], Loss: 1.3025\n",
      "Epoch [6/100], Loss: 1.2361\n",
      "Epoch [7/100], Loss: 1.1996\n",
      "Epoch [8/100], Loss: 1.1738\n",
      "Epoch [9/100], Loss: 1.1484\n",
      "Epoch [10/100], Loss: 1.1240\n",
      "Epoch [11/100], Loss: 1.1126\n",
      "Epoch [12/100], Loss: 1.0813\n",
      "Epoch [13/100], Loss: 1.0665\n",
      "Epoch [14/100], Loss: 1.0493\n",
      "Epoch [15/100], Loss: 1.0315\n",
      "Epoch [16/100], Loss: 1.0238\n",
      "Epoch [17/100], Loss: 0.9983\n",
      "Epoch [18/100], Loss: 0.9897\n",
      "Epoch [19/100], Loss: 0.9846\n",
      "Epoch [20/100], Loss: 0.9748\n",
      "Epoch [21/100], Loss: 0.9738\n",
      "Epoch [22/100], Loss: 0.9468\n",
      "Epoch [23/100], Loss: 0.9459\n",
      "Epoch [24/100], Loss: 0.9394\n",
      "Epoch [25/100], Loss: 0.9243\n",
      "Epoch [26/100], Loss: 0.9111\n",
      "Epoch [27/100], Loss: 0.9056\n",
      "Epoch [28/100], Loss: 0.8962\n",
      "Epoch [29/100], Loss: 0.8944\n",
      "Epoch [30/100], Loss: 0.8866\n",
      "Epoch [31/100], Loss: 0.8748\n",
      "Epoch [32/100], Loss: 0.8641\n",
      "Epoch [33/100], Loss: 0.8582\n",
      "Epoch [34/100], Loss: 0.8550\n",
      "Epoch [35/100], Loss: 0.8369\n",
      "Epoch [36/100], Loss: 0.8401\n",
      "Epoch [37/100], Loss: 0.8390\n",
      "Epoch [38/100], Loss: 0.8241\n",
      "Epoch [39/100], Loss: 0.8264\n",
      "Epoch [40/100], Loss: 0.8132\n",
      "Epoch [41/100], Loss: 0.8087\n",
      "Epoch [42/100], Loss: 0.8025\n",
      "Epoch [43/100], Loss: 0.8087\n",
      "Epoch [44/100], Loss: 0.7984\n",
      "Epoch [45/100], Loss: 0.7881\n",
      "Epoch [46/100], Loss: 0.7829\n",
      "Epoch [47/100], Loss: 0.7837\n",
      "Epoch [48/100], Loss: 0.7883\n",
      "Epoch [49/100], Loss: 0.7763\n",
      "Epoch [50/100], Loss: 0.7675\n",
      "Epoch [51/100], Loss: 0.7620\n",
      "Epoch [52/100], Loss: 0.7643\n",
      "Epoch [53/100], Loss: 0.7539\n",
      "Epoch [54/100], Loss: 0.7513\n",
      "Epoch [55/100], Loss: 0.7478\n",
      "Epoch [56/100], Loss: 0.7406\n",
      "Epoch [57/100], Loss: 0.7419\n",
      "Epoch [58/100], Loss: 0.7285\n",
      "Epoch [59/100], Loss: 0.7313\n",
      "Epoch [60/100], Loss: 0.7342\n",
      "Epoch [61/100], Loss: 0.7228\n",
      "Epoch [62/100], Loss: 0.7262\n",
      "Epoch [63/100], Loss: 0.7169\n",
      "Epoch [64/100], Loss: 0.7176\n",
      "Epoch [65/100], Loss: 0.7083\n",
      "Epoch [66/100], Loss: 0.7033\n",
      "Epoch [67/100], Loss: 0.6972\n",
      "Epoch [68/100], Loss: 0.6983\n",
      "Epoch [69/100], Loss: 0.6978\n",
      "Epoch [70/100], Loss: 0.6897\n",
      "Epoch [71/100], Loss: 0.6867\n",
      "Epoch [72/100], Loss: 0.6920\n",
      "Epoch [73/100], Loss: 0.6784\n",
      "Epoch [74/100], Loss: 0.6847\n",
      "Epoch [75/100], Loss: 0.6753\n",
      "Epoch [76/100], Loss: 0.6693\n",
      "Epoch [77/100], Loss: 0.6665\n",
      "Epoch [78/100], Loss: 0.6786\n",
      "Epoch [79/100], Loss: 0.6716\n",
      "Epoch [80/100], Loss: 0.6581\n",
      "Epoch [81/100], Loss: 0.6559\n",
      "Epoch [82/100], Loss: 0.6595\n",
      "Epoch [83/100], Loss: 0.6577\n",
      "Epoch [84/100], Loss: 0.6504\n",
      "Epoch [85/100], Loss: 0.6491\n",
      "Epoch [86/100], Loss: 0.6460\n",
      "Epoch [87/100], Loss: 0.6415\n",
      "Epoch [88/100], Loss: 0.6463\n",
      "Epoch [89/100], Loss: 0.6389\n",
      "Epoch [90/100], Loss: 0.6379\n",
      "Epoch [91/100], Loss: 0.6390\n",
      "Epoch [92/100], Loss: 0.6507\n",
      "Epoch [93/100], Loss: 0.6315\n",
      "Epoch [94/100], Loss: 0.6315\n",
      "Epoch [95/100], Loss: 0.6269\n",
      "Epoch [96/100], Loss: 0.6282\n",
      "Epoch [97/100], Loss: 0.6219\n",
      "Epoch [98/100], Loss: 0.6121\n",
      "Epoch [99/100], Loss: 0.6184\n",
      "Epoch [100/100], Loss: 0.6113\n",
      "Subset 10000, Epoch [100/100], Loss: 0.6113\n",
      "Test Accuracy Logit Lipschitz: 49.91%\n",
      "Epoch [1/100], Loss: 2.4259\n",
      "Epoch [2/100], Loss: 1.6068\n",
      "Epoch [3/100], Loss: 1.4436\n",
      "Epoch [4/100], Loss: 1.3466\n",
      "Epoch [5/100], Loss: 1.2814\n",
      "Epoch [6/100], Loss: 1.2353\n",
      "Epoch [7/100], Loss: 1.1969\n",
      "Epoch [8/100], Loss: 1.1655\n",
      "Epoch [9/100], Loss: 1.1341\n",
      "Epoch [10/100], Loss: 1.1112\n",
      "Epoch [11/100], Loss: 1.0887\n",
      "Epoch [12/100], Loss: 1.0712\n",
      "Epoch [13/100], Loss: 1.0622\n",
      "Epoch [14/100], Loss: 1.0504\n",
      "Epoch [15/100], Loss: 1.0303\n",
      "Epoch [16/100], Loss: 1.0091\n",
      "Epoch [17/100], Loss: 0.9951\n",
      "Epoch [18/100], Loss: 0.9808\n",
      "Epoch [19/100], Loss: 0.9828\n",
      "Epoch [20/100], Loss: 0.9677\n",
      "Epoch [21/100], Loss: 0.9589\n",
      "Epoch [22/100], Loss: 0.9473\n",
      "Epoch [23/100], Loss: 0.9263\n",
      "Epoch [24/100], Loss: 0.9125\n",
      "Epoch [25/100], Loss: 0.9083\n",
      "Epoch [26/100], Loss: 0.9023\n",
      "Epoch [27/100], Loss: 0.8990\n",
      "Epoch [28/100], Loss: 0.8881\n",
      "Epoch [29/100], Loss: 0.8831\n",
      "Epoch [30/100], Loss: 0.8734\n",
      "Epoch [31/100], Loss: 0.8664\n",
      "Epoch [32/100], Loss: 0.8627\n",
      "Epoch [33/100], Loss: 0.8609\n",
      "Epoch [34/100], Loss: 0.8502\n",
      "Epoch [35/100], Loss: 0.8441\n",
      "Epoch [36/100], Loss: 0.8280\n",
      "Epoch [37/100], Loss: 0.8285\n",
      "Epoch [38/100], Loss: 0.8220\n",
      "Epoch [39/100], Loss: 0.8152\n",
      "Epoch [40/100], Loss: 0.8112\n",
      "Epoch [41/100], Loss: 0.7994\n",
      "Epoch [42/100], Loss: 0.7977\n",
      "Epoch [43/100], Loss: 0.7925\n",
      "Epoch [44/100], Loss: 0.7891\n",
      "Epoch [45/100], Loss: 0.7813\n",
      "Epoch [46/100], Loss: 0.7896\n",
      "Epoch [47/100], Loss: 0.7769\n",
      "Epoch [48/100], Loss: 0.7662\n",
      "Epoch [49/100], Loss: 0.7678\n",
      "Epoch [50/100], Loss: 0.7639\n",
      "Epoch [51/100], Loss: 0.7544\n",
      "Epoch [52/100], Loss: 0.7520\n",
      "Epoch [53/100], Loss: 0.7528\n",
      "Epoch [54/100], Loss: 0.7425\n",
      "Epoch [55/100], Loss: 0.7379\n",
      "Epoch [56/100], Loss: 0.7368\n",
      "Epoch [57/100], Loss: 0.7380\n",
      "Epoch [58/100], Loss: 0.7300\n",
      "Epoch [59/100], Loss: 0.7179\n",
      "Epoch [60/100], Loss: 0.7145\n",
      "Epoch [61/100], Loss: 0.7146\n",
      "Epoch [62/100], Loss: 0.7147\n",
      "Epoch [63/100], Loss: 0.7108\n",
      "Epoch [64/100], Loss: 0.7136\n",
      "Epoch [65/100], Loss: 0.7047\n",
      "Epoch [66/100], Loss: 0.6954\n",
      "Epoch [67/100], Loss: 0.6970\n",
      "Epoch [68/100], Loss: 0.6926\n",
      "Epoch [69/100], Loss: 0.6878\n",
      "Epoch [70/100], Loss: 0.6865\n",
      "Epoch [71/100], Loss: 0.6836\n",
      "Epoch [72/100], Loss: 0.6841\n",
      "Epoch [73/100], Loss: 0.6819\n",
      "Epoch [74/100], Loss: 0.6836\n",
      "Epoch [75/100], Loss: 0.6734\n",
      "Epoch [76/100], Loss: 0.6680\n",
      "Epoch [77/100], Loss: 0.6756\n",
      "Epoch [78/100], Loss: 0.6598\n",
      "Epoch [79/100], Loss: 0.6582\n",
      "Epoch [80/100], Loss: 0.6565\n",
      "Epoch [81/100], Loss: 0.6570\n",
      "Epoch [82/100], Loss: 0.6515\n",
      "Epoch [83/100], Loss: 0.6496\n",
      "Epoch [84/100], Loss: 0.6511\n",
      "Epoch [85/100], Loss: 0.6433\n",
      "Epoch [86/100], Loss: 0.6391\n",
      "Epoch [87/100], Loss: 0.6403\n",
      "Epoch [88/100], Loss: 0.6369\n",
      "Epoch [89/100], Loss: 0.6321\n",
      "Epoch [90/100], Loss: 0.6323\n",
      "Epoch [91/100], Loss: 0.6299\n",
      "Epoch [92/100], Loss: 0.6313\n",
      "Epoch [93/100], Loss: 0.6300\n",
      "Epoch [94/100], Loss: 0.6233\n",
      "Epoch [95/100], Loss: 0.6244\n",
      "Epoch [96/100], Loss: 0.6184\n",
      "Epoch [97/100], Loss: 0.6127\n",
      "Epoch [98/100], Loss: 0.6117\n",
      "Epoch [99/100], Loss: 0.6150\n",
      "Epoch [100/100], Loss: 0.6145\n",
      "Subset 10000, Epoch [100/100], Loss: 0.6145\n",
      "Test Accuracy Logit Lipschitz: 49.39%\n",
      "Epoch [1/100], Loss: 2.4355\n",
      "Epoch [2/100], Loss: 1.5968\n",
      "Epoch [3/100], Loss: 1.4341\n",
      "Epoch [4/100], Loss: 1.3453\n",
      "Epoch [5/100], Loss: 1.2895\n",
      "Epoch [6/100], Loss: 1.2282\n",
      "Epoch [7/100], Loss: 1.1954\n",
      "Epoch [8/100], Loss: 1.1595\n",
      "Epoch [9/100], Loss: 1.1381\n",
      "Epoch [10/100], Loss: 1.1295\n",
      "Epoch [11/100], Loss: 1.1039\n",
      "Epoch [12/100], Loss: 1.0734\n",
      "Epoch [13/100], Loss: 1.0685\n",
      "Epoch [14/100], Loss: 1.0407\n",
      "Epoch [15/100], Loss: 1.0235\n",
      "Epoch [16/100], Loss: 1.0089\n",
      "Epoch [17/100], Loss: 0.9987\n",
      "Epoch [18/100], Loss: 0.9835\n",
      "Epoch [19/100], Loss: 0.9785\n",
      "Epoch [20/100], Loss: 0.9622\n",
      "Epoch [21/100], Loss: 0.9527\n",
      "Epoch [22/100], Loss: 0.9396\n",
      "Epoch [23/100], Loss: 0.9359\n",
      "Epoch [24/100], Loss: 0.9229\n",
      "Epoch [25/100], Loss: 0.9146\n",
      "Epoch [26/100], Loss: 0.9034\n",
      "Epoch [27/100], Loss: 0.8988\n",
      "Epoch [28/100], Loss: 0.8989\n",
      "Epoch [29/100], Loss: 0.8850\n",
      "Epoch [30/100], Loss: 0.8767\n",
      "Epoch [31/100], Loss: 0.8714\n",
      "Epoch [32/100], Loss: 0.8720\n",
      "Epoch [33/100], Loss: 0.8512\n",
      "Epoch [34/100], Loss: 0.8533\n",
      "Epoch [35/100], Loss: 0.8405\n",
      "Epoch [36/100], Loss: 0.8390\n",
      "Epoch [37/100], Loss: 0.8303\n",
      "Epoch [38/100], Loss: 0.8199\n",
      "Epoch [39/100], Loss: 0.8195\n",
      "Epoch [40/100], Loss: 0.8217\n",
      "Epoch [41/100], Loss: 0.8138\n",
      "Epoch [42/100], Loss: 0.7995\n",
      "Epoch [43/100], Loss: 0.7982\n",
      "Epoch [44/100], Loss: 0.7888\n",
      "Epoch [45/100], Loss: 0.7913\n",
      "Epoch [46/100], Loss: 0.7767\n",
      "Epoch [47/100], Loss: 0.7738\n",
      "Epoch [48/100], Loss: 0.7731\n",
      "Epoch [49/100], Loss: 0.7642\n",
      "Epoch [50/100], Loss: 0.7537\n",
      "Epoch [51/100], Loss: 0.7587\n",
      "Epoch [52/100], Loss: 0.7560\n",
      "Epoch [53/100], Loss: 0.7530\n",
      "Epoch [54/100], Loss: 0.7421\n",
      "Epoch [55/100], Loss: 0.7434\n",
      "Epoch [56/100], Loss: 0.7364\n",
      "Epoch [57/100], Loss: 0.7370\n",
      "Epoch [58/100], Loss: 0.7321\n",
      "Epoch [59/100], Loss: 0.7207\n",
      "Epoch [60/100], Loss: 0.7196\n",
      "Epoch [61/100], Loss: 0.7206\n",
      "Epoch [62/100], Loss: 0.7183\n",
      "Epoch [63/100], Loss: 0.7151\n",
      "Epoch [64/100], Loss: 0.7109\n",
      "Epoch [65/100], Loss: 0.7002\n",
      "Epoch [66/100], Loss: 0.7041\n",
      "Epoch [67/100], Loss: 0.6936\n",
      "Epoch [68/100], Loss: 0.6906\n",
      "Epoch [69/100], Loss: 0.6906\n",
      "Epoch [70/100], Loss: 0.6856\n",
      "Epoch [71/100], Loss: 0.6843\n",
      "Epoch [72/100], Loss: 0.6786\n",
      "Epoch [73/100], Loss: 0.6762\n",
      "Epoch [74/100], Loss: 0.6899\n",
      "Epoch [75/100], Loss: 0.6841\n",
      "Epoch [76/100], Loss: 0.6696\n",
      "Epoch [77/100], Loss: 0.6726\n",
      "Epoch [78/100], Loss: 0.6661\n",
      "Epoch [79/100], Loss: 0.6566\n",
      "Epoch [80/100], Loss: 0.6687\n",
      "Epoch [81/100], Loss: 0.6632\n",
      "Epoch [82/100], Loss: 0.6509\n",
      "Epoch [83/100], Loss: 0.6450\n",
      "Epoch [84/100], Loss: 0.6491\n",
      "Epoch [85/100], Loss: 0.6430\n",
      "Epoch [86/100], Loss: 0.6361\n",
      "Epoch [87/100], Loss: 0.6435\n",
      "Epoch [88/100], Loss: 0.6372\n",
      "Epoch [89/100], Loss: 0.6360\n",
      "Epoch [90/100], Loss: 0.6409\n",
      "Epoch [91/100], Loss: 0.6351\n",
      "Epoch [92/100], Loss: 0.6217\n",
      "Epoch [93/100], Loss: 0.6262\n",
      "Epoch [94/100], Loss: 0.6256\n",
      "Epoch [95/100], Loss: 0.6268\n",
      "Epoch [96/100], Loss: 0.6199\n",
      "Epoch [97/100], Loss: 0.6151\n",
      "Epoch [98/100], Loss: 0.6201\n",
      "Epoch [99/100], Loss: 0.6115\n",
      "Epoch [100/100], Loss: 0.6050\n",
      "Subset 10000, Epoch [100/100], Loss: 0.6050\n",
      "Test Accuracy Logit Lipschitz: 48.90%\n",
      "Epoch [1/100], Loss: 2.4259\n",
      "Epoch [2/100], Loss: 1.5925\n",
      "Epoch [3/100], Loss: 1.4197\n",
      "Epoch [4/100], Loss: 1.3348\n",
      "Epoch [5/100], Loss: 1.2770\n",
      "Epoch [6/100], Loss: 1.2264\n",
      "Epoch [7/100], Loss: 1.1806\n",
      "Epoch [8/100], Loss: 1.1643\n",
      "Epoch [9/100], Loss: 1.1262\n",
      "Epoch [10/100], Loss: 1.1036\n",
      "Epoch [11/100], Loss: 1.0934\n",
      "Epoch [12/100], Loss: 1.0717\n",
      "Epoch [13/100], Loss: 1.0492\n",
      "Epoch [14/100], Loss: 1.0346\n",
      "Epoch [15/100], Loss: 1.0195\n",
      "Epoch [16/100], Loss: 1.0131\n",
      "Epoch [17/100], Loss: 0.9894\n",
      "Epoch [18/100], Loss: 0.9766\n",
      "Epoch [19/100], Loss: 0.9611\n",
      "Epoch [20/100], Loss: 0.9466\n",
      "Epoch [21/100], Loss: 0.9439\n",
      "Epoch [22/100], Loss: 0.9304\n",
      "Epoch [23/100], Loss: 0.9310\n",
      "Epoch [24/100], Loss: 0.9170\n",
      "Epoch [25/100], Loss: 0.9028\n",
      "Epoch [26/100], Loss: 0.8990\n",
      "Epoch [27/100], Loss: 0.8956\n",
      "Epoch [28/100], Loss: 0.8804\n",
      "Epoch [29/100], Loss: 0.8722\n",
      "Epoch [30/100], Loss: 0.8638\n",
      "Epoch [31/100], Loss: 0.8647\n",
      "Epoch [32/100], Loss: 0.8563\n",
      "Epoch [33/100], Loss: 0.8511\n",
      "Epoch [34/100], Loss: 0.8343\n",
      "Epoch [35/100], Loss: 0.8297\n",
      "Epoch [36/100], Loss: 0.8272\n",
      "Epoch [37/100], Loss: 0.8267\n",
      "Epoch [38/100], Loss: 0.8123\n",
      "Epoch [39/100], Loss: 0.8051\n",
      "Epoch [40/100], Loss: 0.8022\n",
      "Epoch [41/100], Loss: 0.7946\n",
      "Epoch [42/100], Loss: 0.7931\n",
      "Epoch [43/100], Loss: 0.7923\n",
      "Epoch [44/100], Loss: 0.7867\n",
      "Epoch [45/100], Loss: 0.7718\n",
      "Epoch [46/100], Loss: 0.7682\n",
      "Epoch [47/100], Loss: 0.7701\n",
      "Epoch [48/100], Loss: 0.7635\n",
      "Epoch [49/100], Loss: 0.7617\n",
      "Epoch [50/100], Loss: 0.7597\n",
      "Epoch [51/100], Loss: 0.7480\n",
      "Epoch [52/100], Loss: 0.7419\n",
      "Epoch [53/100], Loss: 0.7468\n",
      "Epoch [54/100], Loss: 0.7424\n",
      "Epoch [55/100], Loss: 0.7302\n",
      "Epoch [56/100], Loss: 0.7329\n",
      "Epoch [57/100], Loss: 0.7268\n",
      "Epoch [58/100], Loss: 0.7201\n",
      "Epoch [59/100], Loss: 0.7117\n",
      "Epoch [60/100], Loss: 0.7159\n",
      "Epoch [61/100], Loss: 0.7070\n",
      "Epoch [62/100], Loss: 0.7105\n",
      "Epoch [63/100], Loss: 0.7070\n",
      "Epoch [64/100], Loss: 0.6988\n",
      "Epoch [65/100], Loss: 0.6978\n",
      "Epoch [66/100], Loss: 0.6922\n",
      "Epoch [67/100], Loss: 0.6952\n",
      "Epoch [68/100], Loss: 0.6868\n",
      "Epoch [69/100], Loss: 0.6775\n",
      "Epoch [70/100], Loss: 0.6783\n",
      "Epoch [71/100], Loss: 0.6748\n",
      "Epoch [72/100], Loss: 0.6744\n",
      "Epoch [73/100], Loss: 0.6734\n",
      "Epoch [74/100], Loss: 0.6681\n",
      "Epoch [75/100], Loss: 0.6672\n",
      "Epoch [76/100], Loss: 0.6631\n",
      "Epoch [77/100], Loss: 0.6603\n",
      "Epoch [78/100], Loss: 0.6648\n",
      "Epoch [79/100], Loss: 0.6517\n",
      "Epoch [80/100], Loss: 0.6514\n",
      "Epoch [81/100], Loss: 0.6563\n",
      "Epoch [82/100], Loss: 0.6492\n",
      "Epoch [83/100], Loss: 0.6399\n",
      "Epoch [84/100], Loss: 0.6412\n",
      "Epoch [85/100], Loss: 0.6347\n",
      "Epoch [86/100], Loss: 0.6369\n",
      "Epoch [87/100], Loss: 0.6402\n",
      "Epoch [88/100], Loss: 0.6280\n",
      "Epoch [89/100], Loss: 0.6218\n",
      "Epoch [90/100], Loss: 0.6281\n",
      "Epoch [91/100], Loss: 0.6267\n",
      "Epoch [92/100], Loss: 0.6268\n",
      "Epoch [93/100], Loss: 0.6207\n",
      "Epoch [94/100], Loss: 0.6138\n",
      "Epoch [95/100], Loss: 0.6150\n",
      "Epoch [96/100], Loss: 0.6182\n",
      "Epoch [97/100], Loss: 0.6072\n",
      "Epoch [98/100], Loss: 0.6084\n",
      "Epoch [99/100], Loss: 0.6103\n",
      "Epoch [100/100], Loss: 0.5996\n",
      "Subset 10000, Epoch [100/100], Loss: 0.5996\n",
      "Test Accuracy Logit Lipschitz: 49.83%\n",
      "Epoch [1/100], Loss: 2.4220\n",
      "Epoch [2/100], Loss: 1.5982\n",
      "Epoch [3/100], Loss: 1.4281\n",
      "Epoch [4/100], Loss: 1.3365\n",
      "Epoch [5/100], Loss: 1.2729\n",
      "Epoch [6/100], Loss: 1.2224\n",
      "Epoch [7/100], Loss: 1.1897\n",
      "Epoch [8/100], Loss: 1.1627\n",
      "Epoch [9/100], Loss: 1.1295\n",
      "Epoch [10/100], Loss: 1.1044\n",
      "Epoch [11/100], Loss: 1.0744\n",
      "Epoch [12/100], Loss: 1.0588\n",
      "Epoch [13/100], Loss: 1.0485\n",
      "Epoch [14/100], Loss: 1.0322\n",
      "Epoch [15/100], Loss: 1.0130\n",
      "Epoch [16/100], Loss: 1.0036\n",
      "Epoch [17/100], Loss: 0.9875\n",
      "Epoch [18/100], Loss: 0.9759\n",
      "Epoch [19/100], Loss: 0.9600\n",
      "Epoch [20/100], Loss: 0.9531\n",
      "Epoch [21/100], Loss: 0.9440\n",
      "Epoch [22/100], Loss: 0.9305\n",
      "Epoch [23/100], Loss: 0.9326\n",
      "Epoch [24/100], Loss: 0.9167\n",
      "Epoch [25/100], Loss: 0.9105\n",
      "Epoch [26/100], Loss: 0.8995\n",
      "Epoch [27/100], Loss: 0.8903\n",
      "Epoch [28/100], Loss: 0.8739\n",
      "Epoch [29/100], Loss: 0.8714\n",
      "Epoch [30/100], Loss: 0.8701\n",
      "Epoch [31/100], Loss: 0.8619\n",
      "Epoch [32/100], Loss: 0.8560\n",
      "Epoch [33/100], Loss: 0.8456\n",
      "Epoch [34/100], Loss: 0.8375\n",
      "Epoch [35/100], Loss: 0.8280\n",
      "Epoch [36/100], Loss: 0.8304\n",
      "Epoch [37/100], Loss: 0.8263\n",
      "Epoch [38/100], Loss: 0.8146\n",
      "Epoch [39/100], Loss: 0.8156\n",
      "Epoch [40/100], Loss: 0.8042\n",
      "Epoch [41/100], Loss: 0.8024\n",
      "Epoch [42/100], Loss: 0.7901\n",
      "Epoch [43/100], Loss: 0.7925\n",
      "Epoch [44/100], Loss: 0.7875\n",
      "Epoch [45/100], Loss: 0.7721\n",
      "Epoch [46/100], Loss: 0.7725\n",
      "Epoch [47/100], Loss: 0.7641\n",
      "Epoch [48/100], Loss: 0.7638\n",
      "Epoch [49/100], Loss: 0.7645\n",
      "Epoch [50/100], Loss: 0.7554\n",
      "Epoch [51/100], Loss: 0.7443\n",
      "Epoch [52/100], Loss: 0.7495\n",
      "Epoch [53/100], Loss: 0.7408\n",
      "Epoch [54/100], Loss: 0.7331\n",
      "Epoch [55/100], Loss: 0.7328\n",
      "Epoch [56/100], Loss: 0.7266\n",
      "Epoch [57/100], Loss: 0.7200\n",
      "Epoch [58/100], Loss: 0.7196\n",
      "Epoch [59/100], Loss: 0.7152\n",
      "Epoch [60/100], Loss: 0.7124\n",
      "Epoch [61/100], Loss: 0.7095\n",
      "Epoch [62/100], Loss: 0.7085\n",
      "Epoch [63/100], Loss: 0.7167\n",
      "Epoch [64/100], Loss: 0.6973\n",
      "Epoch [65/100], Loss: 0.6966\n",
      "Epoch [66/100], Loss: 0.7020\n",
      "Epoch [67/100], Loss: 0.6885\n",
      "Epoch [68/100], Loss: 0.6873\n",
      "Epoch [69/100], Loss: 0.6874\n",
      "Epoch [70/100], Loss: 0.6832\n",
      "Epoch [71/100], Loss: 0.6821\n",
      "Epoch [72/100], Loss: 0.6812\n",
      "Epoch [73/100], Loss: 0.6722\n",
      "Epoch [74/100], Loss: 0.6784\n",
      "Epoch [75/100], Loss: 0.6613\n",
      "Epoch [76/100], Loss: 0.6652\n",
      "Epoch [77/100], Loss: 0.6573\n",
      "Epoch [78/100], Loss: 0.6566\n",
      "Epoch [79/100], Loss: 0.6489\n",
      "Epoch [80/100], Loss: 0.6509\n",
      "Epoch [81/100], Loss: 0.6557\n",
      "Epoch [82/100], Loss: 0.6595\n",
      "Stopping early at epoch 82 (Loss improvement < 0.0001 for 3 epochs)\n",
      "Subset 10000, Epoch [82/100], Loss: 0.6595\n",
      "Test Accuracy Logit Lipschitz: 50.22%\n",
      "Epoch [1/100], Loss: 2.4224\n",
      "Epoch [2/100], Loss: 1.5913\n",
      "Epoch [3/100], Loss: 1.4376\n",
      "Epoch [4/100], Loss: 1.3482\n",
      "Epoch [5/100], Loss: 1.2806\n",
      "Epoch [6/100], Loss: 1.2382\n",
      "Epoch [7/100], Loss: 1.1957\n",
      "Epoch [8/100], Loss: 1.1691\n",
      "Epoch [9/100], Loss: 1.1356\n",
      "Epoch [10/100], Loss: 1.1063\n",
      "Epoch [11/100], Loss: 1.0919\n",
      "Epoch [12/100], Loss: 1.0733\n",
      "Epoch [13/100], Loss: 1.0538\n",
      "Epoch [14/100], Loss: 1.0351\n",
      "Epoch [15/100], Loss: 1.0141\n",
      "Epoch [16/100], Loss: 1.0050\n",
      "Epoch [17/100], Loss: 0.9926\n",
      "Epoch [18/100], Loss: 0.9825\n",
      "Epoch [19/100], Loss: 0.9737\n",
      "Epoch [20/100], Loss: 0.9590\n",
      "Epoch [21/100], Loss: 0.9450\n",
      "Epoch [22/100], Loss: 0.9395\n",
      "Epoch [23/100], Loss: 0.9267\n",
      "Epoch [24/100], Loss: 0.9180\n",
      "Epoch [25/100], Loss: 0.9068\n",
      "Epoch [26/100], Loss: 0.9044\n",
      "Epoch [27/100], Loss: 0.9046\n",
      "Epoch [28/100], Loss: 0.8883\n",
      "Epoch [29/100], Loss: 0.8839\n",
      "Epoch [30/100], Loss: 0.8855\n",
      "Epoch [31/100], Loss: 0.8669\n",
      "Epoch [32/100], Loss: 0.8523\n",
      "Epoch [33/100], Loss: 0.8444\n",
      "Epoch [34/100], Loss: 0.8457\n",
      "Epoch [35/100], Loss: 0.8422\n",
      "Epoch [36/100], Loss: 0.8332\n",
      "Epoch [37/100], Loss: 0.8314\n",
      "Epoch [38/100], Loss: 0.8238\n",
      "Epoch [39/100], Loss: 0.8172\n",
      "Epoch [40/100], Loss: 0.8107\n",
      "Epoch [41/100], Loss: 0.8022\n",
      "Epoch [42/100], Loss: 0.7975\n",
      "Epoch [43/100], Loss: 0.7884\n",
      "Epoch [44/100], Loss: 0.7825\n",
      "Epoch [45/100], Loss: 0.7875\n",
      "Epoch [46/100], Loss: 0.7701\n",
      "Epoch [47/100], Loss: 0.7661\n",
      "Epoch [48/100], Loss: 0.7636\n",
      "Epoch [49/100], Loss: 0.7542\n",
      "Epoch [50/100], Loss: 0.7588\n",
      "Epoch [51/100], Loss: 0.7597\n",
      "Epoch [52/100], Loss: 0.7587\n",
      "Epoch [53/100], Loss: 0.7460\n",
      "Epoch [54/100], Loss: 0.7468\n",
      "Epoch [55/100], Loss: 0.7461\n",
      "Epoch [56/100], Loss: 0.7337\n",
      "Epoch [57/100], Loss: 0.7282\n",
      "Epoch [58/100], Loss: 0.7217\n",
      "Epoch [59/100], Loss: 0.7296\n",
      "Epoch [60/100], Loss: 0.7094\n",
      "Epoch [61/100], Loss: 0.7194\n",
      "Epoch [62/100], Loss: 0.7106\n",
      "Epoch [63/100], Loss: 0.7028\n",
      "Epoch [64/100], Loss: 0.7002\n",
      "Epoch [65/100], Loss: 0.6994\n",
      "Epoch [66/100], Loss: 0.6955\n",
      "Epoch [67/100], Loss: 0.6953\n",
      "Epoch [68/100], Loss: 0.6818\n",
      "Epoch [69/100], Loss: 0.6883\n",
      "Epoch [70/100], Loss: 0.6839\n",
      "Epoch [71/100], Loss: 0.6771\n",
      "Epoch [72/100], Loss: 0.6773\n",
      "Epoch [73/100], Loss: 0.6752\n",
      "Epoch [74/100], Loss: 0.6744\n",
      "Epoch [75/100], Loss: 0.6685\n",
      "Epoch [76/100], Loss: 0.6634\n",
      "Epoch [77/100], Loss: 0.6613\n",
      "Epoch [78/100], Loss: 0.6653\n",
      "Epoch [79/100], Loss: 0.6562\n",
      "Epoch [80/100], Loss: 0.6559\n",
      "Epoch [81/100], Loss: 0.6448\n",
      "Epoch [82/100], Loss: 0.6402\n",
      "Epoch [83/100], Loss: 0.6431\n",
      "Epoch [84/100], Loss: 0.6420\n",
      "Epoch [85/100], Loss: 0.6386\n",
      "Epoch [86/100], Loss: 0.6391\n",
      "Epoch [87/100], Loss: 0.6334\n",
      "Epoch [88/100], Loss: 0.6261\n",
      "Epoch [89/100], Loss: 0.6246\n",
      "Epoch [90/100], Loss: 0.6308\n",
      "Epoch [91/100], Loss: 0.6254\n",
      "Epoch [92/100], Loss: 0.6216\n",
      "Epoch [93/100], Loss: 0.6200\n",
      "Epoch [94/100], Loss: 0.6160\n",
      "Epoch [95/100], Loss: 0.6135\n",
      "Epoch [96/100], Loss: 0.6122\n",
      "Epoch [97/100], Loss: 0.6135\n",
      "Epoch [98/100], Loss: 0.6128\n",
      "Epoch [99/100], Loss: 0.6093\n",
      "Epoch [100/100], Loss: 0.5961\n",
      "Subset 10000, Epoch [100/100], Loss: 0.5961\n",
      "Test Accuracy Logit Lipschitz: 49.64%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-4  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "\n",
    "logit_accuracy_lipschitz = {}\n",
    "\n",
    "for subset_size in num_subsets:\n",
    "    accuracy_list = []\n",
    "    print(f\"Training with subset size: {subset_size}\")\n",
    "\n",
    "    for _ in range(10):\n",
    "        logit_model_lipschitz = LogisticRegression(input_dim, num_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_sdg = optim.SGD(logit_model_lipschitz.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        subset_indices = torch.randperm(train_data.shape[0])[:subset_size]\n",
    "        subset = SubsetRandomSampler(subset_indices)\n",
    "        train_loader = DataLoader(train_set, batch_size=128, sampler=subset)\n",
    "\n",
    "        previous_loss = float('inf')  # Track the best loss\n",
    "        epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            logit_model_lipschitz.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = logit_model_lipschitz(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer_sdg.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "\n",
    "                L = max(param.grad.abs().max().item() for param in logit_model_lipschitz.parameters() if param.grad is not None)\n",
    "                adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "                # Update optimizer's learning rate\n",
    "                for param_group in optimizer_sdg.param_groups:\n",
    "                    param_group['lr'] = adaptive_lr\n",
    "                optimizer_sdg.step()\n",
    "\n",
    "                total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "            current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "            # Check for early stopping\n",
    "            if previous_loss - current_loss < tolerance:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "                    break\n",
    "            else:\n",
    "                epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "            previous_loss = current_loss\n",
    "\n",
    "        print(f\"Subset {subset_size}, Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        logit_model_lipschitz.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                images = images.view(-1, 28*28)\n",
    "                outputs = logit_model_lipschitz(images)\n",
    "                _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Test Accuracy Logit Lipschitz: {accuracy:.2f}%\")\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    logit_accuracy_lipschitz[subset_size] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125000    55.963110\n",
       " 75000     54.911407\n",
       " 50000     54.174385\n",
       " 10000     49.638623\n",
       " dtype: float64,\n",
       " 125000    0.317441\n",
       " 75000     0.447245\n",
       " 50000     0.312347\n",
       " 10000     0.396920\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_accuracy_lipschitz_df = pd.DataFrame(logit_accuracy_lipschitz)\n",
    "logit_accuracy_lipschitz_df.mean(), logit_accuracy_lipschitz_df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with subset size: 125000\n",
      "Epoch [1/75], Loss: 1.0439\n",
      "Epoch [1/75], Val Loss: 0.4348\n",
      "Epoch [2/75], Loss: 0.3642\n",
      "Epoch [2/75], Val Loss: 0.2918\n",
      "Epoch [3/75], Loss: 0.2504\n",
      "Epoch [3/75], Val Loss: 0.2320\n",
      "Epoch [4/75], Loss: 0.1928\n",
      "Epoch [4/75], Val Loss: 0.1865\n",
      "Epoch [5/75], Loss: 0.1554\n",
      "Epoch [5/75], Val Loss: 0.1714\n",
      "Epoch [6/75], Loss: 0.1293\n",
      "Epoch [6/75], Val Loss: 0.1599\n",
      "Epoch [7/75], Loss: 0.1070\n",
      "Epoch [7/75], Val Loss: 0.1416\n",
      "Epoch [8/75], Loss: 0.0906\n",
      "Epoch [8/75], Val Loss: 0.1400\n",
      "Epoch [9/75], Loss: 0.0763\n",
      "Epoch [9/75], Val Loss: 0.1319\n",
      "Epoch [10/75], Loss: 0.0639\n",
      "Epoch [10/75], Val Loss: 0.1217\n",
      "Epoch [11/75], Loss: 0.0549\n",
      "Epoch [11/75], Val Loss: 0.1207\n",
      "Epoch [12/75], Loss: 0.0454\n",
      "Epoch [12/75], Val Loss: 0.1153\n",
      "Epoch [13/75], Loss: 0.0382\n",
      "Epoch [13/75], Val Loss: 0.1196\n",
      "Epoch [14/75], Loss: 0.0328\n",
      "Epoch [14/75], Val Loss: 0.1130\n",
      "Epoch [15/75], Loss: 0.0279\n",
      "Epoch [15/75], Val Loss: 0.1141\n",
      "Epoch [16/75], Loss: 0.0241\n",
      "Epoch [16/75], Val Loss: 0.1160\n",
      "Epoch [17/75], Loss: 0.0203\n",
      "Epoch [17/75], Val Loss: 0.1161\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 125000, Epoch [17/75], Loss: 0.0203\n",
      "Test Accuracy Base CNN: 90.43%\n",
      "Epoch [1/75], Loss: 1.0362\n",
      "Epoch [1/75], Val Loss: 0.4442\n",
      "Epoch [2/75], Loss: 0.3584\n",
      "Epoch [2/75], Val Loss: 0.2906\n",
      "Epoch [3/75], Loss: 0.2478\n",
      "Epoch [3/75], Val Loss: 0.2229\n",
      "Epoch [4/75], Loss: 0.1900\n",
      "Epoch [4/75], Val Loss: 0.1830\n",
      "Epoch [5/75], Loss: 0.1531\n",
      "Epoch [5/75], Val Loss: 0.1666\n",
      "Epoch [6/75], Loss: 0.1271\n",
      "Epoch [6/75], Val Loss: 0.1586\n",
      "Epoch [7/75], Loss: 0.1065\n",
      "Epoch [7/75], Val Loss: 0.1399\n",
      "Epoch [8/75], Loss: 0.0892\n",
      "Epoch [8/75], Val Loss: 0.1397\n",
      "Epoch [9/75], Loss: 0.0755\n",
      "Epoch [9/75], Val Loss: 0.1347\n",
      "Epoch [10/75], Loss: 0.0638\n",
      "Epoch [10/75], Val Loss: 0.1284\n",
      "Epoch [11/75], Loss: 0.0543\n",
      "Epoch [11/75], Val Loss: 0.1295\n",
      "Epoch [12/75], Loss: 0.0458\n",
      "Epoch [12/75], Val Loss: 0.1270\n",
      "Epoch [13/75], Loss: 0.0390\n",
      "Epoch [13/75], Val Loss: 0.1221\n",
      "Epoch [14/75], Loss: 0.0330\n",
      "Epoch [14/75], Val Loss: 0.1227\n",
      "Epoch [15/75], Loss: 0.0285\n",
      "Epoch [15/75], Val Loss: 0.1211\n",
      "Epoch [16/75], Loss: 0.0243\n",
      "Epoch [16/75], Val Loss: 0.1251\n",
      "Epoch [17/75], Loss: 0.0213\n",
      "Epoch [17/75], Val Loss: 0.1204\n",
      "Epoch [18/75], Loss: 0.0184\n",
      "Epoch [18/75], Val Loss: 0.1262\n",
      "Epoch [19/75], Loss: 0.0164\n",
      "Epoch [19/75], Val Loss: 0.1229\n",
      "Epoch [20/75], Loss: 0.0145\n",
      "Epoch [20/75], Val Loss: 0.1232\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 125000, Epoch [20/75], Loss: 0.0145\n",
      "Test Accuracy Base CNN: 90.57%\n",
      "Epoch [1/75], Loss: 1.0467\n",
      "Epoch [1/75], Val Loss: 0.4516\n",
      "Epoch [2/75], Loss: 0.3681\n",
      "Epoch [2/75], Val Loss: 0.3004\n",
      "Epoch [3/75], Loss: 0.2543\n",
      "Epoch [3/75], Val Loss: 0.2320\n",
      "Epoch [4/75], Loss: 0.1985\n",
      "Epoch [4/75], Val Loss: 0.1993\n",
      "Epoch [5/75], Loss: 0.1592\n",
      "Epoch [5/75], Val Loss: 0.1750\n",
      "Epoch [6/75], Loss: 0.1326\n",
      "Epoch [6/75], Val Loss: 0.1621\n",
      "Epoch [7/75], Loss: 0.1110\n",
      "Epoch [7/75], Val Loss: 0.1482\n",
      "Epoch [8/75], Loss: 0.0922\n",
      "Epoch [8/75], Val Loss: 0.1497\n",
      "Epoch [9/75], Loss: 0.0790\n",
      "Epoch [9/75], Val Loss: 0.1358\n",
      "Epoch [10/75], Loss: 0.0667\n",
      "Epoch [10/75], Val Loss: 0.1346\n",
      "Epoch [11/75], Loss: 0.0555\n",
      "Epoch [11/75], Val Loss: 0.1351\n",
      "Epoch [12/75], Loss: 0.0469\n",
      "Epoch [12/75], Val Loss: 0.1290\n",
      "Epoch [13/75], Loss: 0.0392\n",
      "Epoch [13/75], Val Loss: 0.1239\n",
      "Epoch [14/75], Loss: 0.0345\n",
      "Epoch [14/75], Val Loss: 0.1287\n",
      "Epoch [15/75], Loss: 0.0286\n",
      "Epoch [15/75], Val Loss: 0.1229\n",
      "Epoch [16/75], Loss: 0.0241\n",
      "Epoch [16/75], Val Loss: 0.1271\n",
      "Epoch [17/75], Loss: 0.0202\n",
      "Epoch [17/75], Val Loss: 0.1243\n",
      "Epoch [18/75], Loss: 0.0171\n",
      "Epoch [18/75], Val Loss: 0.1247\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 125000, Epoch [18/75], Loss: 0.0171\n",
      "Test Accuracy Base CNN: 90.78%\n",
      "Epoch [1/75], Loss: 1.0240\n",
      "Epoch [1/75], Val Loss: 0.4403\n",
      "Epoch [2/75], Loss: 0.3541\n",
      "Epoch [2/75], Val Loss: 0.2911\n",
      "Epoch [3/75], Loss: 0.2446\n",
      "Epoch [3/75], Val Loss: 0.2306\n",
      "Epoch [4/75], Loss: 0.1887\n",
      "Epoch [4/75], Val Loss: 0.1896\n",
      "Epoch [5/75], Loss: 0.1517\n",
      "Epoch [5/75], Val Loss: 0.1689\n",
      "Epoch [6/75], Loss: 0.1265\n",
      "Epoch [6/75], Val Loss: 0.1538\n",
      "Epoch [7/75], Loss: 0.1057\n",
      "Epoch [7/75], Val Loss: 0.1458\n",
      "Epoch [8/75], Loss: 0.0890\n",
      "Epoch [8/75], Val Loss: 0.1378\n",
      "Epoch [9/75], Loss: 0.0748\n",
      "Epoch [9/75], Val Loss: 0.1293\n",
      "Epoch [10/75], Loss: 0.0626\n",
      "Epoch [10/75], Val Loss: 0.1289\n",
      "Epoch [11/75], Loss: 0.0537\n",
      "Epoch [11/75], Val Loss: 0.1264\n",
      "Epoch [12/75], Loss: 0.0453\n",
      "Epoch [12/75], Val Loss: 0.1309\n",
      "Epoch [13/75], Loss: 0.0384\n",
      "Epoch [13/75], Val Loss: 0.1235\n",
      "Epoch [14/75], Loss: 0.0332\n",
      "Epoch [14/75], Val Loss: 0.1183\n",
      "Epoch [15/75], Loss: 0.0282\n",
      "Epoch [15/75], Val Loss: 0.1182\n",
      "Epoch [16/75], Loss: 0.0240\n",
      "Epoch [16/75], Val Loss: 0.1199\n",
      "Epoch [17/75], Loss: 0.0210\n",
      "Epoch [17/75], Val Loss: 0.1186\n",
      "Epoch [18/75], Loss: 0.0185\n",
      "Epoch [18/75], Val Loss: 0.1238\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 125000, Epoch [18/75], Loss: 0.0185\n",
      "Test Accuracy Base CNN: 90.56%\n",
      "Epoch [1/75], Loss: 1.0493\n",
      "Epoch [1/75], Val Loss: 0.4579\n",
      "Epoch [2/75], Loss: 0.3598\n",
      "Epoch [2/75], Val Loss: 0.2855\n",
      "Epoch [3/75], Loss: 0.2497\n",
      "Epoch [3/75], Val Loss: 0.2287\n",
      "Epoch [4/75], Loss: 0.1929\n",
      "Epoch [4/75], Val Loss: 0.1921\n",
      "Epoch [5/75], Loss: 0.1564\n",
      "Epoch [5/75], Val Loss: 0.1669\n",
      "Epoch [6/75], Loss: 0.1297\n",
      "Epoch [6/75], Val Loss: 0.1561\n",
      "Epoch [7/75], Loss: 0.1091\n",
      "Epoch [7/75], Val Loss: 0.1371\n",
      "Epoch [8/75], Loss: 0.0919\n",
      "Epoch [8/75], Val Loss: 0.1253\n",
      "Epoch [9/75], Loss: 0.0779\n",
      "Epoch [9/75], Val Loss: 0.1203\n",
      "Epoch [10/75], Loss: 0.0656\n",
      "Epoch [10/75], Val Loss: 0.1197\n",
      "Epoch [11/75], Loss: 0.0561\n",
      "Epoch [11/75], Val Loss: 0.1138\n",
      "Epoch [12/75], Loss: 0.0483\n",
      "Epoch [12/75], Val Loss: 0.1199\n",
      "Epoch [13/75], Loss: 0.0409\n",
      "Epoch [13/75], Val Loss: 0.1152\n",
      "Epoch [14/75], Loss: 0.0340\n",
      "Epoch [14/75], Val Loss: 0.1131\n",
      "Epoch [15/75], Loss: 0.0288\n",
      "Epoch [15/75], Val Loss: 0.1113\n",
      "Epoch [16/75], Loss: 0.0251\n",
      "Epoch [16/75], Val Loss: 0.1132\n",
      "Epoch [17/75], Loss: 0.0212\n",
      "Epoch [17/75], Val Loss: 0.1223\n",
      "Epoch [18/75], Loss: 0.0184\n",
      "Epoch [18/75], Val Loss: 0.1110\n",
      "Epoch [19/75], Loss: 0.0158\n",
      "Epoch [19/75], Val Loss: 0.1123\n",
      "Epoch [20/75], Loss: 0.0146\n",
      "Epoch [20/75], Val Loss: 0.1108\n",
      "Epoch [21/75], Loss: 0.0125\n",
      "Epoch [21/75], Val Loss: 0.1095\n",
      "Epoch [22/75], Loss: 0.0116\n",
      "Epoch [22/75], Val Loss: 0.1129\n",
      "Epoch [23/75], Loss: 0.0106\n",
      "Epoch [23/75], Val Loss: 0.1132\n",
      "Epoch [24/75], Loss: 0.0092\n",
      "Epoch [24/75], Val Loss: 0.1207\n",
      "Stopping early at epoch 24 (No improvement in 3 epochs)\n",
      "Subset 125000, Epoch [24/75], Loss: 0.0092\n",
      "Test Accuracy Base CNN: 90.81%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/75], Loss: 1.4472\n",
      "Epoch [1/75], Val Loss: 0.6581\n",
      "Epoch [2/75], Loss: 0.5231\n",
      "Epoch [2/75], Val Loss: 0.4226\n",
      "Epoch [3/75], Loss: 0.3557\n",
      "Epoch [3/75], Val Loss: 0.3150\n",
      "Epoch [4/75], Loss: 0.2677\n",
      "Epoch [4/75], Val Loss: 0.2715\n",
      "Epoch [5/75], Loss: 0.2160\n",
      "Epoch [5/75], Val Loss: 0.2440\n",
      "Epoch [6/75], Loss: 0.1741\n",
      "Epoch [6/75], Val Loss: 0.2261\n",
      "Epoch [7/75], Loss: 0.1452\n",
      "Epoch [7/75], Val Loss: 0.2076\n",
      "Epoch [8/75], Loss: 0.1207\n",
      "Epoch [8/75], Val Loss: 0.2073\n",
      "Epoch [9/75], Loss: 0.0988\n",
      "Epoch [9/75], Val Loss: 0.1939\n",
      "Epoch [10/75], Loss: 0.0803\n",
      "Epoch [10/75], Val Loss: 0.1826\n",
      "Epoch [11/75], Loss: 0.0680\n",
      "Epoch [11/75], Val Loss: 0.1875\n",
      "Epoch [12/75], Loss: 0.0554\n",
      "Epoch [12/75], Val Loss: 0.1840\n",
      "Epoch [13/75], Loss: 0.0464\n",
      "Epoch [13/75], Val Loss: 0.1957\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [13/75], Loss: 0.0464\n",
      "Test Accuracy Base CNN: 88.39%\n",
      "Epoch [1/75], Loss: 1.4211\n",
      "Epoch [1/75], Val Loss: 0.6651\n",
      "Epoch [2/75], Loss: 0.5142\n",
      "Epoch [2/75], Val Loss: 0.4186\n",
      "Epoch [3/75], Loss: 0.3448\n",
      "Epoch [3/75], Val Loss: 0.3254\n",
      "Epoch [4/75], Loss: 0.2599\n",
      "Epoch [4/75], Val Loss: 0.2678\n",
      "Epoch [5/75], Loss: 0.2076\n",
      "Epoch [5/75], Val Loss: 0.2420\n",
      "Epoch [6/75], Loss: 0.1686\n",
      "Epoch [6/75], Val Loss: 0.2157\n",
      "Epoch [7/75], Loss: 0.1381\n",
      "Epoch [7/75], Val Loss: 0.2237\n",
      "Epoch [8/75], Loss: 0.1146\n",
      "Epoch [8/75], Val Loss: 0.1996\n",
      "Epoch [9/75], Loss: 0.0939\n",
      "Epoch [9/75], Val Loss: 0.2010\n",
      "Epoch [10/75], Loss: 0.0780\n",
      "Epoch [10/75], Val Loss: 0.2004\n",
      "Epoch [11/75], Loss: 0.0657\n",
      "Epoch [11/75], Val Loss: 0.1951\n",
      "Epoch [12/75], Loss: 0.0529\n",
      "Epoch [12/75], Val Loss: 0.1948\n",
      "Epoch [13/75], Loss: 0.0439\n",
      "Epoch [13/75], Val Loss: 0.1944\n",
      "Epoch [14/75], Loss: 0.0366\n",
      "Epoch [14/75], Val Loss: 0.1907\n",
      "Epoch [15/75], Loss: 0.0304\n",
      "Epoch [15/75], Val Loss: 0.2031\n",
      "Epoch [16/75], Loss: 0.0258\n",
      "Epoch [16/75], Val Loss: 0.1952\n",
      "Epoch [17/75], Loss: 0.0215\n",
      "Epoch [17/75], Val Loss: 0.1939\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [17/75], Loss: 0.0215\n",
      "Test Accuracy Base CNN: 88.65%\n",
      "Epoch [1/75], Loss: 1.4401\n",
      "Epoch [1/75], Val Loss: 0.6740\n",
      "Epoch [2/75], Loss: 0.5101\n",
      "Epoch [2/75], Val Loss: 0.4134\n",
      "Epoch [3/75], Loss: 0.3441\n",
      "Epoch [3/75], Val Loss: 0.3245\n",
      "Epoch [4/75], Loss: 0.2613\n",
      "Epoch [4/75], Val Loss: 0.2719\n",
      "Epoch [5/75], Loss: 0.2107\n",
      "Epoch [5/75], Val Loss: 0.2555\n",
      "Epoch [6/75], Loss: 0.1727\n",
      "Epoch [6/75], Val Loss: 0.2260\n",
      "Epoch [7/75], Loss: 0.1425\n",
      "Epoch [7/75], Val Loss: 0.2152\n",
      "Epoch [8/75], Loss: 0.1199\n",
      "Epoch [8/75], Val Loss: 0.2075\n",
      "Epoch [9/75], Loss: 0.1001\n",
      "Epoch [9/75], Val Loss: 0.2058\n",
      "Epoch [10/75], Loss: 0.0819\n",
      "Epoch [10/75], Val Loss: 0.1971\n",
      "Epoch [11/75], Loss: 0.0708\n",
      "Epoch [11/75], Val Loss: 0.1970\n",
      "Epoch [12/75], Loss: 0.0563\n",
      "Epoch [12/75], Val Loss: 0.1858\n",
      "Epoch [13/75], Loss: 0.0460\n",
      "Epoch [13/75], Val Loss: 0.1953\n",
      "Epoch [14/75], Loss: 0.0403\n",
      "Epoch [14/75], Val Loss: 0.1992\n",
      "Epoch [15/75], Loss: 0.0344\n",
      "Epoch [15/75], Val Loss: 0.1937\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [15/75], Loss: 0.0344\n",
      "Test Accuracy Base CNN: 88.58%\n",
      "Epoch [1/75], Loss: 1.4226\n",
      "Epoch [1/75], Val Loss: 0.6691\n",
      "Epoch [2/75], Loss: 0.5273\n",
      "Epoch [2/75], Val Loss: 0.4271\n",
      "Epoch [3/75], Loss: 0.3492\n",
      "Epoch [3/75], Val Loss: 0.3249\n",
      "Epoch [4/75], Loss: 0.2644\n",
      "Epoch [4/75], Val Loss: 0.2669\n",
      "Epoch [5/75], Loss: 0.2076\n",
      "Epoch [5/75], Val Loss: 0.2433\n",
      "Epoch [6/75], Loss: 0.1694\n",
      "Epoch [6/75], Val Loss: 0.2219\n",
      "Epoch [7/75], Loss: 0.1382\n",
      "Epoch [7/75], Val Loss: 0.2065\n",
      "Epoch [8/75], Loss: 0.1146\n",
      "Epoch [8/75], Val Loss: 0.1964\n",
      "Epoch [9/75], Loss: 0.0953\n",
      "Epoch [9/75], Val Loss: 0.2001\n",
      "Epoch [10/75], Loss: 0.0788\n",
      "Epoch [10/75], Val Loss: 0.1979\n",
      "Epoch [11/75], Loss: 0.0640\n",
      "Epoch [11/75], Val Loss: 0.1941\n",
      "Epoch [12/75], Loss: 0.0538\n",
      "Epoch [12/75], Val Loss: 0.1893\n",
      "Epoch [13/75], Loss: 0.0424\n",
      "Epoch [13/75], Val Loss: 0.1919\n",
      "Epoch [14/75], Loss: 0.0370\n",
      "Epoch [14/75], Val Loss: 0.1875\n",
      "Epoch [15/75], Loss: 0.0307\n",
      "Epoch [15/75], Val Loss: 0.1891\n",
      "Epoch [16/75], Loss: 0.0262\n",
      "Epoch [16/75], Val Loss: 0.1954\n",
      "Epoch [17/75], Loss: 0.0212\n",
      "Epoch [17/75], Val Loss: 0.1946\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [17/75], Loss: 0.0212\n",
      "Test Accuracy Base CNN: 88.66%\n",
      "Epoch [1/75], Loss: 1.3843\n",
      "Epoch [1/75], Val Loss: 0.6304\n",
      "Epoch [2/75], Loss: 0.4972\n",
      "Epoch [2/75], Val Loss: 0.3951\n",
      "Epoch [3/75], Loss: 0.3360\n",
      "Epoch [3/75], Val Loss: 0.3043\n",
      "Epoch [4/75], Loss: 0.2565\n",
      "Epoch [4/75], Val Loss: 0.2707\n",
      "Epoch [5/75], Loss: 0.2048\n",
      "Epoch [5/75], Val Loss: 0.2369\n",
      "Epoch [6/75], Loss: 0.1645\n",
      "Epoch [6/75], Val Loss: 0.2211\n",
      "Epoch [7/75], Loss: 0.1361\n",
      "Epoch [7/75], Val Loss: 0.2036\n",
      "Epoch [8/75], Loss: 0.1112\n",
      "Epoch [8/75], Val Loss: 0.1958\n",
      "Epoch [9/75], Loss: 0.0929\n",
      "Epoch [9/75], Val Loss: 0.1888\n",
      "Epoch [10/75], Loss: 0.0773\n",
      "Epoch [10/75], Val Loss: 0.1874\n",
      "Epoch [11/75], Loss: 0.0634\n",
      "Epoch [11/75], Val Loss: 0.1953\n",
      "Epoch [12/75], Loss: 0.0532\n",
      "Epoch [12/75], Val Loss: 0.1956\n",
      "Epoch [13/75], Loss: 0.0423\n",
      "Epoch [13/75], Val Loss: 0.1955\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [13/75], Loss: 0.0423\n",
      "Test Accuracy Base CNN: 88.45%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/75], Loss: 1.6755\n",
      "Epoch [1/75], Val Loss: 0.8619\n",
      "Epoch [2/75], Loss: 0.6507\n",
      "Epoch [2/75], Val Loss: 0.5342\n",
      "Epoch [3/75], Loss: 0.4348\n",
      "Epoch [3/75], Val Loss: 0.4088\n",
      "Epoch [4/75], Loss: 0.3268\n",
      "Epoch [4/75], Val Loss: 0.3542\n",
      "Epoch [5/75], Loss: 0.2597\n",
      "Epoch [5/75], Val Loss: 0.3162\n",
      "Epoch [6/75], Loss: 0.2093\n",
      "Epoch [6/75], Val Loss: 0.2904\n",
      "Epoch [7/75], Loss: 0.1718\n",
      "Epoch [7/75], Val Loss: 0.2797\n",
      "Epoch [8/75], Loss: 0.1428\n",
      "Epoch [8/75], Val Loss: 0.2590\n",
      "Epoch [9/75], Loss: 0.1171\n",
      "Epoch [9/75], Val Loss: 0.2498\n",
      "Epoch [10/75], Loss: 0.0979\n",
      "Epoch [10/75], Val Loss: 0.2583\n",
      "Epoch [11/75], Loss: 0.0777\n",
      "Epoch [11/75], Val Loss: 0.2588\n",
      "Epoch [12/75], Loss: 0.0649\n",
      "Epoch [12/75], Val Loss: 0.2580\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [12/75], Loss: 0.0649\n",
      "Test Accuracy Base CNN: 86.22%\n",
      "Epoch [1/75], Loss: 1.7224\n",
      "Epoch [1/75], Val Loss: 0.8908\n",
      "Epoch [2/75], Loss: 0.6802\n",
      "Epoch [2/75], Val Loss: 0.5508\n",
      "Epoch [3/75], Loss: 0.4547\n",
      "Epoch [3/75], Val Loss: 0.4340\n",
      "Epoch [4/75], Loss: 0.3409\n",
      "Epoch [4/75], Val Loss: 0.3582\n",
      "Epoch [5/75], Loss: 0.2679\n",
      "Epoch [5/75], Val Loss: 0.3291\n",
      "Epoch [6/75], Loss: 0.2154\n",
      "Epoch [6/75], Val Loss: 0.3075\n",
      "Epoch [7/75], Loss: 0.1755\n",
      "Epoch [7/75], Val Loss: 0.2892\n",
      "Epoch [8/75], Loss: 0.1446\n",
      "Epoch [8/75], Val Loss: 0.2697\n",
      "Epoch [9/75], Loss: 0.1179\n",
      "Epoch [9/75], Val Loss: 0.2656\n",
      "Epoch [10/75], Loss: 0.0971\n",
      "Epoch [10/75], Val Loss: 0.2644\n",
      "Epoch [11/75], Loss: 0.0780\n",
      "Epoch [11/75], Val Loss: 0.2593\n",
      "Epoch [12/75], Loss: 0.0632\n",
      "Epoch [12/75], Val Loss: 0.2679\n",
      "Epoch [13/75], Loss: 0.0526\n",
      "Epoch [13/75], Val Loss: 0.2762\n",
      "Epoch [14/75], Loss: 0.0422\n",
      "Epoch [14/75], Val Loss: 0.2688\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [14/75], Loss: 0.0422\n",
      "Test Accuracy Base CNN: 86.16%\n",
      "Epoch [1/75], Loss: 1.6629\n",
      "Epoch [1/75], Val Loss: 0.8643\n",
      "Epoch [2/75], Loss: 0.6838\n",
      "Epoch [2/75], Val Loss: 0.5427\n",
      "Epoch [3/75], Loss: 0.4534\n",
      "Epoch [3/75], Val Loss: 0.4280\n",
      "Epoch [4/75], Loss: 0.3358\n",
      "Epoch [4/75], Val Loss: 0.3683\n",
      "Epoch [5/75], Loss: 0.2644\n",
      "Epoch [5/75], Val Loss: 0.3065\n",
      "Epoch [6/75], Loss: 0.2116\n",
      "Epoch [6/75], Val Loss: 0.2837\n",
      "Epoch [7/75], Loss: 0.1760\n",
      "Epoch [7/75], Val Loss: 0.2669\n",
      "Epoch [8/75], Loss: 0.1432\n",
      "Epoch [8/75], Val Loss: 0.2562\n",
      "Epoch [9/75], Loss: 0.1200\n",
      "Epoch [9/75], Val Loss: 0.2621\n",
      "Epoch [10/75], Loss: 0.0972\n",
      "Epoch [10/75], Val Loss: 0.2509\n",
      "Epoch [11/75], Loss: 0.0798\n",
      "Epoch [11/75], Val Loss: 0.2467\n",
      "Epoch [12/75], Loss: 0.0663\n",
      "Epoch [12/75], Val Loss: 0.2504\n",
      "Epoch [13/75], Loss: 0.0547\n",
      "Epoch [13/75], Val Loss: 0.2628\n",
      "Epoch [14/75], Loss: 0.0450\n",
      "Epoch [14/75], Val Loss: 0.2536\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [14/75], Loss: 0.0450\n",
      "Test Accuracy Base CNN: 86.39%\n",
      "Epoch [1/75], Loss: 1.6599\n",
      "Epoch [1/75], Val Loss: 0.8805\n",
      "Epoch [2/75], Loss: 0.6805\n",
      "Epoch [2/75], Val Loss: 0.5555\n",
      "Epoch [3/75], Loss: 0.4540\n",
      "Epoch [3/75], Val Loss: 0.4196\n",
      "Epoch [4/75], Loss: 0.3409\n",
      "Epoch [4/75], Val Loss: 0.3454\n",
      "Epoch [5/75], Loss: 0.2683\n",
      "Epoch [5/75], Val Loss: 0.3118\n",
      "Epoch [6/75], Loss: 0.2188\n",
      "Epoch [6/75], Val Loss: 0.2991\n",
      "Epoch [7/75], Loss: 0.1817\n",
      "Epoch [7/75], Val Loss: 0.2737\n",
      "Epoch [8/75], Loss: 0.1490\n",
      "Epoch [8/75], Val Loss: 0.2616\n",
      "Epoch [9/75], Loss: 0.1215\n",
      "Epoch [9/75], Val Loss: 0.2532\n",
      "Epoch [10/75], Loss: 0.1020\n",
      "Epoch [10/75], Val Loss: 0.2551\n",
      "Epoch [11/75], Loss: 0.0827\n",
      "Epoch [11/75], Val Loss: 0.2471\n",
      "Epoch [12/75], Loss: 0.0663\n",
      "Epoch [12/75], Val Loss: 0.2487\n",
      "Epoch [13/75], Loss: 0.0579\n",
      "Epoch [13/75], Val Loss: 0.2567\n",
      "Epoch [14/75], Loss: 0.0464\n",
      "Epoch [14/75], Val Loss: 0.2567\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [14/75], Loss: 0.0464\n",
      "Test Accuracy Base CNN: 86.90%\n",
      "Epoch [1/75], Loss: 1.6176\n",
      "Epoch [1/75], Val Loss: 0.8385\n",
      "Epoch [2/75], Loss: 0.6342\n",
      "Epoch [2/75], Val Loss: 0.5383\n",
      "Epoch [3/75], Loss: 0.4206\n",
      "Epoch [3/75], Val Loss: 0.4008\n",
      "Epoch [4/75], Loss: 0.3184\n",
      "Epoch [4/75], Val Loss: 0.3644\n",
      "Epoch [5/75], Loss: 0.2525\n",
      "Epoch [5/75], Val Loss: 0.3125\n",
      "Epoch [6/75], Loss: 0.2054\n",
      "Epoch [6/75], Val Loss: 0.2830\n",
      "Epoch [7/75], Loss: 0.1678\n",
      "Epoch [7/75], Val Loss: 0.2714\n",
      "Epoch [8/75], Loss: 0.1381\n",
      "Epoch [8/75], Val Loss: 0.2587\n",
      "Epoch [9/75], Loss: 0.1161\n",
      "Epoch [9/75], Val Loss: 0.2549\n",
      "Epoch [10/75], Loss: 0.0937\n",
      "Epoch [10/75], Val Loss: 0.2621\n",
      "Epoch [11/75], Loss: 0.0781\n",
      "Epoch [11/75], Val Loss: 0.2576\n",
      "Epoch [12/75], Loss: 0.0630\n",
      "Epoch [12/75], Val Loss: 0.2549\n",
      "Epoch [13/75], Loss: 0.0514\n",
      "Epoch [13/75], Val Loss: 0.2539\n",
      "Epoch [14/75], Loss: 0.0439\n",
      "Epoch [14/75], Val Loss: 0.2626\n",
      "Epoch [15/75], Loss: 0.0368\n",
      "Epoch [15/75], Val Loss: 0.2631\n",
      "Epoch [16/75], Loss: 0.0309\n",
      "Epoch [16/75], Val Loss: 0.2653\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [16/75], Loss: 0.0309\n",
      "Test Accuracy Base CNN: 86.75%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/75], Loss: 3.2287\n",
      "Epoch [1/75], Val Loss: 2.0925\n",
      "Epoch [2/75], Loss: 1.6652\n",
      "Epoch [2/75], Val Loss: 1.5327\n",
      "Epoch [3/75], Loss: 1.2551\n",
      "Epoch [3/75], Val Loss: 1.1637\n",
      "Epoch [4/75], Loss: 0.9779\n",
      "Epoch [4/75], Val Loss: 1.0380\n",
      "Epoch [5/75], Loss: 0.7739\n",
      "Epoch [5/75], Val Loss: 0.8951\n",
      "Epoch [6/75], Loss: 0.6483\n",
      "Epoch [6/75], Val Loss: 0.7989\n",
      "Epoch [7/75], Loss: 0.5446\n",
      "Epoch [7/75], Val Loss: 0.7283\n",
      "Epoch [8/75], Loss: 0.4481\n",
      "Epoch [8/75], Val Loss: 0.7155\n",
      "Epoch [9/75], Loss: 0.3668\n",
      "Epoch [9/75], Val Loss: 0.6422\n",
      "Epoch [10/75], Loss: 0.3050\n",
      "Epoch [10/75], Val Loss: 0.6100\n",
      "Epoch [11/75], Loss: 0.2411\n",
      "Epoch [11/75], Val Loss: 0.6615\n",
      "Epoch [12/75], Loss: 0.2170\n",
      "Epoch [12/75], Val Loss: 0.6514\n",
      "Epoch [13/75], Loss: 0.1985\n",
      "Epoch [13/75], Val Loss: 0.6206\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [13/75], Loss: 0.1985\n",
      "Test Accuracy Base CNN: 73.91%\n",
      "Epoch [1/75], Loss: 3.5021\n",
      "Epoch [1/75], Val Loss: 2.7566\n",
      "Epoch [2/75], Loss: 1.9470\n",
      "Epoch [2/75], Val Loss: 1.5686\n",
      "Epoch [3/75], Loss: 1.3445\n",
      "Epoch [3/75], Val Loss: 1.2806\n",
      "Epoch [4/75], Loss: 1.0529\n",
      "Epoch [4/75], Val Loss: 1.0528\n",
      "Epoch [5/75], Loss: 0.8611\n",
      "Epoch [5/75], Val Loss: 0.9429\n",
      "Epoch [6/75], Loss: 0.7222\n",
      "Epoch [6/75], Val Loss: 0.8223\n",
      "Epoch [7/75], Loss: 0.5957\n",
      "Epoch [7/75], Val Loss: 0.7479\n",
      "Epoch [8/75], Loss: 0.5042\n",
      "Epoch [8/75], Val Loss: 0.7077\n",
      "Epoch [9/75], Loss: 0.4191\n",
      "Epoch [9/75], Val Loss: 0.6645\n",
      "Epoch [10/75], Loss: 0.3424\n",
      "Epoch [10/75], Val Loss: 0.6432\n",
      "Epoch [11/75], Loss: 0.3044\n",
      "Epoch [11/75], Val Loss: 0.6365\n",
      "Epoch [12/75], Loss: 0.2503\n",
      "Epoch [12/75], Val Loss: 0.6092\n",
      "Epoch [13/75], Loss: 0.2037\n",
      "Epoch [13/75], Val Loss: 0.6059\n",
      "Epoch [14/75], Loss: 0.1622\n",
      "Epoch [14/75], Val Loss: 0.6137\n",
      "Epoch [15/75], Loss: 0.1272\n",
      "Epoch [15/75], Val Loss: 0.5957\n",
      "Epoch [16/75], Loss: 0.1026\n",
      "Epoch [16/75], Val Loss: 0.5967\n",
      "Epoch [17/75], Loss: 0.0796\n",
      "Epoch [17/75], Val Loss: 0.6007\n",
      "Epoch [18/75], Loss: 0.0720\n",
      "Epoch [18/75], Val Loss: 0.6159\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [18/75], Loss: 0.0720\n",
      "Test Accuracy Base CNN: 75.88%\n",
      "Epoch [1/75], Loss: 3.2631\n",
      "Epoch [1/75], Val Loss: 2.1206\n",
      "Epoch [2/75], Loss: 1.6889\n",
      "Epoch [2/75], Val Loss: 1.5492\n",
      "Epoch [3/75], Loss: 1.2729\n",
      "Epoch [3/75], Val Loss: 1.2131\n",
      "Epoch [4/75], Loss: 0.9859\n",
      "Epoch [4/75], Val Loss: 1.0240\n",
      "Epoch [5/75], Loss: 0.7997\n",
      "Epoch [5/75], Val Loss: 0.8928\n",
      "Epoch [6/75], Loss: 0.6577\n",
      "Epoch [6/75], Val Loss: 0.8081\n",
      "Epoch [7/75], Loss: 0.5566\n",
      "Epoch [7/75], Val Loss: 0.7171\n",
      "Epoch [8/75], Loss: 0.4672\n",
      "Epoch [8/75], Val Loss: 0.6986\n",
      "Epoch [9/75], Loss: 0.3945\n",
      "Epoch [9/75], Val Loss: 0.6528\n",
      "Epoch [10/75], Loss: 0.3246\n",
      "Epoch [10/75], Val Loss: 0.6262\n",
      "Epoch [11/75], Loss: 0.2750\n",
      "Epoch [11/75], Val Loss: 0.6079\n",
      "Epoch [12/75], Loss: 0.2267\n",
      "Epoch [12/75], Val Loss: 0.5999\n",
      "Epoch [13/75], Loss: 0.1983\n",
      "Epoch [13/75], Val Loss: 0.6157\n",
      "Epoch [14/75], Loss: 0.1829\n",
      "Epoch [14/75], Val Loss: 0.5863\n",
      "Epoch [15/75], Loss: 0.1370\n",
      "Epoch [15/75], Val Loss: 0.5955\n",
      "Epoch [16/75], Loss: 0.1278\n",
      "Epoch [16/75], Val Loss: 0.5960\n",
      "Epoch [17/75], Loss: 0.0836\n",
      "Epoch [17/75], Val Loss: 0.6086\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [17/75], Loss: 0.0836\n",
      "Test Accuracy Base CNN: 75.54%\n",
      "Epoch [1/75], Loss: 3.3574\n",
      "Epoch [1/75], Val Loss: 2.3303\n",
      "Epoch [2/75], Loss: 1.7798\n",
      "Epoch [2/75], Val Loss: 1.4947\n",
      "Epoch [3/75], Loss: 1.2901\n",
      "Epoch [3/75], Val Loss: 1.2483\n",
      "Epoch [4/75], Loss: 1.0387\n",
      "Epoch [4/75], Val Loss: 1.0531\n",
      "Epoch [5/75], Loss: 0.8514\n",
      "Epoch [5/75], Val Loss: 0.8907\n",
      "Epoch [6/75], Loss: 0.6991\n",
      "Epoch [6/75], Val Loss: 0.7937\n",
      "Epoch [7/75], Loss: 0.5667\n",
      "Epoch [7/75], Val Loss: 0.7640\n",
      "Epoch [8/75], Loss: 0.4725\n",
      "Epoch [8/75], Val Loss: 0.6830\n",
      "Epoch [9/75], Loss: 0.4088\n",
      "Epoch [9/75], Val Loss: 0.6650\n",
      "Epoch [10/75], Loss: 0.3499\n",
      "Epoch [10/75], Val Loss: 0.6247\n",
      "Epoch [11/75], Loss: 0.2817\n",
      "Epoch [11/75], Val Loss: 0.6256\n",
      "Epoch [12/75], Loss: 0.2361\n",
      "Epoch [12/75], Val Loss: 0.6242\n",
      "Epoch [13/75], Loss: 0.1918\n",
      "Epoch [13/75], Val Loss: 0.6016\n",
      "Epoch [14/75], Loss: 0.1746\n",
      "Epoch [14/75], Val Loss: 0.6045\n",
      "Epoch [15/75], Loss: 0.1285\n",
      "Epoch [15/75], Val Loss: 0.6017\n",
      "Epoch [16/75], Loss: 0.1129\n",
      "Epoch [16/75], Val Loss: 0.6094\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [16/75], Loss: 0.1129\n",
      "Test Accuracy Base CNN: 74.63%\n",
      "Epoch [1/75], Loss: 3.2846\n",
      "Epoch [1/75], Val Loss: 2.2259\n",
      "Epoch [2/75], Loss: 1.7381\n",
      "Epoch [2/75], Val Loss: 1.4746\n",
      "Epoch [3/75], Loss: 1.2932\n",
      "Epoch [3/75], Val Loss: 1.2193\n",
      "Epoch [4/75], Loss: 1.0171\n",
      "Epoch [4/75], Val Loss: 1.0197\n",
      "Epoch [5/75], Loss: 0.8105\n",
      "Epoch [5/75], Val Loss: 0.8526\n",
      "Epoch [6/75], Loss: 0.6574\n",
      "Epoch [6/75], Val Loss: 0.7934\n",
      "Epoch [7/75], Loss: 0.5526\n",
      "Epoch [7/75], Val Loss: 0.6956\n",
      "Epoch [8/75], Loss: 0.4539\n",
      "Epoch [8/75], Val Loss: 0.6885\n",
      "Epoch [9/75], Loss: 0.3880\n",
      "Epoch [9/75], Val Loss: 0.6651\n",
      "Epoch [10/75], Loss: 0.3301\n",
      "Epoch [10/75], Val Loss: 0.6164\n",
      "Epoch [11/75], Loss: 0.2871\n",
      "Epoch [11/75], Val Loss: 0.5962\n",
      "Epoch [12/75], Loss: 0.2401\n",
      "Epoch [12/75], Val Loss: 0.5748\n",
      "Epoch [13/75], Loss: 0.1864\n",
      "Epoch [13/75], Val Loss: 0.5839\n",
      "Epoch [14/75], Loss: 0.1431\n",
      "Epoch [14/75], Val Loss: 0.5906\n",
      "Epoch [15/75], Loss: 0.1248\n",
      "Epoch [15/75], Val Loss: 0.5954\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [15/75], Loss: 0.1248\n",
      "Test Accuracy Base CNN: 75.06%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 75\n",
    "tolerance = 1e-4  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "logit_accuracy_cnn = {}\n",
    "\n",
    "for subset_size in num_subsets:\n",
    "    accuracy_list = []\n",
    "    print(f\"Training with subset size: {subset_size}\")\n",
    "\n",
    "    for _ in range(5):\n",
    "        cnn_model = KMNIST_CNN(num_classes).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_sdg = optim.SGD(cnn_model.parameters(), lr=0.005, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "\n",
    "        def init_weights(m):\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)  # Set all weights to zero\n",
    "                    m.bias.data.fill_(0.01)\n",
    "        cnn_model.apply(init_weights)\n",
    "\n",
    "        subset_indices = torch.randperm(train_data.shape[0])[:subset_size]\n",
    "        subset = SubsetRandomSampler(subset_indices)\n",
    "        train_loader = DataLoader(train_set, batch_size=128, sampler=subset)\n",
    "\n",
    "        best_val_loss = float('inf')  # Track the best loss\n",
    "        epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            cnn_model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = cnn_model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer_sdg.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer_sdg.step()\n",
    "\n",
    "                total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "            current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "            # Validation loop\n",
    "            cnn_model.eval()\n",
    "            val_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader_cnn:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                    outputs = cnn_model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader_cnn)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "                    break\n",
    "\n",
    "        print(f\"Subset {subset_size}, Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        cnn_model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = cnn_model(images)\n",
    "                _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Test Accuracy Base CNN: {accuracy:.2f}%\")\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    logit_accuracy_cnn[subset_size] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125000    90.629621\n",
       " 75000     88.546969\n",
       " 50000     86.483514\n",
       " 10000     75.004540\n",
       " dtype: float64,\n",
       " 125000    0.162932\n",
       " 75000     0.120295\n",
       " 50000     0.327632\n",
       " 10000     0.773520\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_accuracy_cnn_df = pd.DataFrame(logit_accuracy_cnn)\n",
    "logit_accuracy_cnn_df.mean(), logit_accuracy_cnn_df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Lipschitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with subset size: 125000\n",
      "Epoch [1/75], Loss: 0.8801\n",
      "Epoch [1/75], Val Loss: 0.3494\n",
      "Epoch [2/75], Loss: 0.2800\n",
      "Epoch [2/75], Val Loss: 0.2360\n",
      "Epoch [3/75], Loss: 0.1893\n",
      "Epoch [3/75], Val Loss: 0.1783\n",
      "Epoch [4/75], Loss: 0.1401\n",
      "Epoch [4/75], Val Loss: 0.1553\n",
      "Epoch [5/75], Loss: 0.1082\n",
      "Epoch [5/75], Val Loss: 0.1337\n",
      "Epoch [6/75], Loss: 0.0855\n",
      "Epoch [6/75], Val Loss: 0.1283\n",
      "Epoch [7/75], Loss: 0.0673\n",
      "Epoch [7/75], Val Loss: 0.1179\n",
      "Epoch [8/75], Loss: 0.0537\n",
      "Epoch [8/75], Val Loss: 0.1201\n",
      "Epoch [9/75], Loss: 0.0415\n",
      "Epoch [9/75], Val Loss: 0.1180\n",
      "Epoch [10/75], Loss: 0.0337\n",
      "Epoch [10/75], Val Loss: 0.1146\n",
      "Epoch [11/75], Loss: 0.0265\n",
      "Epoch [11/75], Val Loss: 0.1132\n",
      "Epoch [12/75], Loss: 0.0213\n",
      "Epoch [12/75], Val Loss: 0.1177\n",
      "Epoch [13/75], Loss: 0.0184\n",
      "Epoch [13/75], Val Loss: 0.1216\n",
      "Epoch [14/75], Loss: 0.0158\n",
      "Epoch [14/75], Val Loss: 0.1134\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 125000, Epoch [14/75], Loss: 0.0158\n",
      "Test Accuracy CNN Lipschitz: 91.07%\n",
      "Epoch [1/75], Loss: 0.8421\n",
      "Epoch [1/75], Val Loss: 0.3387\n",
      "Epoch [2/75], Loss: 0.2745\n",
      "Epoch [2/75], Val Loss: 0.2309\n",
      "Epoch [3/75], Loss: 0.1846\n",
      "Epoch [3/75], Val Loss: 0.1777\n",
      "Epoch [4/75], Loss: 0.1404\n",
      "Epoch [4/75], Val Loss: 0.1577\n",
      "Epoch [5/75], Loss: 0.1089\n",
      "Epoch [5/75], Val Loss: 0.1356\n",
      "Epoch [6/75], Loss: 0.0850\n",
      "Epoch [6/75], Val Loss: 0.1304\n",
      "Epoch [7/75], Loss: 0.0683\n",
      "Epoch [7/75], Val Loss: 0.1212\n",
      "Epoch [8/75], Loss: 0.0540\n",
      "Epoch [8/75], Val Loss: 0.1220\n",
      "Epoch [9/75], Loss: 0.0421\n",
      "Epoch [9/75], Val Loss: 0.1183\n",
      "Epoch [10/75], Loss: 0.0334\n",
      "Epoch [10/75], Val Loss: 0.1180\n",
      "Epoch [11/75], Loss: 0.0277\n",
      "Epoch [11/75], Val Loss: 0.1143\n",
      "Epoch [12/75], Loss: 0.0217\n",
      "Epoch [12/75], Val Loss: 0.1230\n",
      "Epoch [13/75], Loss: 0.0178\n",
      "Epoch [13/75], Val Loss: 0.1157\n",
      "Epoch [14/75], Loss: 0.0144\n",
      "Epoch [14/75], Val Loss: 0.1153\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 125000, Epoch [14/75], Loss: 0.0144\n",
      "Test Accuracy CNN Lipschitz: 91.31%\n",
      "Epoch [1/75], Loss: 0.8356\n",
      "Epoch [1/75], Val Loss: 0.3284\n",
      "Epoch [2/75], Loss: 0.2700\n",
      "Epoch [2/75], Val Loss: 0.2150\n",
      "Epoch [3/75], Loss: 0.1849\n",
      "Epoch [3/75], Val Loss: 0.1746\n",
      "Epoch [4/75], Loss: 0.1400\n",
      "Epoch [4/75], Val Loss: 0.1490\n",
      "Epoch [5/75], Loss: 0.1082\n",
      "Epoch [5/75], Val Loss: 0.1369\n",
      "Epoch [6/75], Loss: 0.0853\n",
      "Epoch [6/75], Val Loss: 0.1319\n",
      "Epoch [7/75], Loss: 0.0677\n",
      "Epoch [7/75], Val Loss: 0.1235\n",
      "Epoch [8/75], Loss: 0.0534\n",
      "Epoch [8/75], Val Loss: 0.1193\n",
      "Epoch [9/75], Loss: 0.0421\n",
      "Epoch [9/75], Val Loss: 0.1183\n",
      "Epoch [10/75], Loss: 0.0336\n",
      "Epoch [10/75], Val Loss: 0.1176\n",
      "Epoch [11/75], Loss: 0.0285\n",
      "Epoch [11/75], Val Loss: 0.1130\n",
      "Epoch [12/75], Loss: 0.0221\n",
      "Epoch [12/75], Val Loss: 0.1148\n",
      "Epoch [13/75], Loss: 0.0180\n",
      "Epoch [13/75], Val Loss: 0.1184\n",
      "Epoch [14/75], Loss: 0.0157\n",
      "Epoch [14/75], Val Loss: 0.1132\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 125000, Epoch [14/75], Loss: 0.0157\n",
      "Test Accuracy CNN Lipschitz: 91.15%\n",
      "Epoch [1/75], Loss: 0.8095\n",
      "Epoch [1/75], Val Loss: 0.3227\n",
      "Epoch [2/75], Loss: 0.2710\n",
      "Epoch [2/75], Val Loss: 0.2353\n",
      "Epoch [3/75], Loss: 0.1850\n",
      "Epoch [3/75], Val Loss: 0.1753\n",
      "Epoch [4/75], Loss: 0.1358\n",
      "Epoch [4/75], Val Loss: 0.1544\n",
      "Epoch [5/75], Loss: 0.1061\n",
      "Epoch [5/75], Val Loss: 0.1469\n",
      "Epoch [6/75], Loss: 0.0834\n",
      "Epoch [6/75], Val Loss: 0.1286\n",
      "Epoch [7/75], Loss: 0.0651\n",
      "Epoch [7/75], Val Loss: 0.1242\n",
      "Epoch [8/75], Loss: 0.0521\n",
      "Epoch [8/75], Val Loss: 0.1163\n",
      "Epoch [9/75], Loss: 0.0404\n",
      "Epoch [9/75], Val Loss: 0.1170\n",
      "Epoch [10/75], Loss: 0.0334\n",
      "Epoch [10/75], Val Loss: 0.1132\n",
      "Epoch [11/75], Loss: 0.0259\n",
      "Epoch [11/75], Val Loss: 0.1158\n",
      "Epoch [12/75], Loss: 0.0204\n",
      "Epoch [12/75], Val Loss: 0.1151\n",
      "Epoch [13/75], Loss: 0.0173\n",
      "Epoch [13/75], Val Loss: 0.1165\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 125000, Epoch [13/75], Loss: 0.0173\n",
      "Test Accuracy CNN Lipschitz: 91.00%\n",
      "Epoch [1/75], Loss: 0.8130\n",
      "Epoch [1/75], Val Loss: 0.3227\n",
      "Epoch [2/75], Loss: 0.2657\n",
      "Epoch [2/75], Val Loss: 0.2075\n",
      "Epoch [3/75], Loss: 0.1822\n",
      "Epoch [3/75], Val Loss: 0.1751\n",
      "Epoch [4/75], Loss: 0.1358\n",
      "Epoch [4/75], Val Loss: 0.1445\n",
      "Epoch [5/75], Loss: 0.1056\n",
      "Epoch [5/75], Val Loss: 0.1291\n",
      "Epoch [6/75], Loss: 0.0829\n",
      "Epoch [6/75], Val Loss: 0.1242\n",
      "Epoch [7/75], Loss: 0.0660\n",
      "Epoch [7/75], Val Loss: 0.1158\n",
      "Epoch [8/75], Loss: 0.0516\n",
      "Epoch [8/75], Val Loss: 0.1134\n",
      "Epoch [9/75], Loss: 0.0394\n",
      "Epoch [9/75], Val Loss: 0.1069\n",
      "Epoch [10/75], Loss: 0.0328\n",
      "Epoch [10/75], Val Loss: 0.1107\n",
      "Epoch [11/75], Loss: 0.0274\n",
      "Epoch [11/75], Val Loss: 0.1066\n",
      "Epoch [12/75], Loss: 0.0205\n",
      "Epoch [12/75], Val Loss: 0.1104\n",
      "Epoch [13/75], Loss: 0.0169\n",
      "Epoch [13/75], Val Loss: 0.1062\n",
      "Epoch [14/75], Loss: 0.0138\n",
      "Epoch [14/75], Val Loss: 0.1079\n",
      "Epoch [15/75], Loss: 0.0123\n",
      "Epoch [15/75], Val Loss: 0.1067\n",
      "Epoch [16/75], Loss: 0.0099\n",
      "Epoch [16/75], Val Loss: 0.1050\n",
      "Epoch [17/75], Loss: 0.0095\n",
      "Epoch [17/75], Val Loss: 0.1113\n",
      "Epoch [18/75], Loss: 0.0090\n",
      "Epoch [18/75], Val Loss: 0.1054\n",
      "Epoch [19/75], Loss: 0.0080\n",
      "Epoch [19/75], Val Loss: 0.1047\n",
      "Epoch [20/75], Loss: 0.0065\n",
      "Epoch [20/75], Val Loss: 0.1052\n",
      "Epoch [21/75], Loss: 0.0061\n",
      "Epoch [21/75], Val Loss: 0.1077\n",
      "Epoch [22/75], Loss: 0.0054\n",
      "Epoch [22/75], Val Loss: 0.1061\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 125000, Epoch [22/75], Loss: 0.0054\n",
      "Test Accuracy CNN Lipschitz: 91.70%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/75], Loss: 1.0671\n",
      "Epoch [1/75], Val Loss: 0.4535\n",
      "Epoch [2/75], Loss: 0.3531\n",
      "Epoch [2/75], Val Loss: 0.2997\n",
      "Epoch [3/75], Loss: 0.2356\n",
      "Epoch [3/75], Val Loss: 0.2441\n",
      "Epoch [4/75], Loss: 0.1714\n",
      "Epoch [4/75], Val Loss: 0.2253\n",
      "Epoch [5/75], Loss: 0.1283\n",
      "Epoch [5/75], Val Loss: 0.1902\n",
      "Epoch [6/75], Loss: 0.0989\n",
      "Epoch [6/75], Val Loss: 0.1897\n",
      "Epoch [7/75], Loss: 0.0774\n",
      "Epoch [7/75], Val Loss: 0.1849\n",
      "Epoch [8/75], Loss: 0.0583\n",
      "Epoch [8/75], Val Loss: 0.1830\n",
      "Epoch [9/75], Loss: 0.0442\n",
      "Epoch [9/75], Val Loss: 0.1833\n",
      "Epoch [10/75], Loss: 0.0341\n",
      "Epoch [10/75], Val Loss: 0.1893\n",
      "Epoch [11/75], Loss: 0.0268\n",
      "Epoch [11/75], Val Loss: 0.1900\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [11/75], Loss: 0.0268\n",
      "Test Accuracy CNN Lipschitz: 89.19%\n",
      "Epoch [1/75], Loss: 1.0457\n",
      "Epoch [1/75], Val Loss: 0.4396\n",
      "Epoch [2/75], Loss: 0.3500\n",
      "Epoch [2/75], Val Loss: 0.3059\n",
      "Epoch [3/75], Loss: 0.2337\n",
      "Epoch [3/75], Val Loss: 0.2388\n",
      "Epoch [4/75], Loss: 0.1734\n",
      "Epoch [4/75], Val Loss: 0.2088\n",
      "Epoch [5/75], Loss: 0.1296\n",
      "Epoch [5/75], Val Loss: 0.2024\n",
      "Epoch [6/75], Loss: 0.0990\n",
      "Epoch [6/75], Val Loss: 0.1894\n",
      "Epoch [7/75], Loss: 0.0767\n",
      "Epoch [7/75], Val Loss: 0.1897\n",
      "Epoch [8/75], Loss: 0.0597\n",
      "Epoch [8/75], Val Loss: 0.1853\n",
      "Epoch [9/75], Loss: 0.0444\n",
      "Epoch [9/75], Val Loss: 0.1917\n",
      "Epoch [10/75], Loss: 0.0361\n",
      "Epoch [10/75], Val Loss: 0.1883\n",
      "Epoch [11/75], Loss: 0.0275\n",
      "Epoch [11/75], Val Loss: 0.1922\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [11/75], Loss: 0.0275\n",
      "Test Accuracy CNN Lipschitz: 89.25%\n",
      "Epoch [1/75], Loss: 1.1109\n",
      "Epoch [1/75], Val Loss: 0.4818\n",
      "Epoch [2/75], Loss: 0.3731\n",
      "Epoch [2/75], Val Loss: 0.3220\n",
      "Epoch [3/75], Loss: 0.2466\n",
      "Epoch [3/75], Val Loss: 0.2607\n",
      "Epoch [4/75], Loss: 0.1804\n",
      "Epoch [4/75], Val Loss: 0.2098\n",
      "Epoch [5/75], Loss: 0.1341\n",
      "Epoch [5/75], Val Loss: 0.2079\n",
      "Epoch [6/75], Loss: 0.1038\n",
      "Epoch [6/75], Val Loss: 0.1886\n",
      "Epoch [7/75], Loss: 0.0785\n",
      "Epoch [7/75], Val Loss: 0.1849\n",
      "Epoch [8/75], Loss: 0.0577\n",
      "Epoch [8/75], Val Loss: 0.1875\n",
      "Epoch [9/75], Loss: 0.0446\n",
      "Epoch [9/75], Val Loss: 0.1881\n",
      "Epoch [10/75], Loss: 0.0359\n",
      "Epoch [10/75], Val Loss: 0.1917\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [10/75], Loss: 0.0359\n",
      "Test Accuracy CNN Lipschitz: 89.00%\n",
      "Epoch [1/75], Loss: 1.1020\n",
      "Epoch [1/75], Val Loss: 0.4547\n",
      "Epoch [2/75], Loss: 0.3626\n",
      "Epoch [2/75], Val Loss: 0.2989\n",
      "Epoch [3/75], Loss: 0.2402\n",
      "Epoch [3/75], Val Loss: 0.2346\n",
      "Epoch [4/75], Loss: 0.1732\n",
      "Epoch [4/75], Val Loss: 0.2173\n",
      "Epoch [5/75], Loss: 0.1318\n",
      "Epoch [5/75], Val Loss: 0.2048\n",
      "Epoch [6/75], Loss: 0.0993\n",
      "Epoch [6/75], Val Loss: 0.1849\n",
      "Epoch [7/75], Loss: 0.0752\n",
      "Epoch [7/75], Val Loss: 0.1853\n",
      "Epoch [8/75], Loss: 0.0606\n",
      "Epoch [8/75], Val Loss: 0.1839\n",
      "Epoch [9/75], Loss: 0.0453\n",
      "Epoch [9/75], Val Loss: 0.1794\n",
      "Epoch [10/75], Loss: 0.0352\n",
      "Epoch [10/75], Val Loss: 0.1833\n",
      "Epoch [11/75], Loss: 0.0290\n",
      "Epoch [11/75], Val Loss: 0.1886\n",
      "Epoch [12/75], Loss: 0.0237\n",
      "Epoch [12/75], Val Loss: 0.1861\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [12/75], Loss: 0.0237\n",
      "Test Accuracy CNN Lipschitz: 89.61%\n",
      "Epoch [1/75], Loss: 1.0869\n",
      "Epoch [1/75], Val Loss: 0.4638\n",
      "Epoch [2/75], Loss: 0.3675\n",
      "Epoch [2/75], Val Loss: 0.3051\n",
      "Epoch [3/75], Loss: 0.2431\n",
      "Epoch [3/75], Val Loss: 0.2556\n",
      "Epoch [4/75], Loss: 0.1757\n",
      "Epoch [4/75], Val Loss: 0.2115\n",
      "Epoch [5/75], Loss: 0.1348\n",
      "Epoch [5/75], Val Loss: 0.1917\n",
      "Epoch [6/75], Loss: 0.1010\n",
      "Epoch [6/75], Val Loss: 0.1847\n",
      "Epoch [7/75], Loss: 0.0774\n",
      "Epoch [7/75], Val Loss: 0.1814\n",
      "Epoch [8/75], Loss: 0.0613\n",
      "Epoch [8/75], Val Loss: 0.1904\n",
      "Epoch [9/75], Loss: 0.0450\n",
      "Epoch [9/75], Val Loss: 0.1762\n",
      "Epoch [10/75], Loss: 0.0342\n",
      "Epoch [10/75], Val Loss: 0.1821\n",
      "Epoch [11/75], Loss: 0.0262\n",
      "Epoch [11/75], Val Loss: 0.1804\n",
      "Epoch [12/75], Loss: 0.0201\n",
      "Epoch [12/75], Val Loss: 0.1831\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [12/75], Loss: 0.0201\n",
      "Test Accuracy CNN Lipschitz: 89.39%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/75], Loss: 1.3555\n",
      "Epoch [1/75], Val Loss: 0.5844\n",
      "Epoch [2/75], Loss: 0.4594\n",
      "Epoch [2/75], Val Loss: 0.3829\n",
      "Epoch [3/75], Loss: 0.2943\n",
      "Epoch [3/75], Val Loss: 0.3188\n",
      "Epoch [4/75], Loss: 0.2100\n",
      "Epoch [4/75], Val Loss: 0.2733\n",
      "Epoch [5/75], Loss: 0.1525\n",
      "Epoch [5/75], Val Loss: 0.2478\n",
      "Epoch [6/75], Loss: 0.1145\n",
      "Epoch [6/75], Val Loss: 0.2471\n",
      "Epoch [7/75], Loss: 0.0852\n",
      "Epoch [7/75], Val Loss: 0.2454\n",
      "Epoch [8/75], Loss: 0.0636\n",
      "Epoch [8/75], Val Loss: 0.2481\n",
      "Epoch [9/75], Loss: 0.0493\n",
      "Epoch [9/75], Val Loss: 0.2491\n",
      "Epoch [10/75], Loss: 0.0370\n",
      "Epoch [10/75], Val Loss: 0.2527\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [10/75], Loss: 0.0370\n",
      "Test Accuracy CNN Lipschitz: 87.13%\n",
      "Epoch [1/75], Loss: 1.3738\n",
      "Epoch [1/75], Val Loss: 0.6272\n",
      "Epoch [2/75], Loss: 0.4874\n",
      "Epoch [2/75], Val Loss: 0.4029\n",
      "Epoch [3/75], Loss: 0.3143\n",
      "Epoch [3/75], Val Loss: 0.3231\n",
      "Epoch [4/75], Loss: 0.2220\n",
      "Epoch [4/75], Val Loss: 0.2797\n",
      "Epoch [5/75], Loss: 0.1662\n",
      "Epoch [5/75], Val Loss: 0.2685\n",
      "Epoch [6/75], Loss: 0.1270\n",
      "Epoch [6/75], Val Loss: 0.2570\n",
      "Epoch [7/75], Loss: 0.0933\n",
      "Epoch [7/75], Val Loss: 0.2514\n",
      "Epoch [8/75], Loss: 0.0708\n",
      "Epoch [8/75], Val Loss: 0.2506\n",
      "Epoch [9/75], Loss: 0.0533\n",
      "Epoch [9/75], Val Loss: 0.2466\n",
      "Epoch [10/75], Loss: 0.0420\n",
      "Epoch [10/75], Val Loss: 0.2597\n",
      "Epoch [11/75], Loss: 0.0316\n",
      "Epoch [11/75], Val Loss: 0.2475\n",
      "Epoch [12/75], Loss: 0.0242\n",
      "Epoch [12/75], Val Loss: 0.2587\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [12/75], Loss: 0.0242\n",
      "Test Accuracy CNN Lipschitz: 87.21%\n",
      "Epoch [1/75], Loss: 1.4260\n",
      "Epoch [1/75], Val Loss: 0.6480\n",
      "Epoch [2/75], Loss: 0.4952\n",
      "Epoch [2/75], Val Loss: 0.4066\n",
      "Epoch [3/75], Loss: 0.3214\n",
      "Epoch [3/75], Val Loss: 0.3443\n",
      "Epoch [4/75], Loss: 0.2290\n",
      "Epoch [4/75], Val Loss: 0.3035\n",
      "Epoch [5/75], Loss: 0.1704\n",
      "Epoch [5/75], Val Loss: 0.2709\n",
      "Epoch [6/75], Loss: 0.1313\n",
      "Epoch [6/75], Val Loss: 0.2556\n",
      "Epoch [7/75], Loss: 0.0965\n",
      "Epoch [7/75], Val Loss: 0.2559\n",
      "Epoch [8/75], Loss: 0.0730\n",
      "Epoch [8/75], Val Loss: 0.2499\n",
      "Epoch [9/75], Loss: 0.0563\n",
      "Epoch [9/75], Val Loss: 0.2607\n",
      "Epoch [10/75], Loss: 0.0452\n",
      "Epoch [10/75], Val Loss: 0.2694\n",
      "Epoch [11/75], Loss: 0.0318\n",
      "Epoch [11/75], Val Loss: 0.2557\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [11/75], Loss: 0.0318\n",
      "Test Accuracy CNN Lipschitz: 87.26%\n",
      "Epoch [1/75], Loss: 1.3532\n",
      "Epoch [1/75], Val Loss: 0.6216\n",
      "Epoch [2/75], Loss: 0.4744\n",
      "Epoch [2/75], Val Loss: 0.3970\n",
      "Epoch [3/75], Loss: 0.3113\n",
      "Epoch [3/75], Val Loss: 0.3106\n",
      "Epoch [4/75], Loss: 0.2242\n",
      "Epoch [4/75], Val Loss: 0.2936\n",
      "Epoch [5/75], Loss: 0.1713\n",
      "Epoch [5/75], Val Loss: 0.2563\n",
      "Epoch [6/75], Loss: 0.1256\n",
      "Epoch [6/75], Val Loss: 0.2511\n",
      "Epoch [7/75], Loss: 0.0977\n",
      "Epoch [7/75], Val Loss: 0.2428\n",
      "Epoch [8/75], Loss: 0.0716\n",
      "Epoch [8/75], Val Loss: 0.2503\n",
      "Epoch [9/75], Loss: 0.0534\n",
      "Epoch [9/75], Val Loss: 0.2429\n",
      "Epoch [10/75], Loss: 0.0403\n",
      "Epoch [10/75], Val Loss: 0.2498\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [10/75], Loss: 0.0403\n",
      "Test Accuracy CNN Lipschitz: 87.03%\n",
      "Epoch [1/75], Loss: 1.3180\n",
      "Epoch [1/75], Val Loss: 0.5806\n",
      "Epoch [2/75], Loss: 0.4697\n",
      "Epoch [2/75], Val Loss: 0.3873\n",
      "Epoch [3/75], Loss: 0.3063\n",
      "Epoch [3/75], Val Loss: 0.3060\n",
      "Epoch [4/75], Loss: 0.2198\n",
      "Epoch [4/75], Val Loss: 0.2669\n",
      "Epoch [5/75], Loss: 0.1648\n",
      "Epoch [5/75], Val Loss: 0.2497\n",
      "Epoch [6/75], Loss: 0.1231\n",
      "Epoch [6/75], Val Loss: 0.2501\n",
      "Epoch [7/75], Loss: 0.0966\n",
      "Epoch [7/75], Val Loss: 0.2372\n",
      "Epoch [8/75], Loss: 0.0713\n",
      "Epoch [8/75], Val Loss: 0.2361\n",
      "Epoch [9/75], Loss: 0.0551\n",
      "Epoch [9/75], Val Loss: 0.2423\n",
      "Epoch [10/75], Loss: 0.0401\n",
      "Epoch [10/75], Val Loss: 0.2376\n",
      "Epoch [11/75], Loss: 0.0320\n",
      "Epoch [11/75], Val Loss: 0.2432\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [11/75], Loss: 0.0320\n",
      "Test Accuracy CNN Lipschitz: 87.00%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/75], Loss: 2.8980\n",
      "Epoch [1/75], Val Loss: 1.8639\n",
      "Epoch [2/75], Loss: 1.4219\n",
      "Epoch [2/75], Val Loss: 1.2392\n",
      "Epoch [3/75], Loss: 0.9601\n",
      "Epoch [3/75], Val Loss: 0.9163\n",
      "Epoch [4/75], Loss: 0.6998\n",
      "Epoch [4/75], Val Loss: 0.7574\n",
      "Epoch [5/75], Loss: 0.5213\n",
      "Epoch [5/75], Val Loss: 0.6327\n",
      "Epoch [6/75], Loss: 0.3860\n",
      "Epoch [6/75], Val Loss: 0.6166\n",
      "Epoch [7/75], Loss: 0.3266\n",
      "Epoch [7/75], Val Loss: 0.6087\n",
      "Epoch [8/75], Loss: 0.2364\n",
      "Epoch [8/75], Val Loss: 0.5748\n",
      "Epoch [9/75], Loss: 0.1685\n",
      "Epoch [9/75], Val Loss: 0.5583\n",
      "Epoch [10/75], Loss: 0.1281\n",
      "Epoch [10/75], Val Loss: 0.5682\n",
      "Epoch [11/75], Loss: 0.0930\n",
      "Epoch [11/75], Val Loss: 0.5668\n",
      "Epoch [12/75], Loss: 0.0619\n",
      "Epoch [12/75], Val Loss: 0.5818\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [12/75], Loss: 0.0619\n",
      "Test Accuracy CNN Lipschitz: 76.66%\n",
      "Epoch [1/75], Loss: 2.7484\n",
      "Epoch [1/75], Val Loss: 1.7091\n",
      "Epoch [2/75], Loss: 1.3679\n",
      "Epoch [2/75], Val Loss: 1.1235\n",
      "Epoch [3/75], Loss: 0.9322\n",
      "Epoch [3/75], Val Loss: 0.8789\n",
      "Epoch [4/75], Loss: 0.6631\n",
      "Epoch [4/75], Val Loss: 0.7562\n",
      "Epoch [5/75], Loss: 0.5078\n",
      "Epoch [5/75], Val Loss: 0.6457\n",
      "Epoch [6/75], Loss: 0.3671\n",
      "Epoch [6/75], Val Loss: 0.6061\n",
      "Epoch [7/75], Loss: 0.2914\n",
      "Epoch [7/75], Val Loss: 0.5839\n",
      "Epoch [8/75], Loss: 0.2299\n",
      "Epoch [8/75], Val Loss: 0.5969\n",
      "Epoch [9/75], Loss: 0.1584\n",
      "Epoch [9/75], Val Loss: 0.6011\n",
      "Epoch [10/75], Loss: 0.1282\n",
      "Epoch [10/75], Val Loss: 0.5737\n",
      "Epoch [11/75], Loss: 0.0878\n",
      "Epoch [11/75], Val Loss: 0.5922\n",
      "Epoch [12/75], Loss: 0.0649\n",
      "Epoch [12/75], Val Loss: 0.6008\n",
      "Epoch [13/75], Loss: 0.0585\n",
      "Epoch [13/75], Val Loss: 0.5922\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [13/75], Loss: 0.0585\n",
      "Test Accuracy CNN Lipschitz: 76.97%\n",
      "Epoch [1/75], Loss: 2.8517\n",
      "Epoch [1/75], Val Loss: 1.8585\n",
      "Epoch [2/75], Loss: 1.4021\n",
      "Epoch [2/75], Val Loss: 1.1788\n",
      "Epoch [3/75], Loss: 0.9085\n",
      "Epoch [3/75], Val Loss: 0.8744\n",
      "Epoch [4/75], Loss: 0.6750\n",
      "Epoch [4/75], Val Loss: 0.7901\n",
      "Epoch [5/75], Loss: 0.5134\n",
      "Epoch [5/75], Val Loss: 0.6714\n",
      "Epoch [6/75], Loss: 0.3929\n",
      "Epoch [6/75], Val Loss: 0.6102\n",
      "Epoch [7/75], Loss: 0.2995\n",
      "Epoch [7/75], Val Loss: 0.6002\n",
      "Epoch [8/75], Loss: 0.2288\n",
      "Epoch [8/75], Val Loss: 0.5741\n",
      "Epoch [9/75], Loss: 0.1849\n",
      "Epoch [9/75], Val Loss: 0.5702\n",
      "Epoch [10/75], Loss: 0.1252\n",
      "Epoch [10/75], Val Loss: 0.5719\n",
      "Epoch [11/75], Loss: 0.0997\n",
      "Epoch [11/75], Val Loss: 0.5937\n",
      "Epoch [12/75], Loss: 0.0669\n",
      "Epoch [12/75], Val Loss: 0.5941\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [12/75], Loss: 0.0669\n",
      "Test Accuracy CNN Lipschitz: 76.64%\n",
      "Epoch [1/75], Loss: 2.7499\n",
      "Epoch [1/75], Val Loss: 1.8081\n",
      "Epoch [2/75], Loss: 1.3871\n",
      "Epoch [2/75], Val Loss: 1.1850\n",
      "Epoch [3/75], Loss: 0.9386\n",
      "Epoch [3/75], Val Loss: 0.9569\n",
      "Epoch [4/75], Loss: 0.6761\n",
      "Epoch [4/75], Val Loss: 0.7860\n",
      "Epoch [5/75], Loss: 0.4959\n",
      "Epoch [5/75], Val Loss: 0.6657\n",
      "Epoch [6/75], Loss: 0.3991\n",
      "Epoch [6/75], Val Loss: 0.6131\n",
      "Epoch [7/75], Loss: 0.3136\n",
      "Epoch [7/75], Val Loss: 0.6247\n",
      "Epoch [8/75], Loss: 0.2385\n",
      "Epoch [8/75], Val Loss: 0.5618\n",
      "Epoch [9/75], Loss: 0.1675\n",
      "Epoch [9/75], Val Loss: 0.5560\n",
      "Epoch [10/75], Loss: 0.1208\n",
      "Epoch [10/75], Val Loss: 0.5697\n",
      "Epoch [11/75], Loss: 0.0824\n",
      "Epoch [11/75], Val Loss: 0.5986\n",
      "Epoch [12/75], Loss: 0.0818\n",
      "Epoch [12/75], Val Loss: 0.5835\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [12/75], Loss: 0.0818\n",
      "Test Accuracy CNN Lipschitz: 76.54%\n",
      "Epoch [1/75], Loss: 2.9472\n",
      "Epoch [1/75], Val Loss: 1.9947\n",
      "Epoch [2/75], Loss: 1.4254\n",
      "Epoch [2/75], Val Loss: 1.1418\n",
      "Epoch [3/75], Loss: 0.9429\n",
      "Epoch [3/75], Val Loss: 0.8911\n",
      "Epoch [4/75], Loss: 0.6886\n",
      "Epoch [4/75], Val Loss: 0.7784\n",
      "Epoch [5/75], Loss: 0.5251\n",
      "Epoch [5/75], Val Loss: 0.6813\n",
      "Epoch [6/75], Loss: 0.4241\n",
      "Epoch [6/75], Val Loss: 0.6286\n",
      "Epoch [7/75], Loss: 0.3218\n",
      "Epoch [7/75], Val Loss: 0.6087\n",
      "Epoch [8/75], Loss: 0.2388\n",
      "Epoch [8/75], Val Loss: 0.5906\n",
      "Epoch [9/75], Loss: 0.1559\n",
      "Epoch [9/75], Val Loss: 0.5785\n",
      "Epoch [10/75], Loss: 0.1090\n",
      "Epoch [10/75], Val Loss: 0.5795\n",
      "Epoch [11/75], Loss: 0.0736\n",
      "Epoch [11/75], Val Loss: 0.5909\n",
      "Epoch [12/75], Loss: 0.0506\n",
      "Epoch [12/75], Val Loss: 0.6389\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [12/75], Loss: 0.0506\n",
      "Test Accuracy CNN Lipschitz: 75.50%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 75\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "logit_accuracy_cnn_lipschitz = {}\n",
    "\n",
    "for subset_size in num_subsets:\n",
    "    accuracy_list = []\n",
    "    print(f\"Training with subset size: {subset_size}\")\n",
    "\n",
    "    for _ in range(5):\n",
    "        cnn_model_lipschitz = KMNIST_CNN(num_classes).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_sdg = optim.SGD(cnn_model_lipschitz.parameters(), lr=0.005, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "\n",
    "        def init_weights(m):\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)  # Set all weights to zero\n",
    "                    m.bias.data.fill_(0.01)\n",
    "        cnn_model_lipschitz.apply(init_weights)\n",
    "\n",
    "        subset_indices = torch.randperm(train_data.shape[0])[:subset_size]\n",
    "        subset = SubsetRandomSampler(subset_indices)\n",
    "        train_loader = DataLoader(train_set, batch_size=128, sampler=subset)\n",
    "\n",
    "        best_val_loss = float('inf')  # Track the best loss\n",
    "        epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            cnn_model_lipschitz.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = cnn_model_lipschitz(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer_sdg.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "\n",
    "                L = max(param.grad.abs().max().item() for param in cnn_model_lipschitz.parameters() if param.grad is not None)\n",
    "                adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "                # Update optimizer's learning rate\n",
    "                for param_group in optimizer_sdg.param_groups:\n",
    "                    param_group['lr'] = adaptive_lr\n",
    "\n",
    "                optimizer_sdg.step()\n",
    "\n",
    "                total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "            current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "           # Validation loop\n",
    "            cnn_model_lipschitz.eval()\n",
    "            val_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader_cnn:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                    outputs = cnn_model_lipschitz(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader_cnn)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "                    break\n",
    "\n",
    "        print(f\"Subset {subset_size}, Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        cnn_model_lipschitz.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = cnn_model_lipschitz(images)\n",
    "                _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Test Accuracy CNN Lipschitz: {accuracy:.2f}%\")\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    logit_accuracy_cnn_lipschitz[subset_size] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125000    91.247049\n",
       " 75000     89.287363\n",
       " 50000     87.123252\n",
       " 10000     76.460944\n",
       " dtype: float64,\n",
       " 125000    0.279209\n",
       " 75000     0.229898\n",
       " 50000     0.112866\n",
       " 10000     0.562266\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_accuracy_cnn_lipschitz_df = pd.DataFrame(logit_accuracy_cnn_lipschitz)\n",
    "logit_accuracy_cnn_lipschitz_df.mean(), logit_accuracy_cnn_lipschitz_df.std(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans(Dataset):\n",
    "    def __init__(self, centroids, labels):\n",
    "        \"\"\"\n",
    "        centroids: torch.Tensor or numpy array of shape (N, D) where N is the number of samples, D is the feature size.\n",
    "        labels: torch.Tensor or numpy array of shape (N,) containing class labels.\n",
    "        \"\"\"\n",
    "        self.centroids = centroids.clone().detach().float()\n",
    "        self.labels = labels.clone().detach().long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centroids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.centroids[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 18.9194\n",
      "Epoch [2/100], Loss: 16.3348\n",
      "Epoch [3/100], Loss: 15.9327\n",
      "Epoch [4/100], Loss: 15.6991\n",
      "Epoch [5/100], Loss: 15.5490\n",
      "Epoch [6/100], Loss: 15.4531\n",
      "Epoch [7/100], Loss: 15.3724\n",
      "Epoch [8/100], Loss: 15.3165\n",
      "Epoch [9/100], Loss: 15.2503\n",
      "Epoch [10/100], Loss: 15.1655\n",
      "Epoch [11/100], Loss: 15.1266\n",
      "Epoch [12/100], Loss: 15.1142\n",
      "Epoch [13/100], Loss: 15.0751\n",
      "Epoch [14/100], Loss: 15.0460\n",
      "Epoch [15/100], Loss: 15.0048\n",
      "Epoch [16/100], Loss: 14.9556\n",
      "Epoch [17/100], Loss: 14.9425\n",
      "Epoch [18/100], Loss: 14.9061\n",
      "Epoch [19/100], Loss: 14.9003\n",
      "Epoch [20/100], Loss: 14.8724\n",
      "Epoch [21/100], Loss: 14.8549\n",
      "Epoch [22/100], Loss: 14.8445\n",
      "Epoch [23/100], Loss: 14.8059\n",
      "Epoch [24/100], Loss: 14.8018\n",
      "Epoch [25/100], Loss: 14.7791\n",
      "Epoch [26/100], Loss: 14.7691\n",
      "Epoch [27/100], Loss: 14.7603\n",
      "Epoch [28/100], Loss: 14.7451\n",
      "Epoch [29/100], Loss: 14.7422\n",
      "Epoch [30/100], Loss: 14.7196\n",
      "Epoch [31/100], Loss: 14.7156\n",
      "Epoch [32/100], Loss: 14.7174\n",
      "Epoch [33/100], Loss: 14.6942\n",
      "Epoch [34/100], Loss: 14.6822\n",
      "Epoch [35/100], Loss: 14.6691\n",
      "Epoch [36/100], Loss: 14.6527\n",
      "Epoch [37/100], Loss: 14.6466\n",
      "Epoch [38/100], Loss: 14.6489\n",
      "Epoch [39/100], Loss: 14.6411\n",
      "Epoch [40/100], Loss: 14.6450\n",
      "Epoch [41/100], Loss: 14.6266\n",
      "Epoch [42/100], Loss: 14.6155\n",
      "Epoch [43/100], Loss: 14.5840\n",
      "Epoch [44/100], Loss: 14.6054\n",
      "Epoch [45/100], Loss: 14.5847\n",
      "Epoch [46/100], Loss: 14.5904\n",
      "Epoch [47/100], Loss: 14.5896\n",
      "Epoch [48/100], Loss: 14.5956\n",
      "Stopping early at epoch 48 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Base Logit K Means: 55.69%\n",
      "Epoch [1/100], Loss: 12.0447\n",
      "Epoch [2/100], Loss: 9.8982\n",
      "Epoch [3/100], Loss: 9.5472\n",
      "Epoch [4/100], Loss: 9.3660\n",
      "Epoch [5/100], Loss: 9.2371\n",
      "Epoch [6/100], Loss: 9.1268\n",
      "Epoch [7/100], Loss: 9.0511\n",
      "Epoch [8/100], Loss: 8.9989\n",
      "Epoch [9/100], Loss: 8.9337\n",
      "Epoch [10/100], Loss: 8.9001\n",
      "Epoch [11/100], Loss: 8.8606\n",
      "Epoch [12/100], Loss: 8.8115\n",
      "Epoch [13/100], Loss: 8.7724\n",
      "Epoch [14/100], Loss: 8.7464\n",
      "Epoch [15/100], Loss: 8.7181\n",
      "Epoch [16/100], Loss: 8.6915\n",
      "Epoch [17/100], Loss: 8.6734\n",
      "Epoch [18/100], Loss: 8.6521\n",
      "Epoch [19/100], Loss: 8.6362\n",
      "Epoch [20/100], Loss: 8.5900\n",
      "Epoch [21/100], Loss: 8.5885\n",
      "Epoch [22/100], Loss: 8.5715\n",
      "Epoch [23/100], Loss: 8.5556\n",
      "Epoch [24/100], Loss: 8.5495\n",
      "Epoch [25/100], Loss: 8.5295\n",
      "Epoch [26/100], Loss: 8.5187\n",
      "Epoch [27/100], Loss: 8.4981\n",
      "Epoch [28/100], Loss: 8.4847\n",
      "Epoch [29/100], Loss: 8.4736\n",
      "Epoch [30/100], Loss: 8.4472\n",
      "Epoch [31/100], Loss: 8.4407\n",
      "Epoch [32/100], Loss: 8.4237\n",
      "Epoch [33/100], Loss: 8.4295\n",
      "Epoch [34/100], Loss: 8.4054\n",
      "Epoch [35/100], Loss: 8.3888\n",
      "Epoch [36/100], Loss: 8.3742\n",
      "Epoch [37/100], Loss: 8.3645\n",
      "Epoch [38/100], Loss: 8.3787\n",
      "Epoch [39/100], Loss: 8.3470\n",
      "Epoch [40/100], Loss: 8.3466\n",
      "Epoch [41/100], Loss: 8.3356\n",
      "Epoch [42/100], Loss: 8.3471\n",
      "Epoch [43/100], Loss: 8.3391\n",
      "Epoch [44/100], Loss: 8.3208\n",
      "Epoch [45/100], Loss: 8.3229\n",
      "Epoch [46/100], Loss: 8.2876\n",
      "Epoch [47/100], Loss: 8.2843\n",
      "Epoch [48/100], Loss: 8.2900\n",
      "Epoch [49/100], Loss: 8.2760\n",
      "Epoch [50/100], Loss: 8.2728\n",
      "Epoch [51/100], Loss: 8.2791\n",
      "Epoch [52/100], Loss: 8.2591\n",
      "Epoch [53/100], Loss: 8.2774\n",
      "Epoch [54/100], Loss: 8.2463\n",
      "Epoch [55/100], Loss: 8.2534\n",
      "Epoch [56/100], Loss: 8.2466\n",
      "Epoch [57/100], Loss: 8.2471\n",
      "Epoch [58/100], Loss: 8.2326\n",
      "Epoch [59/100], Loss: 8.2169\n",
      "Epoch [60/100], Loss: 8.2173\n",
      "Epoch [61/100], Loss: 8.2133\n",
      "Epoch [62/100], Loss: 8.2283\n",
      "Epoch [63/100], Loss: 8.2082\n",
      "Epoch [64/100], Loss: 8.1912\n",
      "Epoch [65/100], Loss: 8.1938\n",
      "Epoch [66/100], Loss: 8.1766\n",
      "Epoch [67/100], Loss: 8.1986\n",
      "Epoch [68/100], Loss: 8.1802\n",
      "Epoch [69/100], Loss: 8.1725\n",
      "Epoch [70/100], Loss: 8.1606\n",
      "Epoch [71/100], Loss: 8.1688\n",
      "Epoch [72/100], Loss: 8.1459\n",
      "Epoch [73/100], Loss: 8.1637\n",
      "Epoch [74/100], Loss: 8.1440\n",
      "Epoch [75/100], Loss: 8.1482\n",
      "Epoch [76/100], Loss: 8.1523\n",
      "Epoch [77/100], Loss: 8.1502\n",
      "Epoch [78/100], Loss: 8.1362\n",
      "Epoch [79/100], Loss: 8.1275\n",
      "Epoch [80/100], Loss: 8.1316\n",
      "Epoch [81/100], Loss: 8.1332\n",
      "Epoch [82/100], Loss: 8.1153\n",
      "Epoch [83/100], Loss: 8.1166\n",
      "Epoch [84/100], Loss: 8.1173\n",
      "Epoch [85/100], Loss: 8.1080\n",
      "Epoch [86/100], Loss: 8.1090\n",
      "Epoch [87/100], Loss: 8.1234\n",
      "Epoch [88/100], Loss: 8.1115\n",
      "Epoch [89/100], Loss: 8.1028\n",
      "Epoch [90/100], Loss: 8.1251\n",
      "Epoch [91/100], Loss: 8.1001\n",
      "Epoch [92/100], Loss: 8.0879\n",
      "Epoch [93/100], Loss: 8.1007\n",
      "Epoch [94/100], Loss: 8.0854\n",
      "Epoch [95/100], Loss: 8.0768\n",
      "Epoch [96/100], Loss: 8.0920\n",
      "Epoch [97/100], Loss: 8.0914\n",
      "Epoch [98/100], Loss: 8.0817\n",
      "Epoch [99/100], Loss: 8.0780\n",
      "Epoch [100/100], Loss: 8.0734\n",
      "Test Accuracy Base Logit K Means: 54.18%\n",
      "Epoch [1/100], Loss: 8.4522\n",
      "Epoch [2/100], Loss: 6.5961\n",
      "Epoch [3/100], Loss: 6.2651\n",
      "Epoch [4/100], Loss: 6.1199\n",
      "Epoch [5/100], Loss: 6.0055\n",
      "Epoch [6/100], Loss: 5.9207\n",
      "Epoch [7/100], Loss: 5.8586\n",
      "Epoch [8/100], Loss: 5.7856\n",
      "Epoch [9/100], Loss: 5.7572\n",
      "Epoch [10/100], Loss: 5.7182\n",
      "Epoch [11/100], Loss: 5.6651\n",
      "Epoch [12/100], Loss: 5.6300\n",
      "Epoch [13/100], Loss: 5.6179\n",
      "Epoch [14/100], Loss: 5.5757\n",
      "Epoch [15/100], Loss: 5.5499\n",
      "Epoch [16/100], Loss: 5.5354\n",
      "Epoch [17/100], Loss: 5.5046\n",
      "Epoch [18/100], Loss: 5.4871\n",
      "Epoch [19/100], Loss: 5.4682\n",
      "Epoch [20/100], Loss: 5.4526\n",
      "Epoch [21/100], Loss: 5.4322\n",
      "Epoch [22/100], Loss: 5.4097\n",
      "Epoch [23/100], Loss: 5.3919\n",
      "Epoch [24/100], Loss: 5.3904\n",
      "Epoch [25/100], Loss: 5.3696\n",
      "Epoch [26/100], Loss: 5.3575\n",
      "Epoch [27/100], Loss: 5.3336\n",
      "Epoch [28/100], Loss: 5.3287\n",
      "Epoch [29/100], Loss: 5.3180\n",
      "Epoch [30/100], Loss: 5.3105\n",
      "Epoch [31/100], Loss: 5.2994\n",
      "Epoch [32/100], Loss: 5.2727\n",
      "Epoch [33/100], Loss: 5.2567\n",
      "Epoch [34/100], Loss: 5.2702\n",
      "Epoch [35/100], Loss: 5.2459\n",
      "Epoch [36/100], Loss: 5.2457\n",
      "Epoch [37/100], Loss: 5.2266\n",
      "Epoch [38/100], Loss: 5.2215\n",
      "Epoch [39/100], Loss: 5.2089\n",
      "Epoch [40/100], Loss: 5.2023\n",
      "Epoch [41/100], Loss: 5.1943\n",
      "Epoch [42/100], Loss: 5.1854\n",
      "Epoch [43/100], Loss: 5.1865\n",
      "Epoch [44/100], Loss: 5.1701\n",
      "Epoch [45/100], Loss: 5.1652\n",
      "Epoch [46/100], Loss: 5.1536\n",
      "Epoch [47/100], Loss: 5.1411\n",
      "Epoch [48/100], Loss: 5.1463\n",
      "Epoch [49/100], Loss: 5.1338\n",
      "Epoch [50/100], Loss: 5.1245\n",
      "Epoch [51/100], Loss: 5.1195\n",
      "Epoch [52/100], Loss: 5.1133\n",
      "Epoch [53/100], Loss: 5.1217\n",
      "Epoch [54/100], Loss: 5.1077\n",
      "Epoch [55/100], Loss: 5.0950\n",
      "Epoch [56/100], Loss: 5.0937\n",
      "Epoch [57/100], Loss: 5.0855\n",
      "Epoch [58/100], Loss: 5.0789\n",
      "Epoch [59/100], Loss: 5.0763\n",
      "Epoch [60/100], Loss: 5.0709\n",
      "Epoch [61/100], Loss: 5.0646\n",
      "Epoch [62/100], Loss: 5.0614\n",
      "Epoch [63/100], Loss: 5.0464\n",
      "Epoch [64/100], Loss: 5.0488\n",
      "Epoch [65/100], Loss: 5.0406\n",
      "Epoch [66/100], Loss: 5.0379\n",
      "Epoch [67/100], Loss: 5.0310\n",
      "Epoch [68/100], Loss: 5.0306\n",
      "Epoch [69/100], Loss: 5.0184\n",
      "Epoch [70/100], Loss: 5.0182\n",
      "Epoch [71/100], Loss: 5.0243\n",
      "Epoch [72/100], Loss: 5.0089\n",
      "Epoch [73/100], Loss: 5.0113\n",
      "Epoch [74/100], Loss: 4.9997\n",
      "Epoch [75/100], Loss: 4.9967\n",
      "Epoch [76/100], Loss: 4.9981\n",
      "Epoch [77/100], Loss: 4.9960\n",
      "Epoch [78/100], Loss: 4.9880\n",
      "Epoch [79/100], Loss: 4.9783\n",
      "Epoch [80/100], Loss: 4.9742\n",
      "Epoch [81/100], Loss: 4.9787\n",
      "Epoch [82/100], Loss: 4.9610\n",
      "Epoch [83/100], Loss: 4.9625\n",
      "Epoch [84/100], Loss: 4.9695\n",
      "Epoch [85/100], Loss: 4.9587\n",
      "Epoch [86/100], Loss: 4.9543\n",
      "Epoch [87/100], Loss: 4.9571\n",
      "Epoch [88/100], Loss: 4.9473\n",
      "Epoch [89/100], Loss: 4.9432\n",
      "Epoch [90/100], Loss: 4.9356\n",
      "Epoch [91/100], Loss: 4.9353\n",
      "Epoch [92/100], Loss: 4.9387\n",
      "Epoch [93/100], Loss: 4.9307\n",
      "Epoch [94/100], Loss: 4.9335\n",
      "Epoch [95/100], Loss: 4.9335\n",
      "Epoch [96/100], Loss: 4.9242\n",
      "Epoch [97/100], Loss: 4.9167\n",
      "Epoch [98/100], Loss: 4.9166\n",
      "Epoch [99/100], Loss: 4.9109\n",
      "Epoch [100/100], Loss: 4.9194\n",
      "Test Accuracy Base Logit K Means: 52.98%\n",
      "Epoch [1/100], Loss: 2.4153\n",
      "Epoch [2/100], Loss: 1.5296\n",
      "Epoch [3/100], Loss: 1.3303\n",
      "Epoch [4/100], Loss: 1.2441\n",
      "Epoch [5/100], Loss: 1.1719\n",
      "Epoch [6/100], Loss: 1.1351\n",
      "Epoch [7/100], Loss: 1.0909\n",
      "Epoch [8/100], Loss: 1.0592\n",
      "Epoch [9/100], Loss: 1.0314\n",
      "Epoch [10/100], Loss: 1.0178\n",
      "Epoch [11/100], Loss: 0.9984\n",
      "Epoch [12/100], Loss: 0.9726\n",
      "Epoch [13/100], Loss: 0.9672\n",
      "Epoch [14/100], Loss: 0.9591\n",
      "Epoch [15/100], Loss: 0.9376\n",
      "Epoch [16/100], Loss: 0.9282\n",
      "Epoch [17/100], Loss: 0.9228\n",
      "Epoch [18/100], Loss: 0.9069\n",
      "Epoch [19/100], Loss: 0.8987\n",
      "Epoch [20/100], Loss: 0.8936\n",
      "Epoch [21/100], Loss: 0.8781\n",
      "Epoch [22/100], Loss: 0.8694\n",
      "Epoch [23/100], Loss: 0.8660\n",
      "Epoch [24/100], Loss: 0.8585\n",
      "Epoch [25/100], Loss: 0.8584\n",
      "Epoch [26/100], Loss: 0.8470\n",
      "Epoch [27/100], Loss: 0.8357\n",
      "Epoch [28/100], Loss: 0.8530\n",
      "Epoch [29/100], Loss: 0.8312\n",
      "Epoch [30/100], Loss: 0.8274\n",
      "Epoch [31/100], Loss: 0.8179\n",
      "Epoch [32/100], Loss: 0.8077\n",
      "Epoch [33/100], Loss: 0.8070\n",
      "Epoch [34/100], Loss: 0.8022\n",
      "Epoch [35/100], Loss: 0.7985\n",
      "Epoch [36/100], Loss: 0.7960\n",
      "Epoch [37/100], Loss: 0.7862\n",
      "Epoch [38/100], Loss: 0.7891\n",
      "Epoch [39/100], Loss: 0.7839\n",
      "Epoch [40/100], Loss: 0.7714\n",
      "Epoch [41/100], Loss: 0.7716\n",
      "Epoch [42/100], Loss: 0.7653\n",
      "Epoch [43/100], Loss: 0.7684\n",
      "Epoch [44/100], Loss: 0.7637\n",
      "Epoch [45/100], Loss: 0.7615\n",
      "Epoch [46/100], Loss: 0.7487\n",
      "Epoch [47/100], Loss: 0.7486\n",
      "Epoch [48/100], Loss: 0.7447\n",
      "Epoch [49/100], Loss: 0.7443\n",
      "Epoch [50/100], Loss: 0.7336\n",
      "Epoch [51/100], Loss: 0.7365\n",
      "Epoch [52/100], Loss: 0.7320\n",
      "Epoch [53/100], Loss: 0.7317\n",
      "Epoch [54/100], Loss: 0.7325\n",
      "Epoch [55/100], Loss: 0.7361\n",
      "Stopping early at epoch 55 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Base Logit K Means: 52.45%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "base_logit_kmeans = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"kmnist_centroids/kmnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"kmnist_centroids/kmnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    logit_model = LogisticRegression(input_dim, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(logit_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "    previous_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logit_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = logit_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if previous_loss - current_loss < tolerance:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "                break\n",
    "        else:\n",
    "            epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "        previous_loss = current_loss\n",
    "\n",
    "    logit_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 28*28)\n",
    "            outputs = logit_model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy Base Logit K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    base_logit_kmeans[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>125000</th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.687862</td>\n",
       "      <td>54.175422</td>\n",
       "      <td>52.984668</td>\n",
       "      <td>52.450256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      125000     75000      50000      10000 \n",
       "0  55.687862  54.175422  52.984668  52.450256"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_logit_kmeans_df = pd.DataFrame(base_logit_kmeans)\n",
    "base_logit_kmeans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipschitz Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 18.9393\n",
      "Epoch [2/100], Loss: 16.3279\n",
      "Epoch [3/100], Loss: 15.9129\n",
      "Epoch [4/100], Loss: 15.7286\n",
      "Epoch [5/100], Loss: 15.5626\n",
      "Epoch [6/100], Loss: 15.4461\n",
      "Epoch [7/100], Loss: 15.3590\n",
      "Epoch [8/100], Loss: 15.2751\n",
      "Epoch [9/100], Loss: 15.2094\n",
      "Epoch [10/100], Loss: 15.1794\n",
      "Epoch [11/100], Loss: 15.1414\n",
      "Epoch [12/100], Loss: 15.0781\n",
      "Epoch [13/100], Loss: 15.0762\n",
      "Epoch [14/100], Loss: 15.0515\n",
      "Epoch [15/100], Loss: 14.9777\n",
      "Epoch [16/100], Loss: 14.9454\n",
      "Epoch [17/100], Loss: 14.9477\n",
      "Epoch [18/100], Loss: 14.9341\n",
      "Epoch [19/100], Loss: 14.9012\n",
      "Epoch [20/100], Loss: 14.8855\n",
      "Epoch [21/100], Loss: 14.8505\n",
      "Epoch [22/100], Loss: 14.8307\n",
      "Epoch [23/100], Loss: 14.8236\n",
      "Epoch [24/100], Loss: 14.7988\n",
      "Epoch [25/100], Loss: 14.8006\n",
      "Epoch [26/100], Loss: 14.7672\n",
      "Epoch [27/100], Loss: 14.7775\n",
      "Epoch [28/100], Loss: 14.7523\n",
      "Epoch [29/100], Loss: 14.7426\n",
      "Epoch [30/100], Loss: 14.7055\n",
      "Epoch [31/100], Loss: 14.7215\n",
      "Epoch [32/100], Loss: 14.7012\n",
      "Epoch [33/100], Loss: 14.6885\n",
      "Epoch [34/100], Loss: 14.6758\n",
      "Epoch [35/100], Loss: 14.6580\n",
      "Epoch [36/100], Loss: 14.6687\n",
      "Epoch [37/100], Loss: 14.6703\n",
      "Epoch [38/100], Loss: 14.6586\n",
      "Epoch [39/100], Loss: 14.6563\n",
      "Epoch [40/100], Loss: 14.6278\n",
      "Epoch [41/100], Loss: 14.6392\n",
      "Epoch [42/100], Loss: 14.6049\n",
      "Epoch [43/100], Loss: 14.6058\n",
      "Epoch [44/100], Loss: 14.6088\n",
      "Epoch [45/100], Loss: 14.5972\n",
      "Epoch [46/100], Loss: 14.6141\n",
      "Epoch [47/100], Loss: 14.5853\n",
      "Epoch [48/100], Loss: 14.5729\n",
      "Epoch [49/100], Loss: 14.5754\n",
      "Epoch [50/100], Loss: 14.5694\n",
      "Epoch [51/100], Loss: 14.5568\n",
      "Epoch [52/100], Loss: 14.5690\n",
      "Epoch [53/100], Loss: 14.5588\n",
      "Epoch [54/100], Loss: 14.5582\n",
      "Epoch [55/100], Loss: 14.5567\n",
      "Epoch [56/100], Loss: 14.5444\n",
      "Epoch [57/100], Loss: 14.5438\n",
      "Epoch [58/100], Loss: 14.5522\n",
      "Epoch [59/100], Loss: 14.5271\n",
      "Epoch [60/100], Loss: 14.5230\n",
      "Epoch [61/100], Loss: 14.5164\n",
      "Epoch [62/100], Loss: 14.5198\n",
      "Epoch [63/100], Loss: 14.5020\n",
      "Epoch [64/100], Loss: 14.5280\n",
      "Epoch [65/100], Loss: 14.5009\n",
      "Epoch [66/100], Loss: 14.4964\n",
      "Epoch [67/100], Loss: 14.5113\n",
      "Epoch [68/100], Loss: 14.4886\n",
      "Epoch [69/100], Loss: 14.4939\n",
      "Epoch [70/100], Loss: 14.4956\n",
      "Epoch [71/100], Loss: 14.4743\n",
      "Epoch [72/100], Loss: 14.5030\n",
      "Epoch [73/100], Loss: 14.4868\n",
      "Epoch [74/100], Loss: 14.4741\n",
      "Epoch [75/100], Loss: 14.4769\n",
      "Epoch [76/100], Loss: 14.4820\n",
      "Epoch [77/100], Loss: 14.4786\n",
      "Epoch [78/100], Loss: 14.4665\n",
      "Epoch [79/100], Loss: 14.4512\n",
      "Epoch [80/100], Loss: 14.4641\n",
      "Epoch [81/100], Loss: 14.4523\n",
      "Epoch [82/100], Loss: 14.4758\n",
      "Epoch [83/100], Loss: 14.4431\n",
      "Epoch [84/100], Loss: 14.4628\n",
      "Epoch [85/100], Loss: 14.4575\n",
      "Epoch [86/100], Loss: 14.4617\n",
      "Epoch [87/100], Loss: 14.4523\n",
      "Epoch [88/100], Loss: 14.4248\n",
      "Epoch [89/100], Loss: 14.4249\n",
      "Epoch [90/100], Loss: 14.4434\n",
      "Epoch [91/100], Loss: 14.4363\n",
      "Epoch [92/100], Loss: 14.4358\n",
      "Epoch [93/100], Loss: 14.4241\n",
      "Epoch [94/100], Loss: 14.4180\n",
      "Epoch [95/100], Loss: 14.4251\n",
      "Epoch [96/100], Loss: 14.4202\n",
      "Epoch [97/100], Loss: 14.4352\n",
      "Epoch [98/100], Loss: 14.4168\n",
      "Epoch [99/100], Loss: 14.4216\n",
      "Epoch [100/100], Loss: 14.4239\n",
      "Test Accuracy Lipschitz Logit K Means: 55.18%\n",
      "Epoch [1/100], Loss: 12.0478\n",
      "Epoch [2/100], Loss: 9.8929\n",
      "Epoch [3/100], Loss: 9.5594\n",
      "Epoch [4/100], Loss: 9.3727\n",
      "Epoch [5/100], Loss: 9.2231\n",
      "Epoch [6/100], Loss: 9.1327\n",
      "Epoch [7/100], Loss: 9.0359\n",
      "Epoch [8/100], Loss: 8.9918\n",
      "Epoch [9/100], Loss: 8.9397\n",
      "Epoch [10/100], Loss: 8.8922\n",
      "Epoch [11/100], Loss: 8.8548\n",
      "Epoch [12/100], Loss: 8.8053\n",
      "Epoch [13/100], Loss: 8.7828\n",
      "Epoch [14/100], Loss: 8.7514\n",
      "Epoch [15/100], Loss: 8.7209\n",
      "Epoch [16/100], Loss: 8.6991\n",
      "Epoch [17/100], Loss: 8.6638\n",
      "Epoch [18/100], Loss: 8.6517\n",
      "Epoch [19/100], Loss: 8.6167\n",
      "Epoch [20/100], Loss: 8.5951\n",
      "Epoch [21/100], Loss: 8.6055\n",
      "Epoch [22/100], Loss: 8.5706\n",
      "Epoch [23/100], Loss: 8.5596\n",
      "Epoch [24/100], Loss: 8.5480\n",
      "Epoch [25/100], Loss: 8.5300\n",
      "Epoch [26/100], Loss: 8.4986\n",
      "Epoch [27/100], Loss: 8.4792\n",
      "Epoch [28/100], Loss: 8.4689\n",
      "Epoch [29/100], Loss: 8.4711\n",
      "Epoch [30/100], Loss: 8.4418\n",
      "Epoch [31/100], Loss: 8.4365\n",
      "Epoch [32/100], Loss: 8.4274\n",
      "Epoch [33/100], Loss: 8.4175\n",
      "Epoch [34/100], Loss: 8.4085\n",
      "Epoch [35/100], Loss: 8.3963\n",
      "Epoch [36/100], Loss: 8.4051\n",
      "Epoch [37/100], Loss: 8.3841\n",
      "Epoch [38/100], Loss: 8.3715\n",
      "Epoch [39/100], Loss: 8.3573\n",
      "Epoch [40/100], Loss: 8.3518\n",
      "Epoch [41/100], Loss: 8.3371\n",
      "Epoch [42/100], Loss: 8.3381\n",
      "Epoch [43/100], Loss: 8.3383\n",
      "Epoch [44/100], Loss: 8.3105\n",
      "Epoch [45/100], Loss: 8.3107\n",
      "Epoch [46/100], Loss: 8.2954\n",
      "Epoch [47/100], Loss: 8.3031\n",
      "Epoch [48/100], Loss: 8.2837\n",
      "Epoch [49/100], Loss: 8.2886\n",
      "Epoch [50/100], Loss: 8.2693\n",
      "Epoch [51/100], Loss: 8.2640\n",
      "Epoch [52/100], Loss: 8.2676\n",
      "Epoch [53/100], Loss: 8.2501\n",
      "Epoch [54/100], Loss: 8.2640\n",
      "Epoch [55/100], Loss: 8.2437\n",
      "Epoch [56/100], Loss: 8.2324\n",
      "Epoch [57/100], Loss: 8.2182\n",
      "Epoch [58/100], Loss: 8.2150\n",
      "Epoch [59/100], Loss: 8.2257\n",
      "Epoch [60/100], Loss: 8.2207\n",
      "Epoch [61/100], Loss: 8.2085\n",
      "Epoch [62/100], Loss: 8.2168\n",
      "Epoch [63/100], Loss: 8.1951\n",
      "Epoch [64/100], Loss: 8.1825\n",
      "Epoch [65/100], Loss: 8.1918\n",
      "Epoch [66/100], Loss: 8.1979\n",
      "Epoch [67/100], Loss: 8.1817\n",
      "Epoch [68/100], Loss: 8.1799\n",
      "Epoch [69/100], Loss: 8.1839\n",
      "Epoch [70/100], Loss: 8.1642\n",
      "Epoch [71/100], Loss: 8.1584\n",
      "Epoch [72/100], Loss: 8.1625\n",
      "Epoch [73/100], Loss: 8.1570\n",
      "Epoch [74/100], Loss: 8.1545\n",
      "Epoch [75/100], Loss: 8.1518\n",
      "Epoch [76/100], Loss: 8.1329\n",
      "Epoch [77/100], Loss: 8.1390\n",
      "Epoch [78/100], Loss: 8.1505\n",
      "Epoch [79/100], Loss: 8.1294\n",
      "Epoch [80/100], Loss: 8.1420\n",
      "Epoch [81/100], Loss: 8.1337\n",
      "Epoch [82/100], Loss: 8.1221\n",
      "Epoch [83/100], Loss: 8.1222\n",
      "Epoch [84/100], Loss: 8.1114\n",
      "Epoch [85/100], Loss: 8.1165\n",
      "Epoch [86/100], Loss: 8.1328\n",
      "Epoch [87/100], Loss: 8.1086\n",
      "Epoch [88/100], Loss: 8.1027\n",
      "Epoch [89/100], Loss: 8.1076\n",
      "Epoch [90/100], Loss: 8.1063\n",
      "Epoch [91/100], Loss: 8.1002\n",
      "Epoch [92/100], Loss: 8.0874\n",
      "Epoch [93/100], Loss: 8.0750\n",
      "Epoch [94/100], Loss: 8.0903\n",
      "Epoch [95/100], Loss: 8.0818\n",
      "Epoch [96/100], Loss: 8.0766\n",
      "Epoch [97/100], Loss: 8.0837\n",
      "Epoch [98/100], Loss: 8.0790\n",
      "Epoch [99/100], Loss: 8.0773\n",
      "Epoch [100/100], Loss: 8.0724\n",
      "Test Accuracy Lipschitz Logit K Means: 54.23%\n",
      "Epoch [1/100], Loss: 8.4584\n",
      "Epoch [2/100], Loss: 6.5919\n",
      "Epoch [3/100], Loss: 6.2776\n",
      "Epoch [4/100], Loss: 6.1185\n",
      "Epoch [5/100], Loss: 5.9996\n",
      "Epoch [6/100], Loss: 5.9169\n",
      "Epoch [7/100], Loss: 5.8702\n",
      "Epoch [8/100], Loss: 5.7989\n",
      "Epoch [9/100], Loss: 5.7426\n",
      "Epoch [10/100], Loss: 5.7005\n",
      "Epoch [11/100], Loss: 5.6830\n",
      "Epoch [12/100], Loss: 5.6297\n",
      "Epoch [13/100], Loss: 5.5969\n",
      "Epoch [14/100], Loss: 5.5799\n",
      "Epoch [15/100], Loss: 5.5449\n",
      "Epoch [16/100], Loss: 5.5287\n",
      "Epoch [17/100], Loss: 5.5077\n",
      "Epoch [18/100], Loss: 5.4913\n",
      "Epoch [19/100], Loss: 5.4770\n",
      "Epoch [20/100], Loss: 5.4451\n",
      "Epoch [21/100], Loss: 5.4317\n",
      "Epoch [22/100], Loss: 5.4112\n",
      "Epoch [23/100], Loss: 5.3973\n",
      "Epoch [24/100], Loss: 5.3933\n",
      "Epoch [25/100], Loss: 5.3657\n",
      "Epoch [26/100], Loss: 5.3496\n",
      "Epoch [27/100], Loss: 5.3308\n",
      "Epoch [28/100], Loss: 5.3312\n",
      "Epoch [29/100], Loss: 5.3265\n",
      "Epoch [30/100], Loss: 5.3045\n",
      "Epoch [31/100], Loss: 5.3075\n",
      "Epoch [32/100], Loss: 5.2750\n",
      "Epoch [33/100], Loss: 5.2739\n",
      "Epoch [34/100], Loss: 5.2591\n",
      "Epoch [35/100], Loss: 5.2590\n",
      "Epoch [36/100], Loss: 5.2404\n",
      "Epoch [37/100], Loss: 5.2295\n",
      "Epoch [38/100], Loss: 5.2296\n",
      "Epoch [39/100], Loss: 5.2116\n",
      "Epoch [40/100], Loss: 5.2014\n",
      "Epoch [41/100], Loss: 5.1852\n",
      "Epoch [42/100], Loss: 5.1993\n",
      "Epoch [43/100], Loss: 5.1866\n",
      "Epoch [44/100], Loss: 5.1636\n",
      "Epoch [45/100], Loss: 5.1671\n",
      "Epoch [46/100], Loss: 5.1631\n",
      "Epoch [47/100], Loss: 5.1542\n",
      "Epoch [48/100], Loss: 5.1324\n",
      "Epoch [49/100], Loss: 5.1254\n",
      "Epoch [50/100], Loss: 5.1099\n",
      "Epoch [51/100], Loss: 5.1194\n",
      "Epoch [52/100], Loss: 5.1228\n",
      "Epoch [53/100], Loss: 5.1137\n",
      "Epoch [54/100], Loss: 5.1070\n",
      "Epoch [55/100], Loss: 5.1008\n",
      "Epoch [56/100], Loss: 5.0913\n",
      "Epoch [57/100], Loss: 5.0923\n",
      "Epoch [58/100], Loss: 5.0693\n",
      "Epoch [59/100], Loss: 5.0844\n",
      "Epoch [60/100], Loss: 5.0583\n",
      "Epoch [61/100], Loss: 5.0682\n",
      "Epoch [62/100], Loss: 5.0615\n",
      "Epoch [63/100], Loss: 5.0549\n",
      "Epoch [64/100], Loss: 5.0514\n",
      "Epoch [65/100], Loss: 5.0393\n",
      "Epoch [66/100], Loss: 5.0330\n",
      "Epoch [67/100], Loss: 5.0307\n",
      "Epoch [68/100], Loss: 5.0201\n",
      "Epoch [69/100], Loss: 5.0152\n",
      "Epoch [70/100], Loss: 5.0237\n",
      "Epoch [71/100], Loss: 5.0228\n",
      "Epoch [72/100], Loss: 5.0065\n",
      "Epoch [73/100], Loss: 5.0099\n",
      "Epoch [74/100], Loss: 5.0009\n",
      "Epoch [75/100], Loss: 4.9941\n",
      "Epoch [76/100], Loss: 4.9944\n",
      "Epoch [77/100], Loss: 4.9948\n",
      "Epoch [78/100], Loss: 4.9867\n",
      "Epoch [79/100], Loss: 4.9861\n",
      "Epoch [80/100], Loss: 4.9802\n",
      "Epoch [81/100], Loss: 4.9798\n",
      "Epoch [82/100], Loss: 4.9753\n",
      "Epoch [83/100], Loss: 4.9696\n",
      "Epoch [84/100], Loss: 4.9621\n",
      "Epoch [85/100], Loss: 4.9472\n",
      "Epoch [86/100], Loss: 4.9635\n",
      "Epoch [87/100], Loss: 4.9532\n",
      "Epoch [88/100], Loss: 4.9505\n",
      "Epoch [89/100], Loss: 4.9510\n",
      "Epoch [90/100], Loss: 4.9431\n",
      "Epoch [91/100], Loss: 4.9375\n",
      "Epoch [92/100], Loss: 4.9339\n",
      "Epoch [93/100], Loss: 4.9269\n",
      "Epoch [94/100], Loss: 4.9368\n",
      "Epoch [95/100], Loss: 4.9343\n",
      "Epoch [96/100], Loss: 4.9355\n",
      "Epoch [97/100], Loss: 4.9400\n",
      "Epoch [98/100], Loss: 4.9130\n",
      "Epoch [99/100], Loss: 4.9154\n",
      "Epoch [100/100], Loss: 4.9128\n",
      "Test Accuracy Lipschitz Logit K Means: 53.49%\n",
      "Epoch [1/100], Loss: 2.4020\n",
      "Epoch [2/100], Loss: 1.5172\n",
      "Epoch [3/100], Loss: 1.3344\n",
      "Epoch [4/100], Loss: 1.2468\n",
      "Epoch [5/100], Loss: 1.1809\n",
      "Epoch [6/100], Loss: 1.1287\n",
      "Epoch [7/100], Loss: 1.0890\n",
      "Epoch [8/100], Loss: 1.0634\n",
      "Epoch [9/100], Loss: 1.0469\n",
      "Epoch [10/100], Loss: 1.0135\n",
      "Epoch [11/100], Loss: 1.0068\n",
      "Epoch [12/100], Loss: 0.9845\n",
      "Epoch [13/100], Loss: 0.9662\n",
      "Epoch [14/100], Loss: 0.9548\n",
      "Epoch [15/100], Loss: 0.9436\n",
      "Epoch [16/100], Loss: 0.9266\n",
      "Epoch [17/100], Loss: 0.9161\n",
      "Epoch [18/100], Loss: 0.9089\n",
      "Epoch [19/100], Loss: 0.9020\n",
      "Epoch [20/100], Loss: 0.8869\n",
      "Epoch [21/100], Loss: 0.8766\n",
      "Epoch [22/100], Loss: 0.8750\n",
      "Epoch [23/100], Loss: 0.8634\n",
      "Epoch [24/100], Loss: 0.8560\n",
      "Epoch [25/100], Loss: 0.8522\n",
      "Epoch [26/100], Loss: 0.8511\n",
      "Epoch [27/100], Loss: 0.8353\n",
      "Epoch [28/100], Loss: 0.8299\n",
      "Epoch [29/100], Loss: 0.8304\n",
      "Epoch [30/100], Loss: 0.8173\n",
      "Epoch [31/100], Loss: 0.8191\n",
      "Epoch [32/100], Loss: 0.8137\n",
      "Epoch [33/100], Loss: 0.8093\n",
      "Epoch [34/100], Loss: 0.7975\n",
      "Epoch [35/100], Loss: 0.8020\n",
      "Epoch [36/100], Loss: 0.7963\n",
      "Epoch [37/100], Loss: 0.7863\n",
      "Epoch [38/100], Loss: 0.7882\n",
      "Epoch [39/100], Loss: 0.7798\n",
      "Epoch [40/100], Loss: 0.7755\n",
      "Epoch [41/100], Loss: 0.7731\n",
      "Epoch [42/100], Loss: 0.7716\n",
      "Epoch [43/100], Loss: 0.7665\n",
      "Epoch [44/100], Loss: 0.7574\n",
      "Epoch [45/100], Loss: 0.7579\n",
      "Epoch [46/100], Loss: 0.7532\n",
      "Epoch [47/100], Loss: 0.7449\n",
      "Epoch [48/100], Loss: 0.7386\n",
      "Epoch [49/100], Loss: 0.7444\n",
      "Epoch [50/100], Loss: 0.7385\n",
      "Epoch [51/100], Loss: 0.7324\n",
      "Epoch [52/100], Loss: 0.7353\n",
      "Epoch [53/100], Loss: 0.7251\n",
      "Epoch [54/100], Loss: 0.7249\n",
      "Epoch [55/100], Loss: 0.7266\n",
      "Epoch [56/100], Loss: 0.7197\n",
      "Epoch [57/100], Loss: 0.7180\n",
      "Epoch [58/100], Loss: 0.7198\n",
      "Epoch [59/100], Loss: 0.7117\n",
      "Epoch [60/100], Loss: 0.7062\n",
      "Epoch [61/100], Loss: 0.7045\n",
      "Epoch [62/100], Loss: 0.7042\n",
      "Epoch [63/100], Loss: 0.7057\n",
      "Epoch [64/100], Loss: 0.7034\n",
      "Epoch [65/100], Loss: 0.6967\n",
      "Epoch [66/100], Loss: 0.6949\n",
      "Epoch [67/100], Loss: 0.6913\n",
      "Epoch [68/100], Loss: 0.6932\n",
      "Epoch [69/100], Loss: 0.6962\n",
      "Epoch [70/100], Loss: 0.6906\n",
      "Epoch [71/100], Loss: 0.6905\n",
      "Epoch [72/100], Loss: 0.6791\n",
      "Epoch [73/100], Loss: 0.6838\n",
      "Epoch [74/100], Loss: 0.6803\n",
      "Epoch [75/100], Loss: 0.6778\n",
      "Epoch [76/100], Loss: 0.6713\n",
      "Epoch [77/100], Loss: 0.6681\n",
      "Epoch [78/100], Loss: 0.6697\n",
      "Epoch [79/100], Loss: 0.6624\n",
      "Epoch [80/100], Loss: 0.6633\n",
      "Epoch [81/100], Loss: 0.6624\n",
      "Epoch [82/100], Loss: 0.6592\n",
      "Epoch [83/100], Loss: 0.6567\n",
      "Epoch [84/100], Loss: 0.6555\n",
      "Epoch [85/100], Loss: 0.6569\n",
      "Epoch [86/100], Loss: 0.6579\n",
      "Epoch [87/100], Loss: 0.6488\n",
      "Epoch [88/100], Loss: 0.6571\n",
      "Epoch [89/100], Loss: 0.6536\n",
      "Epoch [90/100], Loss: 0.6480\n",
      "Epoch [91/100], Loss: 0.6469\n",
      "Epoch [92/100], Loss: 0.6427\n",
      "Epoch [93/100], Loss: 0.6444\n",
      "Epoch [94/100], Loss: 0.6383\n",
      "Epoch [95/100], Loss: 0.6404\n",
      "Epoch [96/100], Loss: 0.6375\n",
      "Epoch [97/100], Loss: 0.6299\n",
      "Epoch [98/100], Loss: 0.6339\n",
      "Epoch [99/100], Loss: 0.6394\n",
      "Epoch [100/100], Loss: 0.6374\n",
      "Test Accuracy Lipschitz Logit K Means: 50.66%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "lipschitz_logit_kmeans = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"kmnist_centroids/kmnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"kmnist_centroids/kmnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    logit_model_lipschitz = LogisticRegression(input_dim, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(logit_model_lipschitz.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "    previous_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logit_model_lipschitz.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = logit_model_lipschitz(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            L = max(param.grad.abs().max().item() for param in logit_model_lipschitz.parameters() if param.grad is not None)\n",
    "            adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "            # Update optimizer's learning rate\n",
    "            for param_group in optimizer_sdg.param_groups:\n",
    "                param_group['lr'] = adaptive_lr\n",
    "\n",
    "\n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if previous_loss - current_loss < tolerance:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "                break\n",
    "        else:\n",
    "            epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "        previous_loss = current_loss\n",
    "\n",
    "    logit_model_lipschitz.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 28*28)\n",
    "            outputs = logit_model_lipschitz(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy Lipschitz Logit K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    lipschitz_logit_kmeans[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>125000</th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.176797</td>\n",
       "      <td>54.227307</td>\n",
       "      <td>53.48795</td>\n",
       "      <td>50.660233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      125000     75000     50000      10000 \n",
       "0  55.176797  54.227307  53.48795  50.660233"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lipschitz_logit_kmeans_df = pd.DataFrame(lipschitz_logit_kmeans)\n",
    "lipschitz_logit_kmeans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 10.6427\n",
      "Epoch [1/50], Val Loss: 0.3288\n",
      "Epoch [2/50], Loss: 3.9959\n",
      "Epoch [2/50], Val Loss: 0.2277\n",
      "Epoch [3/50], Loss: 2.9393\n",
      "Epoch [3/50], Val Loss: 0.1888\n",
      "Epoch [4/50], Loss: 2.3735\n",
      "Epoch [4/50], Val Loss: 0.1559\n",
      "Epoch [5/50], Loss: 1.9634\n",
      "Epoch [5/50], Val Loss: 0.1451\n",
      "Epoch [6/50], Loss: 1.6441\n",
      "Epoch [6/50], Val Loss: 0.1378\n",
      "Epoch [7/50], Loss: 1.3762\n",
      "Epoch [7/50], Val Loss: 0.1305\n",
      "Epoch [8/50], Loss: 1.1576\n",
      "Epoch [8/50], Val Loss: 0.1300\n",
      "Epoch [9/50], Loss: 0.9779\n",
      "Epoch [9/50], Val Loss: 0.1273\n",
      "Epoch [10/50], Loss: 0.8215\n",
      "Epoch [10/50], Val Loss: 0.1226\n",
      "Epoch [11/50], Loss: 0.6846\n",
      "Epoch [11/50], Val Loss: 0.1230\n",
      "Epoch [12/50], Loss: 0.5851\n",
      "Epoch [12/50], Val Loss: 0.1283\n",
      "Epoch [13/50], Loss: 0.4986\n",
      "Epoch [13/50], Val Loss: 0.1303\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 90.49%\n",
      "Epoch [1/50], Loss: 8.6331\n",
      "Epoch [1/50], Val Loss: 0.4819\n",
      "Epoch [2/50], Loss: 3.2442\n",
      "Epoch [2/50], Val Loss: 0.3340\n",
      "Epoch [3/50], Loss: 2.3434\n",
      "Epoch [3/50], Val Loss: 0.2655\n",
      "Epoch [4/50], Loss: 1.8522\n",
      "Epoch [4/50], Val Loss: 0.2262\n",
      "Epoch [5/50], Loss: 1.4974\n",
      "Epoch [5/50], Val Loss: 0.2251\n",
      "Epoch [6/50], Loss: 1.2401\n",
      "Epoch [6/50], Val Loss: 0.2153\n",
      "Epoch [7/50], Loss: 1.0012\n",
      "Epoch [7/50], Val Loss: 0.2067\n",
      "Epoch [8/50], Loss: 0.8297\n",
      "Epoch [8/50], Val Loss: 0.2080\n",
      "Epoch [9/50], Loss: 0.6710\n",
      "Epoch [9/50], Val Loss: 0.2074\n",
      "Epoch [10/50], Loss: 0.5556\n",
      "Epoch [10/50], Val Loss: 0.2155\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 87.58%\n",
      "Epoch [1/50], Loss: 6.9589\n",
      "Epoch [1/50], Val Loss: 0.6531\n",
      "Epoch [2/50], Loss: 2.5977\n",
      "Epoch [2/50], Val Loss: 0.4295\n",
      "Epoch [3/50], Loss: 1.8300\n",
      "Epoch [3/50], Val Loss: 0.3362\n",
      "Epoch [4/50], Loss: 1.4170\n",
      "Epoch [4/50], Val Loss: 0.3020\n",
      "Epoch [5/50], Loss: 1.1331\n",
      "Epoch [5/50], Val Loss: 0.2806\n",
      "Epoch [6/50], Loss: 0.9193\n",
      "Epoch [6/50], Val Loss: 0.2821\n",
      "Epoch [7/50], Loss: 0.7535\n",
      "Epoch [7/50], Val Loss: 0.2849\n",
      "Epoch [8/50], Loss: 0.5986\n",
      "Epoch [8/50], Val Loss: 0.2784\n",
      "Epoch [9/50], Loss: 0.4861\n",
      "Epoch [9/50], Val Loss: 0.2850\n",
      "Epoch [10/50], Loss: 0.3972\n",
      "Epoch [10/50], Val Loss: 0.3057\n",
      "Epoch [11/50], Loss: 0.3202\n",
      "Epoch [11/50], Val Loss: 0.3203\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 85.39%\n",
      "Epoch [1/50], Loss: 2.8327\n",
      "Epoch [1/50], Val Loss: 1.8990\n",
      "Epoch [2/50], Loss: 1.2931\n",
      "Epoch [2/50], Val Loss: 1.3812\n",
      "Epoch [3/50], Loss: 0.8810\n",
      "Epoch [3/50], Val Loss: 1.0860\n",
      "Epoch [4/50], Loss: 0.6544\n",
      "Epoch [4/50], Val Loss: 0.9363\n",
      "Epoch [5/50], Loss: 0.5220\n",
      "Epoch [5/50], Val Loss: 0.8874\n",
      "Epoch [6/50], Loss: 0.4083\n",
      "Epoch [6/50], Val Loss: 0.8069\n",
      "Epoch [7/50], Loss: 0.3340\n",
      "Epoch [7/50], Val Loss: 0.8479\n",
      "Epoch [8/50], Loss: 0.2591\n",
      "Epoch [8/50], Val Loss: 0.8359\n",
      "Epoch [9/50], Loss: 0.2089\n",
      "Epoch [9/50], Val Loss: 0.7829\n",
      "Epoch [10/50], Loss: 0.1744\n",
      "Epoch [10/50], Val Loss: 0.8215\n",
      "Epoch [11/50], Loss: 0.1399\n",
      "Epoch [11/50], Val Loss: 0.8115\n",
      "Epoch [12/50], Loss: 0.1120\n",
      "Epoch [12/50], Val Loss: 0.8626\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 72.21%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "cnn_kmeans = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"kmnist_centroids/kmnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"kmnist_centroids/kmnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    cnn_model = KMNIST_CNN(num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(cnn_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "\n",
    "    def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)  # Set all weights to zero\n",
    "                m.bias.data.fill_(0.01)\n",
    "    cnn_model.apply(init_weights)\n",
    "\n",
    "    best_val_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cnn_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = cnn_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        cnn_model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader_cnn:\n",
    "                images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "                outputs = cnn_model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader_cnn)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "                break\n",
    "\n",
    "\n",
    "    cnn_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "            outputs = cnn_model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy CNN K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    cnn_kmeans[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>125000</th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.492126</td>\n",
       "      <td>87.5788</td>\n",
       "      <td>85.394454</td>\n",
       "      <td>72.213142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      125000   75000      50000      10000 \n",
       "0  90.492126  87.5788  85.394454  72.213142"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_kmeans_df = pd.DataFrame(cnn_kmeans)\n",
    "cnn_kmeans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Lipschitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 10.9141\n",
      "Epoch [1/50], Val Loss: 0.3594\n",
      "Epoch [2/50], Loss: 4.0896\n",
      "Epoch [2/50], Val Loss: 0.2234\n",
      "Epoch [3/50], Loss: 3.0179\n",
      "Epoch [3/50], Val Loss: 0.1868\n",
      "Epoch [4/50], Loss: 2.4269\n",
      "Epoch [4/50], Val Loss: 0.1641\n",
      "Epoch [5/50], Loss: 1.9976\n",
      "Epoch [5/50], Val Loss: 0.1444\n",
      "Epoch [6/50], Loss: 1.6699\n",
      "Epoch [6/50], Val Loss: 0.1396\n",
      "Epoch [7/50], Loss: 1.4062\n",
      "Epoch [7/50], Val Loss: 0.1254\n",
      "Epoch [8/50], Loss: 1.1782\n",
      "Epoch [8/50], Val Loss: 0.1295\n",
      "Epoch [9/50], Loss: 0.9825\n",
      "Epoch [9/50], Val Loss: 0.1210\n",
      "Epoch [10/50], Loss: 0.8279\n",
      "Epoch [10/50], Val Loss: 0.1234\n",
      "Epoch [11/50], Loss: 0.7047\n",
      "Epoch [11/50], Val Loss: 0.1236\n",
      "Epoch [12/50], Loss: 0.5792\n",
      "Epoch [12/50], Val Loss: 0.1270\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 90.50%\n",
      "Epoch [1/50], Loss: 8.7714\n",
      "Epoch [1/50], Val Loss: 0.5025\n",
      "Epoch [2/50], Loss: 3.2658\n",
      "Epoch [2/50], Val Loss: 0.3260\n",
      "Epoch [3/50], Loss: 2.3691\n",
      "Epoch [3/50], Val Loss: 0.2793\n",
      "Epoch [4/50], Loss: 1.8689\n",
      "Epoch [4/50], Val Loss: 0.2415\n",
      "Epoch [5/50], Loss: 1.5267\n",
      "Epoch [5/50], Val Loss: 0.2114\n",
      "Epoch [6/50], Loss: 1.2466\n",
      "Epoch [6/50], Val Loss: 0.2046\n",
      "Epoch [7/50], Loss: 1.0392\n",
      "Epoch [7/50], Val Loss: 0.2051\n",
      "Epoch [8/50], Loss: 0.8522\n",
      "Epoch [8/50], Val Loss: 0.2013\n",
      "Epoch [9/50], Loss: 0.7016\n",
      "Epoch [9/50], Val Loss: 0.1989\n",
      "Epoch [10/50], Loss: 0.5581\n",
      "Epoch [10/50], Val Loss: 0.2210\n",
      "Epoch [11/50], Loss: 0.4588\n",
      "Epoch [11/50], Val Loss: 0.2129\n",
      "Epoch [12/50], Loss: 0.3877\n",
      "Epoch [12/50], Val Loss: 0.2199\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 88.26%\n",
      "Epoch [1/50], Loss: 6.9163\n",
      "Epoch [1/50], Val Loss: 0.6644\n",
      "Epoch [2/50], Loss: 2.5981\n",
      "Epoch [2/50], Val Loss: 0.4297\n",
      "Epoch [3/50], Loss: 1.8060\n",
      "Epoch [3/50], Val Loss: 0.3407\n",
      "Epoch [4/50], Loss: 1.4133\n",
      "Epoch [4/50], Val Loss: 0.3215\n",
      "Epoch [5/50], Loss: 1.1190\n",
      "Epoch [5/50], Val Loss: 0.2890\n",
      "Epoch [6/50], Loss: 0.9172\n",
      "Epoch [6/50], Val Loss: 0.2773\n",
      "Epoch [7/50], Loss: 0.7429\n",
      "Epoch [7/50], Val Loss: 0.2789\n",
      "Epoch [8/50], Loss: 0.6130\n",
      "Epoch [8/50], Val Loss: 0.3152\n",
      "Epoch [9/50], Loss: 0.4908\n",
      "Epoch [9/50], Val Loss: 0.2906\n",
      "Stopping early at epoch 9 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 85.63%\n",
      "Epoch [1/50], Loss: 2.8519\n",
      "Epoch [1/50], Val Loss: 2.0270\n",
      "Epoch [2/50], Loss: 1.2963\n",
      "Epoch [2/50], Val Loss: 1.2906\n",
      "Epoch [3/50], Loss: 0.8531\n",
      "Epoch [3/50], Val Loss: 1.0787\n",
      "Epoch [4/50], Loss: 0.6140\n",
      "Epoch [4/50], Val Loss: 0.9313\n",
      "Epoch [5/50], Loss: 0.4757\n",
      "Epoch [5/50], Val Loss: 0.8392\n",
      "Epoch [6/50], Loss: 0.3671\n",
      "Epoch [6/50], Val Loss: 0.8005\n",
      "Epoch [7/50], Loss: 0.2929\n",
      "Epoch [7/50], Val Loss: 0.7660\n",
      "Epoch [8/50], Loss: 0.2406\n",
      "Epoch [8/50], Val Loss: 0.7190\n",
      "Epoch [9/50], Loss: 0.1919\n",
      "Epoch [9/50], Val Loss: 0.7540\n",
      "Epoch [10/50], Loss: 0.1508\n",
      "Epoch [10/50], Val Loss: 0.7452\n",
      "Epoch [11/50], Loss: 0.1219\n",
      "Epoch [11/50], Val Loss: 0.7740\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 73.73%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "cnn_lipschitz_kmeans = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"kmnist_centroids/kmnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"kmnist_centroids/kmnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    cnn_model_lipschitz = KMNIST_CNN(num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(cnn_model_lipschitz.parameters(), lr=0.005, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "\n",
    "    def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)  # Set all weights to zero\n",
    "                m.bias.data.fill_(0.01)\n",
    "    cnn_model_lipschitz.apply(init_weights)\n",
    "\n",
    "    best_val_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cnn_model_lipschitz.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = cnn_model_lipschitz(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            L = max(param.grad.abs().max().item() for param in cnn_model_lipschitz.parameters() if param.grad is not None)\n",
    "            adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "            # Update optimizer's learning rate\n",
    "            for param_group in optimizer_sdg.param_groups:\n",
    "                param_group['lr'] = adaptive_lr\n",
    "                \n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        cnn_model_lipschitz.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader_cnn:\n",
    "                images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "                outputs = cnn_model_lipschitz(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader_cnn)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "                break\n",
    "\n",
    "\n",
    "    cnn_model_lipschitz.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "            outputs = cnn_model_lipschitz(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy Lipschitz CNN K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    cnn_lipschitz_kmeans[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>125000</th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.497315</td>\n",
       "      <td>88.263678</td>\n",
       "      <td>85.630529</td>\n",
       "      <td>73.73077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      125000     75000      50000     10000 \n",
       "0  90.497315  88.263678  85.630529  73.73077"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_lipschitz_kmeans_df = pd.DataFrame(cnn_lipschitz_kmeans)\n",
    "cnn_lipschitz_kmeans_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
