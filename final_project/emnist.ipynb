{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split, Dataset, SubsetRandomSampler\n",
    "from collections import Counter\n",
    "import os\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: TF.rotate(x, -90)), transforms.Lambda(lambda x: TF.hflip(x)), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_set = torchvision.datasets.EMNIST(root='./data', split='letters', train=True, download=False, transform=transform)\n",
    "test_set = torchvision.datasets.EMNIST(root='./data', split='letters', train=False, download=False, transform=transform)\n",
    "\n",
    "# Convert dataset to DataLoader (load all at once)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False)\n",
    "\n",
    "# Extract entire train and test data\n",
    "train_data = torch.cat([data for data, _ in train_loader], dim=0)\n",
    "train_labels = torch.cat([labels for _, labels in train_loader], dim=0)\n",
    "\n",
    "test_data = torch.cat([data for data, _ in test_loader], dim=0)\n",
    "test_labels = torch.cat([labels for _, labels in test_loader], dim=0)\n",
    "\n",
    "# Flatten images from (N, 1, 28, 28) -> (N, 784)\n",
    "train_data = train_data.view(train_data.shape[0], -1).to(device)  # (60000, 784)\n",
    "test_data = test_data.view(test_data.shape[0], -1).to(device)  # (10000, 784)\n",
    "\n",
    "train_labels = train_labels.to(device)\n",
    "test_labels = test_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_path = './data/EMNIST/raw/emnist-letters-mapping.txt'\n",
    "mapping = pd.read_csv(mapping_path, delimiter=' ', header=None, index_col=0)\n",
    "label_to_char = {index: chr(row[1]) for index, row in mapping.iterrows()}\n",
    "train_labels_mapped = [label_to_char[label.item()] for label in train_labels]\n",
    "test_labels_mapped = [label_to_char[label.item()] for label in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = train_data[500], train_labels_mapped[500]  # Get first image & label\n",
    "plt.imshow(img.view(28,28), cmap=\"gray\")\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subsets = np.array([75000, 50000, 10000, 5000, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(test_data, prototype_data, prototype_labels, k=1):\n",
    "    # Compute full pairwise distance matrix in one go\n",
    "    print(\"Computing full distance matrix...\")\n",
    "    distances = torch.cdist(test_data, prototype_data)  # Shape: (10000, 60000)\n",
    "\n",
    "    # Get indices of k nearest neighbors\n",
    "    k_indices = torch.topk(distances, k, largest=False).indices  # Shape: (10000, k)\n",
    "\n",
    "    # Retrieve the k nearest labels\n",
    "    k_labels = prototype_labels[k_indices]  # Shape: (10000, k)\n",
    "\n",
    "    # Majority voting for prediction\n",
    "    pred_labels = torch.mode(k_labels, dim=1).values  # Shape: (10000,)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = (pred_labels == test_labels).float().mean().item()\n",
    "    print(f'{k}-NN accuracy on full test set (no batching): {accuracy:.4f}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8486\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8475\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8465\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8445\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8475\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8456\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8454\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8458\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8448\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8455\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8463\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8463\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8464\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8453\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8436\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8324\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8334\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8367\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8330\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8301\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8366\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8314\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8339\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8341\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8347\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8315\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8311\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8363\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8332\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8358\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7682\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7695\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7617\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7683\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7674\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7660\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7637\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7643\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7637\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7625\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7629\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7668\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7712\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7720\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7656\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7309\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7280\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7230\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7219\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7249\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7283\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7288\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7288\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7308\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7304\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7262\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7230\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7262\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7259\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7261\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.5933\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.5931\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6066\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6048\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6000\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6010\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6047\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6062\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6035\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6095\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.5985\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.5871\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.5850\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6040\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.5867\n"
     ]
    }
   ],
   "source": [
    "accuracy_dict = {}\n",
    "for subset in num_subsets:\n",
    "    accuracy_list = []\n",
    "    for _ in range(15):\n",
    "        random_indicies = torch.randperm(train_data.shape[0])[:subset]\n",
    "        prototype_train_data = train_data[random_indicies]\n",
    "        prototype_train_labels = train_labels[random_indicies]\n",
    "        accuracy = compute_accuracy(test_data, prototype_train_data, prototype_train_labels)\n",
    "        accuracy_list.append(accuracy)\n",
    "    accuracy_dict[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000    0.845971\n",
       " 50000    0.833625\n",
       " 10000    0.766253\n",
       " 5000     0.726872\n",
       " 1000     0.598933\n",
       " dtype: float64,\n",
       " 75000    0.001280\n",
       " 50000    0.002109\n",
       " 10000    0.003172\n",
       " 5000     0.002885\n",
       " 1000     0.007988\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df = pd.DataFrame(accuracy_dict)\n",
    "accuracy_df.mean(), accuracy_df.std(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8454\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8461\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8450\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8478\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8468\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8451\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8429\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8453\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8461\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8466\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8440\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8469\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8447\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8470\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8468\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8307\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8308\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8343\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8320\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8341\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8332\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8314\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8308\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8343\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8337\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8330\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8338\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8334\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8370\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8332\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7682\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7690\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7681\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7614\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7690\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7645\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7607\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7633\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7612\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7663\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7639\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7706\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7697\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7646\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7696\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7299\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7252\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7234\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7274\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7230\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7239\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7281\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7286\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7255\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7299\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7262\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7269\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7246\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7298\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7257\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.5931\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.5962\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.5991\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.5924\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6090\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6005\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6071\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6048\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6119\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.5956\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.5969\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.5919\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6012\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.6039\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.5932\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8499\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8511\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8527\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8514\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8515\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8487\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8544\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8521\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8494\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8486\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8498\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8501\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8532\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8507\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8494\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8407\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8364\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8364\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8379\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8393\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8395\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8412\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8392\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8386\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8377\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8391\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8369\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8411\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8379\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8382\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7689\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7634\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7677\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7643\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7638\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7692\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7663\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7688\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7691\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7670\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7695\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7662\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7657\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7608\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7648\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7221\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7236\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7195\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7275\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7223\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7273\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7217\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7268\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7213\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7191\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7170\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7209\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7186\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7209\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7204\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.5901\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.5728\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.5853\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.5827\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.5586\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.5916\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.5707\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.5723\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.5821\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.5692\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.5728\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.5897\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.5675\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.5719\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.5799\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8505\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8517\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8520\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8517\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8512\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8548\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8507\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8520\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8525\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8505\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8540\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8504\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8515\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8522\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8509\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8373\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8382\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8417\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8387\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8383\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8361\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8371\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8371\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8387\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8380\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8370\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8360\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8369\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8369\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8362\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7680\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7685\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7615\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7652\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7694\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7637\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7659\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7665\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7651\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7677\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7688\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7672\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7674\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7624\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7680\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7208\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7149\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7223\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7174\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7212\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7200\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7178\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7204\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7239\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7201\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7174\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7205\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7227\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7235\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7241\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.5699\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.5849\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.5723\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.5750\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.5701\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.5623\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.5738\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.5763\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.5698\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.5774\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.5709\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.5713\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.5788\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.5677\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.5842\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8490\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8477\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8478\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8487\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8478\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8490\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8482\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8506\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8479\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8503\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8494\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8486\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8497\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8472\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8474\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8342\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8348\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8334\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8367\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8340\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8345\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8329\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8335\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8340\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8342\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8324\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8330\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8346\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8351\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8356\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7607\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7583\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7585\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7567\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7590\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7582\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7591\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7604\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7602\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7598\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7626\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7560\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7578\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7600\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7608\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7085\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7139\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7177\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7120\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7221\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7102\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7158\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7170\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7145\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7162\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7186\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7138\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7161\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7199\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7142\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.5754\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.5737\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.5671\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.5650\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.5589\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.5766\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.5677\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.5681\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.5763\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.5681\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.5653\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.5739\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.5760\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.5754\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.5725\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8441\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8470\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8426\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8442\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8443\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8443\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8456\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8437\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8459\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8460\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8450\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8445\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8457\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8439\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8461\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8286\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8298\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8303\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8312\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8289\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8308\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8320\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8298\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8293\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8319\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8325\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8327\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8317\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8307\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8314\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7549\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7521\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7548\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7548\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7576\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7521\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7487\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7581\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7569\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7550\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7565\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7538\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7544\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7561\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7528\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7138\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7079\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7063\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7077\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7028\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7083\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7106\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7157\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7097\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7063\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7091\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7011\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7135\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7146\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7138\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.5627\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.5545\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.5533\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.5593\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.5587\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.5621\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.5768\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.5655\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.5569\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.5589\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.5693\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.5521\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.5585\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.5606\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.5718\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8410\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8400\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8424\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8405\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8431\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8405\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8409\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8396\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8410\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8421\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8409\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8416\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8407\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8424\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8397\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8268\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8279\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8270\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8270\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8238\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8293\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8278\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8283\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8278\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8265\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8275\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8243\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8241\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8261\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8299\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7495\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7472\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7544\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7499\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7488\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7495\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7492\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7506\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7471\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7488\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7489\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7512\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7525\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7511\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7476\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7029\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7030\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7035\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7015\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7056\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7014\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7058\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7065\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.6992\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7048\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7033\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7034\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7014\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7047\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7004\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5448\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5413\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5644\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5582\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5674\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5552\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5479\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5531\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5575\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5514\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5578\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5498\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5518\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5532\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.5407\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8386\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8376\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8375\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8384\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8381\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8382\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8369\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8384\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8384\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8404\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8388\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8381\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8371\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8405\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8387\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8235\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8224\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8243\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8239\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8222\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8243\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8219\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8206\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8240\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8222\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8227\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8234\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8239\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8215\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8225\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7456\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7415\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7456\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7454\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7443\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7404\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7405\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7395\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7436\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7441\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7449\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7452\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7466\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7374\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7414\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6987\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6962\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6995\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6977\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6933\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6959\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6971\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6938\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6926\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6964\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6992\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6991\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6865\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6953\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6937\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5436\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5487\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5353\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5641\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5265\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5343\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5507\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5280\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5558\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5325\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5472\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5395\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5559\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5487\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.5402\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8345\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8358\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8338\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8339\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8346\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8330\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8325\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8355\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8354\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8375\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8351\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8327\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8345\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8351\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8333\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8187\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8201\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8197\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8203\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8214\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8222\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8206\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8199\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8196\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8185\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8220\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8213\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8181\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8185\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8175\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7384\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7378\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7400\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7378\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7400\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7364\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7416\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7399\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7462\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7402\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7351\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7365\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7396\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7354\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7346\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6924\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6901\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6884\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6873\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6938\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6928\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6914\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6898\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6897\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6924\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6866\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6854\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6854\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6901\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6943\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5723\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5325\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5366\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5261\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5199\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5413\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5258\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5425\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5221\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5538\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5241\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5596\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5400\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5585\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.5475\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8304\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8333\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8298\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8307\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8297\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8317\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8306\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8330\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8293\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8313\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8306\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8309\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8317\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8296\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8297\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8147\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8162\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8124\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8164\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8163\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8145\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8158\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8149\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8131\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8150\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8167\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8194\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8178\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8171\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8192\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7315\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7326\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7366\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7306\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7354\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7306\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7317\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7319\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7328\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7349\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7299\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7371\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7351\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7363\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7300\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6783\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6902\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6824\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6857\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6867\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6872\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6876\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6924\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6798\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6902\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6854\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6807\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6915\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6779\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6851\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5321\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5127\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5277\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5369\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5086\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5282\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5250\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5296\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5299\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5248\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5348\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5431\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5346\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5330\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.5311\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8305\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8272\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8297\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8291\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8281\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8281\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8279\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8281\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8299\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8290\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8289\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8283\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8279\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8286\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8284\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8137\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8141\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8122\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8125\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8135\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8126\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8114\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8125\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8124\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8109\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8114\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8150\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8116\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8095\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8114\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7307\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7301\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7296\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7295\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7279\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7262\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7280\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7315\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7259\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7287\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7373\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7344\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7337\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7289\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7299\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6711\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6747\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6837\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6796\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6772\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6819\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6849\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6713\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6852\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6765\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6746\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6871\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6788\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6841\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6836\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5147\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.4924\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5103\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5294\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5270\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5227\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5191\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5188\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5206\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5169\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5183\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5076\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5312\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5375\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.5154\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8237\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8249\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8255\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8264\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8245\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8252\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8252\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8263\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8273\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8246\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8264\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8272\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8263\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8251\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8272\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8105\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8104\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8101\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8097\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8082\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8087\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8086\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8089\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8110\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8109\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8081\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8100\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8114\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8103\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8111\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7258\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7234\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7186\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7255\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7242\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7241\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7245\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7273\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7259\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7252\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7294\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7219\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7226\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7240\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7215\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6707\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6772\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6717\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6741\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6630\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6759\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6776\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6795\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6805\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6714\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6773\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6750\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6714\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6755\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6711\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5092\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5068\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5024\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5098\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5309\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5291\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5176\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5201\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5015\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5031\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5160\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5240\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5235\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5317\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.5201\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8242\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8239\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8217\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8248\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8219\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8243\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8233\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8206\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8234\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8231\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8216\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8231\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8230\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8237\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8229\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8060\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8069\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8075\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8070\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8080\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8067\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8051\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8064\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8060\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8066\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8056\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8059\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8072\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8066\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8059\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7240\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7272\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7230\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7191\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7202\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7257\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7213\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7182\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7225\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7234\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7235\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7199\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7268\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7213\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7199\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6732\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6690\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6689\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6523\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6718\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6696\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6644\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6661\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6730\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6716\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6666\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6707\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6602\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6633\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6679\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5120\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5199\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5072\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.4899\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5083\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.4917\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5104\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5269\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5026\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5074\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5105\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5364\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5252\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5190\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.5291\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8207\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8206\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8196\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8212\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8200\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8202\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8212\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8196\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8222\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8221\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8207\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8197\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8211\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8214\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8207\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8030\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8033\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8038\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8040\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8025\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8041\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8039\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8015\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8018\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8021\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8019\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8024\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8020\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8032\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8051\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7141\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7155\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7191\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7180\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7155\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7190\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7150\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7173\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7130\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7099\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7132\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7185\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7217\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7188\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7154\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6687\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6700\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6677\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6721\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6670\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6718\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6594\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6626\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6690\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6648\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6682\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6675\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6602\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6649\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6583\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.5061\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.4958\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.5225\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.5053\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.5175\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.4917\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.4866\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.5003\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.4987\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.4863\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.4624\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.5031\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.4967\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.5103\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.5007\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8167\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8194\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8179\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8184\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8187\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8184\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8195\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8184\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8182\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8168\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8194\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8188\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8172\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8182\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8170\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8005\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8003\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8004\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8018\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7986\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8032\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8017\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7989\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8023\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8025\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7974\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8001\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7995\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7998\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8014\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7092\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7149\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7155\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7126\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7143\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7160\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7144\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7152\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7115\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7188\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7150\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7108\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7080\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7113\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7116\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6601\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6575\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6598\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6580\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6642\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6677\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6623\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6548\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6674\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6600\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6557\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6587\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6623\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6624\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6468\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.4773\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.5047\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.4940\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.4989\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.4926\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.5297\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.5075\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.5152\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.5293\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.5080\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.4987\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.4817\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.5112\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.4925\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.4986\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8148\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8158\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8160\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8147\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8169\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8162\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8139\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8140\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8147\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8153\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8164\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8156\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8163\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8153\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8150\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7992\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7987\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7981\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7980\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7987\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7973\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7964\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7964\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7986\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7982\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8012\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7968\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7983\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7988\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7980\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7037\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7039\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7095\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7118\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7089\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7089\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7062\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7176\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7166\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7034\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7108\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7132\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7036\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7111\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7119\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6584\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6624\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6487\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6590\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6534\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6625\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6490\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6514\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6626\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6572\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6595\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6516\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6543\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6562\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6556\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.5039\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.4747\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.4859\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.4735\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.4893\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.5076\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.4865\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.5034\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.5021\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.4958\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.4754\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.4863\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.5124\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.5081\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.4956\n"
     ]
    }
   ],
   "source": [
    "full_accuracy_df_random = pd.DataFrame()\n",
    "\n",
    "num_neighbors = np.arange(1,31,2)\n",
    "\n",
    "for k in num_neighbors:\n",
    "    accuracy_dict_random_per_neigbor = {}\n",
    "    for subset in num_subsets:\n",
    "        accuracy_list = []\n",
    "        for _ in range(15):\n",
    "            random_indicies = torch.randperm(train_data.shape[0])[:subset]\n",
    "            prototype_train_data = train_data[random_indicies]\n",
    "            prototype_train_labels = train_labels[random_indicies]\n",
    "            accuracy = compute_accuracy(test_data, prototype_train_data, prototype_train_labels, k)\n",
    "            accuracy_list.append(accuracy)\n",
    "        accuracy_dict_random_per_neigbor[subset] = accuracy_list\n",
    "    temp = pd.DataFrame(accuracy_dict_random_per_neigbor)\n",
    "    temp['k'] = k\n",
    "    full_accuracy_df_random = pd.concat([full_accuracy_df_random, temp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "      <th>k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.845433</td>\n",
       "      <td>0.830673</td>\n",
       "      <td>0.768173</td>\n",
       "      <td>0.729856</td>\n",
       "      <td>0.593125</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.846106</td>\n",
       "      <td>0.830817</td>\n",
       "      <td>0.768990</td>\n",
       "      <td>0.725192</td>\n",
       "      <td>0.596250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.844952</td>\n",
       "      <td>0.834327</td>\n",
       "      <td>0.768125</td>\n",
       "      <td>0.723365</td>\n",
       "      <td>0.599135</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.847837</td>\n",
       "      <td>0.832019</td>\n",
       "      <td>0.761394</td>\n",
       "      <td>0.727356</td>\n",
       "      <td>0.592356</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.846779</td>\n",
       "      <td>0.834135</td>\n",
       "      <td>0.768990</td>\n",
       "      <td>0.722981</td>\n",
       "      <td>0.609038</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.816394</td>\n",
       "      <td>0.801154</td>\n",
       "      <td>0.710817</td>\n",
       "      <td>0.659471</td>\n",
       "      <td>0.475433</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.815625</td>\n",
       "      <td>0.796827</td>\n",
       "      <td>0.713173</td>\n",
       "      <td>0.651635</td>\n",
       "      <td>0.486298</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.816298</td>\n",
       "      <td>0.798317</td>\n",
       "      <td>0.703558</td>\n",
       "      <td>0.654327</td>\n",
       "      <td>0.512404</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.815288</td>\n",
       "      <td>0.798798</td>\n",
       "      <td>0.711058</td>\n",
       "      <td>0.656202</td>\n",
       "      <td>0.508125</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.815048</td>\n",
       "      <td>0.798029</td>\n",
       "      <td>0.711875</td>\n",
       "      <td>0.655577</td>\n",
       "      <td>0.495625</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       75000     50000     10000      5000      1000   k\n",
       "0   0.845433  0.830673  0.768173  0.729856  0.593125   1\n",
       "1   0.846106  0.830817  0.768990  0.725192  0.596250   1\n",
       "2   0.844952  0.834327  0.768125  0.723365  0.599135   1\n",
       "3   0.847837  0.832019  0.761394  0.727356  0.592356   1\n",
       "4   0.846779  0.834135  0.768990  0.722981  0.609038   1\n",
       "..       ...       ...       ...       ...       ...  ..\n",
       "10  0.816394  0.801154  0.710817  0.659471  0.475433  29\n",
       "11  0.815625  0.796827  0.713173  0.651635  0.486298  29\n",
       "12  0.816298  0.798317  0.703558  0.654327  0.512404  29\n",
       "13  0.815288  0.798798  0.711058  0.656202  0.508125  29\n",
       "14  0.815048  0.798029  0.711875  0.655577  0.495625  29\n",
       "\n",
       "[225 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_accuracy_df_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(full_accuracy_df_random['k'], full_accuracy_df_random[1000], label='1000')\n",
    "plt.scatter(full_accuracy_df_random['k'], full_accuracy_df_random[5000], label='5000')\n",
    "plt.scatter(full_accuracy_df_random['k'], full_accuracy_df_random[10000], label='10000')\n",
    "plt.scatter(full_accuracy_df_random['k'], full_accuracy_df_random[75000], label='75000')\n",
    "plt.scatter(full_accuracy_df_random['k'], full_accuracy_df_random[50000], label='50000')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('K Nearest Neighbors vs Accuracy (Random Prototyping)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering\n",
    "\n",
    "def optimized_kmeans(X, k, num_iters=100, tol=1e-4, batch_size=5000, device='cpu'):\n",
    "    X = X.to(device, dtype=torch.float32)  # Ensure correct dtype\n",
    "    N, D = X.shape\n",
    "\n",
    "    # Initialize centroids randomly\n",
    "    indices = torch.randperm(N)[:k]\n",
    "    centroids = X[indices]\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        cluster_assignments = torch.empty(N, dtype=torch.long, device=device)\n",
    "\n",
    "        # Compute distances in batches to save memory\n",
    "        for j in range(0, N, batch_size):\n",
    "            batch = X[j:j+batch_size]\n",
    "            distances = torch.cdist(batch, centroids)  # Compute distance for this batch\n",
    "            cluster_assignments[j:j+batch_size] = torch.argmin(distances, dim=1)  # Assign cluster\n",
    "\n",
    "        # Compute new centroids\n",
    "        new_centroids = torch.zeros_like(centroids)\n",
    "        counts = torch.zeros(k, device=device)\n",
    "\n",
    "        for c in range(k):\n",
    "            cluster_indices = (cluster_assignments == c).nonzero(as_tuple=True)[0]\n",
    "            if cluster_indices.numel() > 0:\n",
    "                new_centroids[c] = X[cluster_indices].mean(dim=0)\n",
    "                counts[c] = cluster_indices.numel()\n",
    "            else:\n",
    "                # Assign the farthest point to avoid empty clusters\n",
    "                farthest_point = X[torch.argmax(torch.cdist(X, centroids[c].unsqueeze(0)), dim=0)]\n",
    "                new_centroids[c] = farthest_point\n",
    "\n",
    "        # Check for convergence\n",
    "        if torch.allclose(new_centroids, centroids, atol=tol):\n",
    "            print(f'Converged at iteration {i}')\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return cluster_assignments, centroids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to assign labels to centroids\n",
    "def assign_labels(cluster_labels, y_true, k):\n",
    "    \"\"\"\n",
    "    Assigns a label to each K-Means cluster using majority voting.\n",
    "    \n",
    "    Args:\n",
    "    - cluster_labels (Tensor): Cluster assignments for each point\n",
    "    - y_true (Tensor): True MNIST labels\n",
    "    - k (int): Number of clusters\n",
    "\n",
    "    Returns:\n",
    "    - cluster_to_label (list): List where index `i` corresponds to cluster `i`'s assigned label\n",
    "    \"\"\"\n",
    "    cluster_to_label = [-1] * k  # Initialize list with -1 for empty clusters\n",
    "\n",
    "    for cluster in range(k):\n",
    "        # Get all true labels for this cluster\n",
    "        cluster_indices = (cluster_labels == cluster).nonzero(as_tuple=True)[0]\n",
    "        true_labels = y_true[cluster_indices]\n",
    "\n",
    "        # Find the most common label in this cluster\n",
    "        if len(true_labels) > 0:\n",
    "            most_common_label = Counter(true_labels.tolist()).most_common(1)[0][0]\n",
    "            cluster_to_label[cluster] = most_common_label\n",
    "\n",
    "    return cluster_to_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in num_subsets:\n",
    "    if os.path.exists(f\"emnist_centroids/emnist_kmeans_centroids_{subset}.pth\"):\n",
    "        continue\n",
    "    else:\n",
    "        cluster_labels, centroids = optimized_kmeans(train_data, subset)\n",
    "        cluster_to_label = assign_labels(cluster_labels, train_labels, subset)\n",
    "        if os.path.exists('emnist_centroids') == False:\n",
    "            os.mkdir('emnist_centroids')\n",
    "        torch.save(centroids, f\"emnist_centroids/emnist_kmeans_centroids_{subset}.pth\")\n",
    "        torch.save(cluster_to_label, f\"emnist_centroids/emnist_kmeans_cluster_labels_{subset}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8486\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8454\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8332\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8140\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7510\n"
     ]
    }
   ],
   "source": [
    "accuracy_dict_kmeans = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    accuracy = compute_accuracy(test_data, prototype_train_data, prototype_train_labels)\n",
    "    accuracy_dict_kmeans[subset] = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.848606</td>\n",
       "      <td>0.845433</td>\n",
       "      <td>0.833221</td>\n",
       "      <td>0.81399</td>\n",
       "      <td>0.75101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      75000     50000     10000    5000     1000 \n",
       "0  0.848606  0.845433  0.833221  0.81399  0.75101"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df_kmeans = pd.DataFrame(accuracy_dict_kmeans)\n",
    "accuracy_df_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying size K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8486\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8454\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8332\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8140\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7510\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8577\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8526\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8297\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8112\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7236\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8594\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8555\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8302\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8116\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7172\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8584\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8539\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8244\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8058\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7055\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8562\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8490\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8204\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7987\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.6973\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8539\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8468\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8164\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7932\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.6876\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8498\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8440\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8117\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7892\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6764\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8463\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8408\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8069\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7818\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6659\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8439\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8386\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8030\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7761\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6586\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8400\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8346\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7976\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7718\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6537\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8385\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8321\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7951\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7680\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6480\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8354\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8300\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7902\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7643\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6410\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8347\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8274\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7873\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7609\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6379\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8315\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8259\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7846\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7564\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6326\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8293\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8231\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7814\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7529\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6270\n"
     ]
    }
   ],
   "source": [
    "full_accuracy_df_kmeans = pd.DataFrame()\n",
    "\n",
    "num_neighbors = np.arange(1,30,2)\n",
    "\n",
    "for k in num_neighbors:\n",
    "    accuracy_dict_kmean_per_neigbor = {}\n",
    "    for subset in num_subsets:\n",
    "        prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "        prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "        accuracy = compute_accuracy(test_data, prototype_train_data, prototype_train_labels, k)\n",
    "        accuracy_dict_kmean_per_neigbor[subset] = [accuracy]\n",
    "    temp = pd.DataFrame(accuracy_dict_kmean_per_neigbor)\n",
    "    full_accuracy_df_kmeans = pd.concat([full_accuracy_df_kmeans, temp], axis=0)\n",
    "full_accuracy_df_kmeans['k'] = num_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "      <th>k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.848606</td>\n",
       "      <td>0.845433</td>\n",
       "      <td>0.833221</td>\n",
       "      <td>0.813990</td>\n",
       "      <td>0.751010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.857692</td>\n",
       "      <td>0.852644</td>\n",
       "      <td>0.829712</td>\n",
       "      <td>0.811154</td>\n",
       "      <td>0.723558</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.859423</td>\n",
       "      <td>0.855481</td>\n",
       "      <td>0.830240</td>\n",
       "      <td>0.811635</td>\n",
       "      <td>0.717163</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.858365</td>\n",
       "      <td>0.853942</td>\n",
       "      <td>0.824423</td>\n",
       "      <td>0.805769</td>\n",
       "      <td>0.705481</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.856250</td>\n",
       "      <td>0.849038</td>\n",
       "      <td>0.820433</td>\n",
       "      <td>0.798702</td>\n",
       "      <td>0.697260</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.853942</td>\n",
       "      <td>0.846779</td>\n",
       "      <td>0.816442</td>\n",
       "      <td>0.793221</td>\n",
       "      <td>0.687644</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.849760</td>\n",
       "      <td>0.843990</td>\n",
       "      <td>0.811683</td>\n",
       "      <td>0.789231</td>\n",
       "      <td>0.676442</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.846298</td>\n",
       "      <td>0.840817</td>\n",
       "      <td>0.806875</td>\n",
       "      <td>0.781779</td>\n",
       "      <td>0.665913</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.843894</td>\n",
       "      <td>0.838558</td>\n",
       "      <td>0.802981</td>\n",
       "      <td>0.776058</td>\n",
       "      <td>0.658558</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.834567</td>\n",
       "      <td>0.797596</td>\n",
       "      <td>0.771827</td>\n",
       "      <td>0.653654</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.832115</td>\n",
       "      <td>0.795144</td>\n",
       "      <td>0.767981</td>\n",
       "      <td>0.648029</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.835433</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.790240</td>\n",
       "      <td>0.764279</td>\n",
       "      <td>0.641010</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.834712</td>\n",
       "      <td>0.827404</td>\n",
       "      <td>0.787308</td>\n",
       "      <td>0.760913</td>\n",
       "      <td>0.637885</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.831538</td>\n",
       "      <td>0.825865</td>\n",
       "      <td>0.784567</td>\n",
       "      <td>0.756394</td>\n",
       "      <td>0.632644</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.829327</td>\n",
       "      <td>0.823125</td>\n",
       "      <td>0.781394</td>\n",
       "      <td>0.752885</td>\n",
       "      <td>0.627019</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      75000     50000     10000      5000      1000   k\n",
       "0  0.848606  0.845433  0.833221  0.813990  0.751010   1\n",
       "0  0.857692  0.852644  0.829712  0.811154  0.723558   3\n",
       "0  0.859423  0.855481  0.830240  0.811635  0.717163   5\n",
       "0  0.858365  0.853942  0.824423  0.805769  0.705481   7\n",
       "0  0.856250  0.849038  0.820433  0.798702  0.697260   9\n",
       "0  0.853942  0.846779  0.816442  0.793221  0.687644  11\n",
       "0  0.849760  0.843990  0.811683  0.789231  0.676442  13\n",
       "0  0.846298  0.840817  0.806875  0.781779  0.665913  15\n",
       "0  0.843894  0.838558  0.802981  0.776058  0.658558  17\n",
       "0  0.840000  0.834567  0.797596  0.771827  0.653654  19\n",
       "0  0.838462  0.832115  0.795144  0.767981  0.648029  21\n",
       "0  0.835433  0.830000  0.790240  0.764279  0.641010  23\n",
       "0  0.834712  0.827404  0.787308  0.760913  0.637885  25\n",
       "0  0.831538  0.825865  0.784567  0.756394  0.632644  27\n",
       "0  0.829327  0.823125  0.781394  0.752885  0.627019  29"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_accuracy_df_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for subset in num_subsets:\n",
    "    plt.plot(full_accuracy_df_kmeans['k'], full_accuracy_df_kmeans[subset], label=f'{subset}')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('K Nearest Neighbors vs Accuracy (K Means++)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def parallel_kmeans_plusplus(X: torch.Tensor, k: int) -> torch.Tensor:\n",
    "    N, D = X.shape\n",
    "    device = X.device\n",
    "    centroids = torch.empty((k, D), device=device)\n",
    "\n",
    "    # 1️⃣ Pick first centroid randomly\n",
    "    first_idx = torch.randint(0, N, (1,))\n",
    "    centroids[0] = X[first_idx.squeeze()]  # ✅ Fix indexing\n",
    "\n",
    "    # 2️⃣ Track min distances\n",
    "    min_distances = torch.full((N,), float('inf'), device=device)\n",
    "\n",
    "    for i in range(1, k):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iter {i}\")\n",
    "        # Compute squared distances (ensuring correct shape)\n",
    "        new_distances = torch.sum((X - centroids[i-1].unsqueeze(0)) ** 2, dim=1)  # ✅ Fix broadcasting\n",
    "\n",
    "        # Update min distances\n",
    "        min_distances = torch.minimum(min_distances, new_distances)\n",
    "\n",
    "        # Sample next centroid\n",
    "        probabilities = min_distances / min_distances.sum()\n",
    "        next_idx = torch.multinomial(probabilities, 1)\n",
    "        centroids[i] = X[next_idx.squeeze()]  # ✅ Fix indexing\n",
    "\n",
    "    return centroids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimized_kmeans(X, k, num_iters=100, tol=1e-4, batch_size=10000, device='cpu'):\n",
    "    X = X.to(device, dtype=torch.float32)  # Ensure correct dtype\n",
    "    N, D = X.shape\n",
    "\n",
    "    # Initialize centroids K-Means++\n",
    "    centroids = parallel_kmeans_plusplus(X, k)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        cluster_assignments = torch.empty(N, dtype=torch.long, device=device)\n",
    "\n",
    "        # Compute distances in batches to save memory\n",
    "        for j in range(0, N, batch_size):\n",
    "            batch = X[j:j+batch_size]\n",
    "            distances = torch.cdist(batch, centroids)  # Compute distance for this batch\n",
    "            cluster_assignments[j:j+batch_size] = torch.argmin(distances, dim=1)  # Assign cluster\n",
    "\n",
    "        # Compute new centroids\n",
    "        new_centroids = torch.zeros_like(centroids)\n",
    "        counts = torch.zeros(k, device=device)\n",
    "\n",
    "        for c in range(k):\n",
    "            cluster_indices = (cluster_assignments == c).nonzero(as_tuple=True)[0]\n",
    "            if cluster_indices.numel() > 0:\n",
    "                new_centroids[c] = X[cluster_indices].mean(dim=0)\n",
    "                counts[c] = cluster_indices.numel()\n",
    "            else:\n",
    "                # Assign the farthest point to avoid empty clusters\n",
    "                farthest_point = X[torch.argmax(torch.cdist(X, centroids[c].unsqueeze(0)), dim=0)]\n",
    "                new_centroids[c] = farthest_point\n",
    "\n",
    "        # Check for convergence\n",
    "        if torch.allclose(new_centroids, centroids, atol=tol):\n",
    "            print(f'Converged at iteration {i}')\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return cluster_assignments, centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in num_subsets:\n",
    "    if os.path.exists(f\"emnist_centroids/emnist_kmeans_plus_centroids_{subset}.pth\"):\n",
    "        continue\n",
    "    else:\n",
    "        cluster_labels, centroids = optimized_kmeans(train_data, subset)\n",
    "        cluster_to_label = assign_labels(cluster_labels, train_labels, subset)\n",
    "        torch.save(centroids, f\"emnist_centroids/emnist_kmeans_plus_centroids_{subset}.pth\")\n",
    "        torch.save(cluster_to_label, f\"emnist_centroids/emnist_kmeans_plus_cluster_labels_{subset}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8539\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8472\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8287\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8178\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7526\n"
     ]
    }
   ],
   "source": [
    "accuracy_dict_kmeans_plus = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_plus_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_plus_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    accuracy = compute_accuracy(test_data, prototype_train_data, prototype_train_labels)\n",
    "    accuracy_dict_kmeans_plus[subset] = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.853942</td>\n",
       "      <td>0.847212</td>\n",
       "      <td>0.828702</td>\n",
       "      <td>0.817788</td>\n",
       "      <td>0.752596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      75000     50000     10000     5000      1000 \n",
       "0  0.853942  0.847212  0.828702  0.817788  0.752596"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df_kmeans_plus = pd.DataFrame(accuracy_dict_kmeans_plus)\n",
    "accuracy_df_kmeans_plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8485\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8472\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8287\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8178\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7526\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8584\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8545\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8309\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8131\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7220\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8599\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8545\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8325\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8123\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7172\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8594\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8543\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8280\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8062\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7053\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8563\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8502\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8242\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8010\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.6973\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8534\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8479\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8209\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7956\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.6866\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8502\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8439\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8150\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7879\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6778\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8480\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8405\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8102\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7826\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6677\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8439\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8377\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8053\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7790\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6620\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8402\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8354\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7999\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7740\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6538\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8392\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8344\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7969\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7703\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6483\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8376\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8301\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7929\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7650\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6451\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8351\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8258\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7880\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7606\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6381\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8342\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8251\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7849\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7574\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6335\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8315\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8229\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7813\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7525\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6270\n"
     ]
    }
   ],
   "source": [
    "full_accuracy_df_kmeans_plus = pd.DataFrame()\n",
    "\n",
    "num_neighbors = np.arange(1,30,2)\n",
    "\n",
    "for k in num_neighbors:\n",
    "    accuracy_dict_kmean_per_neigbor = {}\n",
    "    for subset in num_subsets:\n",
    "        prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_plus_centroids_{subset}.pth\", weights_only=False)\n",
    "        prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_plus_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "        accuracy = compute_accuracy(test_data, prototype_train_data, prototype_train_labels, k)\n",
    "        accuracy_dict_kmean_per_neigbor[subset] = [accuracy]\n",
    "    temp = pd.DataFrame(accuracy_dict_kmean_per_neigbor)\n",
    "    full_accuracy_df_kmeans_plus = pd.concat([full_accuracy_df_kmeans_plus, temp], axis=0)\n",
    "full_accuracy_df_kmeans_plus['k'] = num_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "      <th>k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.848462</td>\n",
       "      <td>0.847212</td>\n",
       "      <td>0.828702</td>\n",
       "      <td>0.817788</td>\n",
       "      <td>0.752596</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.858413</td>\n",
       "      <td>0.854471</td>\n",
       "      <td>0.830913</td>\n",
       "      <td>0.813125</td>\n",
       "      <td>0.722019</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.859904</td>\n",
       "      <td>0.854519</td>\n",
       "      <td>0.832452</td>\n",
       "      <td>0.812308</td>\n",
       "      <td>0.717212</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.854279</td>\n",
       "      <td>0.827981</td>\n",
       "      <td>0.806250</td>\n",
       "      <td>0.705337</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.856346</td>\n",
       "      <td>0.850192</td>\n",
       "      <td>0.824183</td>\n",
       "      <td>0.800962</td>\n",
       "      <td>0.697308</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.853365</td>\n",
       "      <td>0.847885</td>\n",
       "      <td>0.820913</td>\n",
       "      <td>0.795625</td>\n",
       "      <td>0.686635</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.850240</td>\n",
       "      <td>0.843894</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.787885</td>\n",
       "      <td>0.677837</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.847981</td>\n",
       "      <td>0.840529</td>\n",
       "      <td>0.810240</td>\n",
       "      <td>0.782596</td>\n",
       "      <td>0.667740</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.843942</td>\n",
       "      <td>0.837740</td>\n",
       "      <td>0.805337</td>\n",
       "      <td>0.778990</td>\n",
       "      <td>0.661971</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.840240</td>\n",
       "      <td>0.835433</td>\n",
       "      <td>0.799904</td>\n",
       "      <td>0.773990</td>\n",
       "      <td>0.653798</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.839231</td>\n",
       "      <td>0.834423</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.770337</td>\n",
       "      <td>0.648269</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.837596</td>\n",
       "      <td>0.830144</td>\n",
       "      <td>0.792933</td>\n",
       "      <td>0.764952</td>\n",
       "      <td>0.645096</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.835144</td>\n",
       "      <td>0.825817</td>\n",
       "      <td>0.788029</td>\n",
       "      <td>0.760625</td>\n",
       "      <td>0.638077</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.834183</td>\n",
       "      <td>0.825096</td>\n",
       "      <td>0.784904</td>\n",
       "      <td>0.757356</td>\n",
       "      <td>0.633462</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.831490</td>\n",
       "      <td>0.822933</td>\n",
       "      <td>0.781298</td>\n",
       "      <td>0.752452</td>\n",
       "      <td>0.627019</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      75000     50000     10000      5000      1000   k\n",
       "0  0.848462  0.847212  0.828702  0.817788  0.752596   1\n",
       "0  0.858413  0.854471  0.830913  0.813125  0.722019   3\n",
       "0  0.859904  0.854519  0.832452  0.812308  0.717212   5\n",
       "0  0.859375  0.854279  0.827981  0.806250  0.705337   7\n",
       "0  0.856346  0.850192  0.824183  0.800962  0.697308   9\n",
       "0  0.853365  0.847885  0.820913  0.795625  0.686635  11\n",
       "0  0.850240  0.843894  0.815000  0.787885  0.677837  13\n",
       "0  0.847981  0.840529  0.810240  0.782596  0.667740  15\n",
       "0  0.843942  0.837740  0.805337  0.778990  0.661971  17\n",
       "0  0.840240  0.835433  0.799904  0.773990  0.653798  19\n",
       "0  0.839231  0.834423  0.796875  0.770337  0.648269  21\n",
       "0  0.837596  0.830144  0.792933  0.764952  0.645096  23\n",
       "0  0.835144  0.825817  0.788029  0.760625  0.638077  25\n",
       "0  0.834183  0.825096  0.784904  0.757356  0.633462  27\n",
       "0  0.831490  0.822933  0.781298  0.752452  0.627019  29"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_accuracy_df_kmeans_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for subset in num_subsets:\n",
    "    plt.plot(full_accuracy_df_kmeans_plus['k'], full_accuracy_df_kmeans_plus[subset], label=f'{subset}')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('K Nearest Neighbors vs Accuracy (K Means++)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)  # Fully connected layer\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)  # No activation, since CrossEntropyLoss applies softmax\n",
    "    \n",
    "    def init_weights(self):\n",
    "        nn.init.zeros_(self.linear.weight)\n",
    "        nn.init.zeros_(self.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28 * 28\n",
    "num_classes = 27\n",
    "logit_model = LogisticRegression(input_dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_sdg = optim.SGD(logit_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.1734\n",
      "Epoch [2/50], Loss: 1.0344\n",
      "Epoch [3/50], Loss: 1.0189\n",
      "Epoch [4/50], Loss: 1.0091\n",
      "Epoch [5/50], Loss: 1.0032\n",
      "Epoch [6/50], Loss: 0.9984\n",
      "Epoch [7/50], Loss: 0.9962\n",
      "Epoch [8/50], Loss: 0.9918\n",
      "Epoch [9/50], Loss: 0.9879\n",
      "Epoch [10/50], Loss: 0.9861\n",
      "Epoch [11/50], Loss: 0.9848\n",
      "Epoch [12/50], Loss: 0.9851\n",
      "Epoch [13/50], Loss: 0.9815\n",
      "Epoch [14/50], Loss: 0.9806\n",
      "Epoch [15/50], Loss: 0.9793\n",
      "Epoch [16/50], Loss: 0.9783\n",
      "Epoch [17/50], Loss: 0.9781\n",
      "Epoch [18/50], Loss: 0.9778\n",
      "Stopping early at epoch 18 (Loss improvement < 0.001 for 3 epochs)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "previous_loss = float('inf')  # Track the best loss\n",
    "epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    logit_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = logit_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer_sdg.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer_sdg.step()\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "    current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if previous_loss - current_loss < tolerance:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "            break\n",
    "    else:\n",
    "        epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "    previous_loss = current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Base Logit: 71.25%\n"
     ]
    }
   ],
   "source": [
    "logit_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28)\n",
    "        outputs = logit_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy Base Logit: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipschitz Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.6129\n",
      "Epoch [2/50], Loss: 1.5576\n",
      "Epoch [3/50], Loss: 1.5813\n",
      "Epoch [4/50], Loss: 1.5550\n",
      "Epoch [5/50], Loss: 1.5188\n",
      "Epoch [6/50], Loss: 1.5362\n",
      "Epoch [7/50], Loss: 1.5241\n",
      "Epoch [8/50], Loss: 1.5394\n",
      "Epoch [9/50], Loss: 1.5186\n",
      "Epoch [10/50], Loss: 1.5239\n",
      "Epoch [11/50], Loss: 1.5152\n",
      "Epoch [12/50], Loss: 1.5130\n",
      "Epoch [13/50], Loss: 1.5249\n",
      "Epoch [14/50], Loss: 1.5295\n",
      "Epoch [15/50], Loss: 1.5067\n",
      "Epoch [16/50], Loss: 1.4989\n",
      "Epoch [17/50], Loss: 1.5026\n",
      "Epoch [18/50], Loss: 1.5074\n",
      "Epoch [19/50], Loss: 1.5150\n",
      "Stopping early at epoch 19 (Loss improvement < 0.001 for 3 epochs)\n"
     ]
    }
   ],
   "source": [
    "# Define Adam Optimizer\n",
    "logit_model_lipschitz = LogisticRegression(input_dim, num_classes)\n",
    "optimizer_sdg = optim.SGD(logit_model_lipschitz.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Early stopping parameters\n",
    "tolerance = 1e-3  # Minimum change in loss to continue training\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "previous_loss = float('inf')\n",
    "epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 50  # Maximum epochs\n",
    "for epoch in range(num_epochs):\n",
    "    logit_model_lipschitz.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(-1, 28 * 28)  # Flatten images\n",
    "\n",
    "        outputs = logit_model_lipschitz(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        logit_model_lipschitz.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        L = max(param.grad.abs().max().item() for param in logit_model_lipschitz.parameters() if param.grad is not None)\n",
    "        adaptive_lr = min(0.1, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "        # Update optimizer's learning rate\n",
    "        for param_group in optimizer_sdg.param_groups:\n",
    "            param_group['lr'] = adaptive_lr\n",
    "\n",
    "        optimizer_sdg.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    current_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if previous_loss - current_loss < tolerance:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "            break\n",
    "    else:\n",
    "        epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "    previous_loss = current_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Newtonian Logit: 61.73%\n"
     ]
    }
   ],
   "source": [
    "logit_model_lipschitz.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(-1, 28*28)\n",
    "        \n",
    "        outputs = logit_model_lipschitz(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy Newtonian Logit: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMNIST_CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EMNIST_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 256)  # Corrected input size\n",
    "        self.fc2 = nn.Linear(256, num_classes)  # Adjust output layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, start_dim=1)  # Dynamically flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = EMNIST_CNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_sdg = optim.SGD(cnn_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(train_set))\n",
    "val_size = len(train_set) - train_size\n",
    "train_set_small, val_set = random_split(train_set, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_cnn = DataLoader(\n",
    "    train_set_small, batch_size=256, shuffle=True, \n",
    "    num_workers=8, pin_memory=True\n",
    ")\n",
    "val_loader_cnn = DataLoader(\n",
    "    val_set, batch_size=256, shuffle=False, \n",
    "    num_workers=8, pin_memory=True\n",
    ")\n",
    "test_loader_cnn = DataLoader(\n",
    "    test_set, batch_size=256, shuffle=False, \n",
    "    num_workers=8, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.9479 Val Loss: 0.4327\n",
      "Epoch [2/50], Train Loss: 0.3451 Val Loss: 0.3116\n",
      "Epoch [3/50], Train Loss: 0.2644 Val Loss: 0.2814\n",
      "Epoch [4/50], Train Loss: 0.2261 Val Loss: 0.2284\n",
      "Epoch [5/50], Train Loss: 0.1990 Val Loss: 0.2249\n",
      "Epoch [6/50], Train Loss: 0.1804 Val Loss: 0.2195\n",
      "Epoch [7/50], Train Loss: 0.1647 Val Loss: 0.2126\n",
      "Epoch [8/50], Train Loss: 0.1523 Val Loss: 0.2042\n",
      "Epoch [9/50], Train Loss: 0.1407 Val Loss: 0.2111\n",
      "Epoch [10/50], Train Loss: 0.1318 Val Loss: 0.2052\n",
      "Epoch [11/50], Train Loss: 0.1206 Val Loss: 0.2088\n",
      "Epoch [12/50], Train Loss: 0.1148 Val Loss: 0.2090\n",
      "Epoch [13/50], Train Loss: 0.1048 Val Loss: 0.2123\n",
      "Stopping early at epoch 13 (No improvement in 5 epochs)\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "\n",
    "    for images, labels in train_loader_cnn:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = cnn_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer_sdg.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer_sdg.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader_cnn)\n",
    "\n",
    "    # Validation loop\n",
    "    cnn_model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_cnn:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = cnn_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "\n",
    "    val_loss /= len(val_loader_cnn)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 92.66%\n"
     ]
    }
   ],
   "source": [
    "cnn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_cnn:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = cnn_model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print(f\"Final Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Lipschitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_lipschitz = EMNIST_CNN(num_classes).to(device)\n",
    "optimizer_sdg = optim.SGD(cnn_model_lipschitz.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.8132 Val Loss: 0.3126\n",
      "Epoch [2/50], Train Loss: 0.2581 Val Loss: 0.2363\n",
      "Epoch [3/50], Train Loss: 0.2081 Val Loss: 0.2281\n",
      "Epoch [4/50], Train Loss: 0.1767 Val Loss: 0.2166\n",
      "Epoch [5/50], Train Loss: 0.1541 Val Loss: 0.2209\n",
      "Epoch [6/50], Train Loss: 0.1391 Val Loss: 0.2208\n",
      "Epoch [7/50], Train Loss: 0.1261 Val Loss: 0.2133\n",
      "Epoch [8/50], Train Loss: 0.1139 Val Loss: 0.2247\n",
      "Epoch [9/50], Train Loss: 0.1098 Val Loss: 0.2379\n",
      "Epoch [10/50], Train Loss: 0.1016 Val Loss: 0.2385\n",
      "Epoch [11/50], Train Loss: 0.0947 Val Loss: 0.2322\n",
      "Epoch [12/50], Train Loss: 0.0886 Val Loss: 0.2481\n",
      "Stopping early at epoch 12 (No improvement in 5 epochs)\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_model_lipschitz.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader_cnn:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = cnn_model_lipschitz(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "    \n",
    "        # Backward pass\n",
    "        optimizer_sdg.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "\n",
    "        L = max(param.grad.abs().max().item() for param in cnn_model_lipschitz.parameters() if param.grad is not None)\n",
    "        adaptive_lr = min(0.1, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "        # Update optimizer's learning rate\n",
    "        for param_group in optimizer_sdg.param_groups:\n",
    "            param_group['lr'] = adaptive_lr\n",
    "\n",
    "        optimizer_sdg.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader_cnn)\n",
    "\n",
    "    # Validation loop\n",
    "    cnn_model_lipschitz.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_cnn:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = cnn_model_lipschitz(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader_cnn)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Newtonian Logit: 92.91%\n"
     ]
    }
   ],
   "source": [
    "cnn_model_lipschitz.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_cnn:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = cnn_model_lipschitz(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy Newtonian Logit: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Prototypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.1747\n",
      "Epoch [2/100], Loss: 1.0639\n",
      "Epoch [3/100], Loss: 1.0450\n",
      "Epoch [4/100], Loss: 1.0391\n",
      "Epoch [5/100], Loss: 1.0332\n",
      "Epoch [6/100], Loss: 1.0272\n",
      "Epoch [7/100], Loss: 1.0214\n",
      "Epoch [8/100], Loss: 1.0207\n",
      "Epoch [9/100], Loss: 1.0152\n",
      "Epoch [10/100], Loss: 1.0156\n",
      "Epoch [11/100], Loss: 1.0115\n",
      "Epoch [12/100], Loss: 1.0118\n",
      "Epoch [13/100], Loss: 1.0062\n",
      "Epoch [14/100], Loss: 1.0059\n",
      "Epoch [15/100], Loss: 1.0049\n",
      "Epoch [16/100], Loss: 1.0038\n",
      "Epoch [17/100], Loss: 1.0016\n",
      "Epoch [18/100], Loss: 1.0018\n",
      "Epoch [19/100], Loss: 1.0020\n",
      "Epoch [20/100], Loss: 0.9990\n",
      "Epoch [21/100], Loss: 0.9967\n",
      "Epoch [22/100], Loss: 0.9992\n",
      "Epoch [23/100], Loss: 0.9963\n",
      "Epoch [24/100], Loss: 0.9958\n",
      "Epoch [25/100], Loss: 0.9940\n",
      "Epoch [26/100], Loss: 0.9942\n",
      "Epoch [27/100], Loss: 0.9926\n",
      "Epoch [28/100], Loss: 0.9958\n",
      "Epoch [29/100], Loss: 0.9921\n",
      "Epoch [30/100], Loss: 0.9931\n",
      "Epoch [31/100], Loss: 0.9904\n",
      "Epoch [32/100], Loss: 0.9903\n",
      "Epoch [33/100], Loss: 0.9916\n",
      "Epoch [34/100], Loss: 0.9916\n",
      "Stopping early at epoch 34 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [34/100], Loss: 0.9916\n",
      "Test Accuracy Base Logit: 69.20%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.1628\n",
      "Epoch [2/100], Loss: 1.0563\n",
      "Epoch [3/100], Loss: 1.0384\n",
      "Epoch [4/100], Loss: 1.0262\n",
      "Epoch [5/100], Loss: 1.0235\n",
      "Epoch [6/100], Loss: 1.0174\n",
      "Epoch [7/100], Loss: 1.0123\n",
      "Epoch [8/100], Loss: 1.0098\n",
      "Epoch [9/100], Loss: 1.0081\n",
      "Epoch [10/100], Loss: 1.0084\n",
      "Epoch [11/100], Loss: 1.0029\n",
      "Epoch [12/100], Loss: 1.0002\n",
      "Epoch [13/100], Loss: 0.9999\n",
      "Epoch [14/100], Loss: 0.9959\n",
      "Epoch [15/100], Loss: 0.9948\n",
      "Epoch [16/100], Loss: 0.9959\n",
      "Epoch [17/100], Loss: 0.9943\n",
      "Epoch [18/100], Loss: 0.9921\n",
      "Epoch [19/100], Loss: 0.9899\n",
      "Epoch [20/100], Loss: 0.9885\n",
      "Epoch [21/100], Loss: 0.9885\n",
      "Epoch [22/100], Loss: 0.9885\n",
      "Epoch [23/100], Loss: 0.9883\n",
      "Stopping early at epoch 23 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [23/100], Loss: 0.9883\n",
      "Test Accuracy Base Logit: 69.77%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.1691\n",
      "Epoch [2/100], Loss: 1.0584\n",
      "Epoch [3/100], Loss: 1.0400\n",
      "Epoch [4/100], Loss: 1.0330\n",
      "Epoch [5/100], Loss: 1.0269\n",
      "Epoch [6/100], Loss: 1.0217\n",
      "Epoch [7/100], Loss: 1.0188\n",
      "Epoch [8/100], Loss: 1.0143\n",
      "Epoch [9/100], Loss: 1.0117\n",
      "Epoch [10/100], Loss: 1.0101\n",
      "Epoch [11/100], Loss: 1.0070\n",
      "Epoch [12/100], Loss: 1.0052\n",
      "Epoch [13/100], Loss: 1.0018\n",
      "Epoch [14/100], Loss: 1.0010\n",
      "Epoch [15/100], Loss: 0.9988\n",
      "Epoch [16/100], Loss: 0.9976\n",
      "Epoch [17/100], Loss: 0.9999\n",
      "Epoch [18/100], Loss: 0.9969\n",
      "Epoch [19/100], Loss: 0.9922\n",
      "Epoch [20/100], Loss: 0.9954\n",
      "Epoch [21/100], Loss: 0.9949\n",
      "Epoch [22/100], Loss: 0.9920\n",
      "Epoch [23/100], Loss: 0.9903\n",
      "Epoch [24/100], Loss: 0.9912\n",
      "Epoch [25/100], Loss: 0.9918\n",
      "Epoch [26/100], Loss: 0.9920\n",
      "Stopping early at epoch 26 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [26/100], Loss: 0.9920\n",
      "Test Accuracy Base Logit: 69.69%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.1711\n",
      "Epoch [2/100], Loss: 1.0590\n",
      "Epoch [3/100], Loss: 1.0451\n",
      "Epoch [4/100], Loss: 1.0319\n",
      "Epoch [5/100], Loss: 1.0262\n",
      "Epoch [6/100], Loss: 1.0221\n",
      "Epoch [7/100], Loss: 1.0176\n",
      "Epoch [8/100], Loss: 1.0148\n",
      "Epoch [9/100], Loss: 1.0117\n",
      "Epoch [10/100], Loss: 1.0093\n",
      "Epoch [11/100], Loss: 1.0056\n",
      "Epoch [12/100], Loss: 1.0026\n",
      "Epoch [13/100], Loss: 1.0022\n",
      "Epoch [14/100], Loss: 1.0037\n",
      "Epoch [15/100], Loss: 0.9984\n",
      "Epoch [16/100], Loss: 0.9977\n",
      "Epoch [17/100], Loss: 0.9966\n",
      "Epoch [18/100], Loss: 0.9978\n",
      "Epoch [19/100], Loss: 0.9951\n",
      "Epoch [20/100], Loss: 0.9935\n",
      "Epoch [21/100], Loss: 0.9920\n",
      "Epoch [22/100], Loss: 0.9905\n",
      "Epoch [23/100], Loss: 0.9917\n",
      "Epoch [24/100], Loss: 0.9917\n",
      "Epoch [25/100], Loss: 0.9908\n",
      "Stopping early at epoch 25 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [25/100], Loss: 0.9908\n",
      "Test Accuracy Base Logit: 69.75%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.1706\n",
      "Epoch [2/100], Loss: 1.0600\n",
      "Epoch [3/100], Loss: 1.0410\n",
      "Epoch [4/100], Loss: 1.0373\n",
      "Epoch [5/100], Loss: 1.0295\n",
      "Epoch [6/100], Loss: 1.0217\n",
      "Epoch [7/100], Loss: 1.0188\n",
      "Epoch [8/100], Loss: 1.0139\n",
      "Epoch [9/100], Loss: 1.0135\n",
      "Epoch [10/100], Loss: 1.0087\n",
      "Epoch [11/100], Loss: 1.0061\n",
      "Epoch [12/100], Loss: 1.0088\n",
      "Epoch [13/100], Loss: 1.0039\n",
      "Epoch [14/100], Loss: 1.0016\n",
      "Epoch [15/100], Loss: 1.0007\n",
      "Epoch [16/100], Loss: 0.9993\n",
      "Epoch [17/100], Loss: 0.9978\n",
      "Epoch [18/100], Loss: 0.9976\n",
      "Epoch [19/100], Loss: 0.9958\n",
      "Epoch [20/100], Loss: 0.9951\n",
      "Epoch [21/100], Loss: 0.9959\n",
      "Epoch [22/100], Loss: 0.9917\n",
      "Epoch [23/100], Loss: 0.9934\n",
      "Epoch [24/100], Loss: 0.9930\n",
      "Epoch [25/100], Loss: 0.9910\n",
      "Epoch [26/100], Loss: 0.9919\n",
      "Epoch [27/100], Loss: 0.9883\n",
      "Epoch [28/100], Loss: 0.9905\n",
      "Epoch [29/100], Loss: 0.9898\n",
      "Epoch [30/100], Loss: 0.9856\n",
      "Epoch [31/100], Loss: 0.9877\n",
      "Epoch [32/100], Loss: 0.9878\n",
      "Epoch [33/100], Loss: 0.9886\n",
      "Stopping early at epoch 33 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [33/100], Loss: 0.9886\n",
      "Test Accuracy Base Logit: 69.67%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.2120\n",
      "Epoch [2/100], Loss: 1.0663\n",
      "Epoch [3/100], Loss: 1.0448\n",
      "Epoch [4/100], Loss: 1.0313\n",
      "Epoch [5/100], Loss: 1.0257\n",
      "Epoch [6/100], Loss: 1.0167\n",
      "Epoch [7/100], Loss: 1.0121\n",
      "Epoch [8/100], Loss: 1.0088\n",
      "Epoch [9/100], Loss: 1.0020\n",
      "Epoch [10/100], Loss: 0.9977\n",
      "Epoch [11/100], Loss: 0.9949\n",
      "Epoch [12/100], Loss: 0.9934\n",
      "Epoch [13/100], Loss: 0.9893\n",
      "Epoch [14/100], Loss: 0.9874\n",
      "Epoch [15/100], Loss: 0.9863\n",
      "Epoch [16/100], Loss: 0.9830\n",
      "Epoch [17/100], Loss: 0.9814\n",
      "Epoch [18/100], Loss: 0.9785\n",
      "Epoch [19/100], Loss: 0.9789\n",
      "Epoch [20/100], Loss: 0.9783\n",
      "Epoch [21/100], Loss: 0.9782\n",
      "Stopping early at epoch 21 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [21/100], Loss: 0.9782\n",
      "Test Accuracy Base Logit: 69.04%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.2142\n",
      "Epoch [2/100], Loss: 1.0672\n",
      "Epoch [3/100], Loss: 1.0488\n",
      "Epoch [4/100], Loss: 1.0343\n",
      "Epoch [5/100], Loss: 1.0262\n",
      "Epoch [6/100], Loss: 1.0216\n",
      "Epoch [7/100], Loss: 1.0169\n",
      "Epoch [8/100], Loss: 1.0114\n",
      "Epoch [9/100], Loss: 1.0034\n",
      "Epoch [10/100], Loss: 1.0030\n",
      "Epoch [11/100], Loss: 1.0006\n",
      "Epoch [12/100], Loss: 0.9985\n",
      "Epoch [13/100], Loss: 0.9940\n",
      "Epoch [14/100], Loss: 0.9901\n",
      "Epoch [15/100], Loss: 0.9893\n",
      "Epoch [16/100], Loss: 0.9906\n",
      "Epoch [17/100], Loss: 0.9872\n",
      "Epoch [18/100], Loss: 0.9843\n",
      "Epoch [19/100], Loss: 0.9831\n",
      "Epoch [20/100], Loss: 0.9856\n",
      "Epoch [21/100], Loss: 0.9819\n",
      "Epoch [22/100], Loss: 0.9782\n",
      "Epoch [23/100], Loss: 0.9780\n",
      "Epoch [24/100], Loss: 0.9790\n",
      "Epoch [25/100], Loss: 0.9769\n",
      "Epoch [26/100], Loss: 0.9711\n",
      "Epoch [27/100], Loss: 0.9726\n",
      "Epoch [28/100], Loss: 0.9712\n",
      "Epoch [29/100], Loss: 0.9698\n",
      "Epoch [30/100], Loss: 0.9735\n",
      "Epoch [31/100], Loss: 0.9701\n",
      "Epoch [32/100], Loss: 0.9700\n",
      "Epoch [33/100], Loss: 0.9683\n",
      "Epoch [34/100], Loss: 0.9662\n",
      "Epoch [35/100], Loss: 0.9651\n",
      "Epoch [36/100], Loss: 0.9663\n",
      "Epoch [37/100], Loss: 0.9605\n",
      "Epoch [38/100], Loss: 0.9615\n",
      "Epoch [39/100], Loss: 0.9630\n",
      "Epoch [40/100], Loss: 0.9649\n",
      "Stopping early at epoch 40 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [40/100], Loss: 0.9649\n",
      "Test Accuracy Base Logit: 68.58%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.2182\n",
      "Epoch [2/100], Loss: 1.0731\n",
      "Epoch [3/100], Loss: 1.0547\n",
      "Epoch [4/100], Loss: 1.0422\n",
      "Epoch [5/100], Loss: 1.0342\n",
      "Epoch [6/100], Loss: 1.0286\n",
      "Epoch [7/100], Loss: 1.0216\n",
      "Epoch [8/100], Loss: 1.0168\n",
      "Epoch [9/100], Loss: 1.0142\n",
      "Epoch [10/100], Loss: 1.0125\n",
      "Epoch [11/100], Loss: 1.0043\n",
      "Epoch [12/100], Loss: 1.0024\n",
      "Epoch [13/100], Loss: 1.0034\n",
      "Epoch [14/100], Loss: 0.9992\n",
      "Epoch [15/100], Loss: 0.9950\n",
      "Epoch [16/100], Loss: 0.9951\n",
      "Epoch [17/100], Loss: 0.9917\n",
      "Epoch [18/100], Loss: 0.9920\n",
      "Epoch [19/100], Loss: 0.9891\n",
      "Epoch [20/100], Loss: 0.9849\n",
      "Epoch [21/100], Loss: 0.9853\n",
      "Epoch [22/100], Loss: 0.9845\n",
      "Epoch [23/100], Loss: 0.9824\n",
      "Epoch [24/100], Loss: 0.9816\n",
      "Epoch [25/100], Loss: 0.9801\n",
      "Epoch [26/100], Loss: 0.9800\n",
      "Epoch [27/100], Loss: 0.9787\n",
      "Epoch [28/100], Loss: 0.9755\n",
      "Epoch [29/100], Loss: 0.9772\n",
      "Epoch [30/100], Loss: 0.9758\n",
      "Epoch [31/100], Loss: 0.9749\n",
      "Epoch [32/100], Loss: 0.9760\n",
      "Epoch [33/100], Loss: 0.9750\n",
      "Stopping early at epoch 33 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [33/100], Loss: 0.9750\n",
      "Test Accuracy Base Logit: 68.95%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.2117\n",
      "Epoch [2/100], Loss: 1.0611\n",
      "Epoch [3/100], Loss: 1.0382\n",
      "Epoch [4/100], Loss: 1.0294\n",
      "Epoch [5/100], Loss: 1.0212\n",
      "Epoch [6/100], Loss: 1.0126\n",
      "Epoch [7/100], Loss: 1.0084\n",
      "Epoch [8/100], Loss: 1.0047\n",
      "Epoch [9/100], Loss: 0.9980\n",
      "Epoch [10/100], Loss: 0.9964\n",
      "Epoch [11/100], Loss: 0.9938\n",
      "Epoch [12/100], Loss: 0.9885\n",
      "Epoch [13/100], Loss: 0.9878\n",
      "Epoch [14/100], Loss: 0.9865\n",
      "Epoch [15/100], Loss: 0.9819\n",
      "Epoch [16/100], Loss: 0.9820\n",
      "Epoch [17/100], Loss: 0.9788\n",
      "Epoch [18/100], Loss: 0.9792\n",
      "Epoch [19/100], Loss: 0.9762\n",
      "Epoch [20/100], Loss: 0.9733\n",
      "Epoch [21/100], Loss: 0.9763\n",
      "Epoch [22/100], Loss: 0.9684\n",
      "Epoch [23/100], Loss: 0.9692\n",
      "Epoch [24/100], Loss: 0.9663\n",
      "Epoch [25/100], Loss: 0.9725\n",
      "Epoch [26/100], Loss: 0.9649\n",
      "Epoch [27/100], Loss: 0.9655\n",
      "Epoch [28/100], Loss: 0.9622\n",
      "Epoch [29/100], Loss: 0.9625\n",
      "Epoch [30/100], Loss: 0.9619\n",
      "Epoch [31/100], Loss: 0.9619\n",
      "Stopping early at epoch 31 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [31/100], Loss: 0.9619\n",
      "Test Accuracy Base Logit: 69.16%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.2067\n",
      "Epoch [2/100], Loss: 1.0598\n",
      "Epoch [3/100], Loss: 1.0386\n",
      "Epoch [4/100], Loss: 1.0268\n",
      "Epoch [5/100], Loss: 1.0201\n",
      "Epoch [6/100], Loss: 1.0115\n",
      "Epoch [7/100], Loss: 1.0064\n",
      "Epoch [8/100], Loss: 1.0017\n",
      "Epoch [9/100], Loss: 1.0001\n",
      "Epoch [10/100], Loss: 0.9953\n",
      "Epoch [11/100], Loss: 0.9927\n",
      "Epoch [12/100], Loss: 0.9849\n",
      "Epoch [13/100], Loss: 0.9858\n",
      "Epoch [14/100], Loss: 0.9834\n",
      "Epoch [15/100], Loss: 0.9794\n",
      "Epoch [16/100], Loss: 0.9788\n",
      "Epoch [17/100], Loss: 0.9758\n",
      "Epoch [18/100], Loss: 0.9741\n",
      "Epoch [19/100], Loss: 0.9735\n",
      "Epoch [20/100], Loss: 0.9724\n",
      "Epoch [21/100], Loss: 0.9679\n",
      "Epoch [22/100], Loss: 0.9692\n",
      "Epoch [23/100], Loss: 0.9691\n",
      "Epoch [24/100], Loss: 0.9674\n",
      "Epoch [25/100], Loss: 0.9673\n",
      "Epoch [26/100], Loss: 0.9645\n",
      "Epoch [27/100], Loss: 0.9614\n",
      "Epoch [28/100], Loss: 0.9641\n",
      "Epoch [29/100], Loss: 0.9643\n",
      "Epoch [30/100], Loss: 0.9594\n",
      "Epoch [31/100], Loss: 0.9597\n",
      "Epoch [32/100], Loss: 0.9580\n",
      "Epoch [33/100], Loss: 0.9607\n",
      "Epoch [34/100], Loss: 0.9587\n",
      "Epoch [35/100], Loss: 0.9579\n",
      "Epoch [36/100], Loss: 0.9570\n",
      "Epoch [37/100], Loss: 0.9548\n",
      "Epoch [38/100], Loss: 0.9554\n",
      "Epoch [39/100], Loss: 0.9542\n",
      "Epoch [40/100], Loss: 0.9532\n",
      "Epoch [41/100], Loss: 0.9535\n",
      "Epoch [42/100], Loss: 0.9528\n",
      "Epoch [43/100], Loss: 0.9516\n",
      "Epoch [44/100], Loss: 0.9535\n",
      "Epoch [45/100], Loss: 0.9513\n",
      "Epoch [46/100], Loss: 0.9487\n",
      "Epoch [47/100], Loss: 0.9500\n",
      "Epoch [48/100], Loss: 0.9474\n",
      "Epoch [49/100], Loss: 0.9477\n",
      "Epoch [50/100], Loss: 0.9479\n",
      "Epoch [51/100], Loss: 0.9489\n",
      "Stopping early at epoch 51 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [51/100], Loss: 0.9489\n",
      "Test Accuracy Base Logit: 68.61%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 1.5723\n",
      "Epoch [2/100], Loss: 1.1381\n",
      "Epoch [3/100], Loss: 1.0635\n",
      "Epoch [4/100], Loss: 1.0242\n",
      "Epoch [5/100], Loss: 0.9980\n",
      "Epoch [6/100], Loss: 0.9827\n",
      "Epoch [7/100], Loss: 0.9621\n",
      "Epoch [8/100], Loss: 0.9482\n",
      "Epoch [9/100], Loss: 0.9374\n",
      "Epoch [10/100], Loss: 0.9209\n",
      "Epoch [11/100], Loss: 0.9146\n",
      "Epoch [12/100], Loss: 0.9007\n",
      "Epoch [13/100], Loss: 0.9010\n",
      "Epoch [14/100], Loss: 0.8854\n",
      "Epoch [15/100], Loss: 0.8814\n",
      "Epoch [16/100], Loss: 0.8690\n",
      "Epoch [17/100], Loss: 0.8698\n",
      "Epoch [18/100], Loss: 0.8697\n",
      "Epoch [19/100], Loss: 0.8577\n",
      "Epoch [20/100], Loss: 0.8472\n",
      "Epoch [21/100], Loss: 0.8445\n",
      "Epoch [22/100], Loss: 0.8332\n",
      "Epoch [23/100], Loss: 0.8291\n",
      "Epoch [24/100], Loss: 0.8267\n",
      "Epoch [25/100], Loss: 0.8286\n",
      "Epoch [26/100], Loss: 0.8144\n",
      "Epoch [27/100], Loss: 0.8100\n",
      "Epoch [28/100], Loss: 0.8061\n",
      "Epoch [29/100], Loss: 0.8141\n",
      "Epoch [30/100], Loss: 0.8082\n",
      "Epoch [31/100], Loss: 0.7994\n",
      "Epoch [32/100], Loss: 0.7896\n",
      "Epoch [33/100], Loss: 0.7839\n",
      "Epoch [34/100], Loss: 0.7767\n",
      "Epoch [35/100], Loss: 0.7780\n",
      "Epoch [36/100], Loss: 0.7755\n",
      "Epoch [37/100], Loss: 0.7695\n",
      "Epoch [38/100], Loss: 0.7784\n",
      "Epoch [39/100], Loss: 0.7735\n",
      "Epoch [40/100], Loss: 0.7657\n",
      "Epoch [41/100], Loss: 0.7580\n",
      "Epoch [42/100], Loss: 0.7581\n",
      "Epoch [43/100], Loss: 0.7621\n",
      "Epoch [44/100], Loss: 0.7559\n",
      "Epoch [45/100], Loss: 0.7554\n",
      "Epoch [46/100], Loss: 0.7544\n",
      "Epoch [47/100], Loss: 0.7521\n",
      "Epoch [48/100], Loss: 0.7451\n",
      "Epoch [49/100], Loss: 0.7481\n",
      "Epoch [50/100], Loss: 0.7523\n",
      "Epoch [51/100], Loss: 0.7280\n",
      "Epoch [52/100], Loss: 0.7274\n",
      "Epoch [53/100], Loss: 0.7243\n",
      "Epoch [54/100], Loss: 0.7335\n",
      "Epoch [55/100], Loss: 0.7353\n",
      "Epoch [56/100], Loss: 0.7241\n",
      "Epoch [57/100], Loss: 0.7274\n",
      "Epoch [58/100], Loss: 0.7282\n",
      "Epoch [59/100], Loss: 0.7225\n",
      "Epoch [60/100], Loss: 0.7218\n",
      "Epoch [61/100], Loss: 0.7139\n",
      "Epoch [62/100], Loss: 0.7120\n",
      "Epoch [63/100], Loss: 0.7154\n",
      "Epoch [64/100], Loss: 0.7165\n",
      "Epoch [65/100], Loss: 0.7119\n",
      "Epoch [66/100], Loss: 0.7097\n",
      "Epoch [67/100], Loss: 0.7034\n",
      "Epoch [68/100], Loss: 0.7046\n",
      "Epoch [69/100], Loss: 0.6901\n",
      "Epoch [70/100], Loss: 0.7020\n",
      "Epoch [71/100], Loss: 0.6987\n",
      "Epoch [72/100], Loss: 0.6967\n",
      "Epoch [73/100], Loss: 0.6951\n",
      "Epoch [74/100], Loss: 0.6982\n",
      "Epoch [75/100], Loss: 0.6878\n",
      "Epoch [76/100], Loss: 0.6952\n",
      "Epoch [77/100], Loss: 0.6954\n",
      "Epoch [78/100], Loss: 0.6792\n",
      "Epoch [79/100], Loss: 0.6844\n",
      "Epoch [80/100], Loss: 0.6826\n",
      "Epoch [81/100], Loss: 0.6841\n",
      "Epoch [82/100], Loss: 0.6780\n",
      "Epoch [83/100], Loss: 0.6802\n",
      "Epoch [84/100], Loss: 0.6751\n",
      "Epoch [85/100], Loss: 0.6749\n",
      "Epoch [86/100], Loss: 0.6783\n",
      "Epoch [87/100], Loss: 0.6789\n",
      "Stopping early at epoch 87 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [87/100], Loss: 0.6789\n",
      "Test Accuracy Base Logit: 65.07%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 1.5514\n",
      "Epoch [2/100], Loss: 1.1158\n",
      "Epoch [3/100], Loss: 1.0603\n",
      "Epoch [4/100], Loss: 1.0110\n",
      "Epoch [5/100], Loss: 0.9881\n",
      "Epoch [6/100], Loss: 0.9671\n",
      "Epoch [7/100], Loss: 0.9532\n",
      "Epoch [8/100], Loss: 0.9319\n",
      "Epoch [9/100], Loss: 0.9286\n",
      "Epoch [10/100], Loss: 0.9151\n",
      "Epoch [11/100], Loss: 0.9041\n",
      "Epoch [12/100], Loss: 0.8886\n",
      "Epoch [13/100], Loss: 0.8755\n",
      "Epoch [14/100], Loss: 0.8638\n",
      "Epoch [15/100], Loss: 0.8655\n",
      "Epoch [16/100], Loss: 0.8484\n",
      "Epoch [17/100], Loss: 0.8499\n",
      "Epoch [18/100], Loss: 0.8341\n",
      "Epoch [19/100], Loss: 0.8301\n",
      "Epoch [20/100], Loss: 0.8332\n",
      "Epoch [21/100], Loss: 0.8220\n",
      "Epoch [22/100], Loss: 0.8124\n",
      "Epoch [23/100], Loss: 0.8131\n",
      "Epoch [24/100], Loss: 0.8092\n",
      "Epoch [25/100], Loss: 0.8103\n",
      "Epoch [26/100], Loss: 0.8111\n",
      "Epoch [27/100], Loss: 0.7970\n",
      "Epoch [28/100], Loss: 0.7901\n",
      "Epoch [29/100], Loss: 0.7852\n",
      "Epoch [30/100], Loss: 0.7850\n",
      "Epoch [31/100], Loss: 0.7763\n",
      "Epoch [32/100], Loss: 0.7781\n",
      "Epoch [33/100], Loss: 0.7784\n",
      "Epoch [34/100], Loss: 0.7753\n",
      "Epoch [35/100], Loss: 0.7660\n",
      "Epoch [36/100], Loss: 0.7601\n",
      "Epoch [37/100], Loss: 0.7602\n",
      "Epoch [38/100], Loss: 0.7534\n",
      "Epoch [39/100], Loss: 0.7489\n",
      "Epoch [40/100], Loss: 0.7612\n",
      "Epoch [41/100], Loss: 0.7406\n",
      "Epoch [42/100], Loss: 0.7389\n",
      "Epoch [43/100], Loss: 0.7319\n",
      "Epoch [44/100], Loss: 0.7389\n",
      "Epoch [45/100], Loss: 0.7281\n",
      "Epoch [46/100], Loss: 0.7292\n",
      "Epoch [47/100], Loss: 0.7325\n",
      "Epoch [48/100], Loss: 0.7295\n",
      "Epoch [49/100], Loss: 0.7281\n",
      "Epoch [50/100], Loss: 0.7217\n",
      "Epoch [51/100], Loss: 0.7163\n",
      "Epoch [52/100], Loss: 0.7170\n",
      "Epoch [53/100], Loss: 0.7205\n",
      "Epoch [54/100], Loss: 0.7097\n",
      "Epoch [55/100], Loss: 0.7152\n",
      "Epoch [56/100], Loss: 0.7074\n",
      "Epoch [57/100], Loss: 0.7091\n",
      "Epoch [58/100], Loss: 0.7005\n",
      "Epoch [59/100], Loss: 0.7084\n",
      "Epoch [60/100], Loss: 0.7039\n",
      "Epoch [61/100], Loss: 0.7069\n",
      "Epoch [62/100], Loss: 0.7002\n",
      "Epoch [63/100], Loss: 0.6935\n",
      "Epoch [64/100], Loss: 0.6906\n",
      "Epoch [65/100], Loss: 0.6903\n",
      "Epoch [66/100], Loss: 0.6941\n",
      "Epoch [67/100], Loss: 0.6834\n",
      "Epoch [68/100], Loss: 0.6922\n",
      "Epoch [69/100], Loss: 0.6824\n",
      "Epoch [70/100], Loss: 0.6795\n",
      "Epoch [71/100], Loss: 0.6782\n",
      "Epoch [72/100], Loss: 0.6709\n",
      "Epoch [73/100], Loss: 0.6853\n",
      "Epoch [74/100], Loss: 0.6744\n",
      "Epoch [75/100], Loss: 0.6726\n",
      "Epoch [76/100], Loss: 0.6743\n",
      "Epoch [77/100], Loss: 0.6666\n",
      "Epoch [78/100], Loss: 0.6646\n",
      "Epoch [79/100], Loss: 0.6625\n",
      "Epoch [80/100], Loss: 0.6587\n",
      "Epoch [81/100], Loss: 0.6625\n",
      "Epoch [82/100], Loss: 0.6604\n",
      "Epoch [83/100], Loss: 0.6673\n",
      "Epoch [84/100], Loss: 0.6601\n",
      "Epoch [85/100], Loss: 0.6522\n",
      "Epoch [86/100], Loss: 0.6510\n",
      "Epoch [87/100], Loss: 0.6440\n",
      "Epoch [88/100], Loss: 0.6569\n",
      "Epoch [89/100], Loss: 0.6527\n",
      "Epoch [90/100], Loss: 0.6461\n",
      "Epoch [91/100], Loss: 0.6486\n",
      "Epoch [92/100], Loss: 0.6450\n",
      "Epoch [93/100], Loss: 0.6481\n",
      "Epoch [94/100], Loss: 0.6449\n",
      "Epoch [95/100], Loss: 0.6435\n",
      "Epoch [96/100], Loss: 0.6419\n",
      "Epoch [97/100], Loss: 0.6396\n",
      "Epoch [98/100], Loss: 0.6408\n",
      "Epoch [99/100], Loss: 0.6449\n",
      "Epoch [100/100], Loss: 0.6379\n",
      "Subset 10000, Epoch [100/100], Loss: 0.6379\n",
      "Test Accuracy Base Logit: 64.98%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 1.5652\n",
      "Epoch [2/100], Loss: 1.1160\n",
      "Epoch [3/100], Loss: 1.0528\n",
      "Epoch [4/100], Loss: 1.0117\n",
      "Epoch [5/100], Loss: 0.9813\n",
      "Epoch [6/100], Loss: 0.9655\n",
      "Epoch [7/100], Loss: 0.9548\n",
      "Epoch [8/100], Loss: 0.9407\n",
      "Epoch [9/100], Loss: 0.9277\n",
      "Epoch [10/100], Loss: 0.9062\n",
      "Epoch [11/100], Loss: 0.8930\n",
      "Epoch [12/100], Loss: 0.8934\n",
      "Epoch [13/100], Loss: 0.8823\n",
      "Epoch [14/100], Loss: 0.8856\n",
      "Epoch [15/100], Loss: 0.8731\n",
      "Epoch [16/100], Loss: 0.8547\n",
      "Epoch [17/100], Loss: 0.8452\n",
      "Epoch [18/100], Loss: 0.8405\n",
      "Epoch [19/100], Loss: 0.8428\n",
      "Epoch [20/100], Loss: 0.8303\n",
      "Epoch [21/100], Loss: 0.8247\n",
      "Epoch [22/100], Loss: 0.8214\n",
      "Epoch [23/100], Loss: 0.8223\n",
      "Epoch [24/100], Loss: 0.8141\n",
      "Epoch [25/100], Loss: 0.8074\n",
      "Epoch [26/100], Loss: 0.8078\n",
      "Epoch [27/100], Loss: 0.8039\n",
      "Epoch [28/100], Loss: 0.7926\n",
      "Epoch [29/100], Loss: 0.7871\n",
      "Epoch [30/100], Loss: 0.7870\n",
      "Epoch [31/100], Loss: 0.7794\n",
      "Epoch [32/100], Loss: 0.7729\n",
      "Epoch [33/100], Loss: 0.7769\n",
      "Epoch [34/100], Loss: 0.7662\n",
      "Epoch [35/100], Loss: 0.7669\n",
      "Epoch [36/100], Loss: 0.7633\n",
      "Epoch [37/100], Loss: 0.7591\n",
      "Epoch [38/100], Loss: 0.7564\n",
      "Epoch [39/100], Loss: 0.7708\n",
      "Epoch [40/100], Loss: 0.7456\n",
      "Epoch [41/100], Loss: 0.7439\n",
      "Epoch [42/100], Loss: 0.7465\n",
      "Epoch [43/100], Loss: 0.7434\n",
      "Epoch [44/100], Loss: 0.7355\n",
      "Epoch [45/100], Loss: 0.7347\n",
      "Epoch [46/100], Loss: 0.7328\n",
      "Epoch [47/100], Loss: 0.7412\n",
      "Epoch [48/100], Loss: 0.7403\n",
      "Epoch [49/100], Loss: 0.7210\n",
      "Epoch [50/100], Loss: 0.7257\n",
      "Epoch [51/100], Loss: 0.7256\n",
      "Epoch [52/100], Loss: 0.7141\n",
      "Epoch [53/100], Loss: 0.7177\n",
      "Epoch [54/100], Loss: 0.7099\n",
      "Epoch [55/100], Loss: 0.7133\n",
      "Epoch [56/100], Loss: 0.7144\n",
      "Epoch [57/100], Loss: 0.7061\n",
      "Epoch [58/100], Loss: 0.7030\n",
      "Epoch [59/100], Loss: 0.7089\n",
      "Epoch [60/100], Loss: 0.7002\n",
      "Epoch [61/100], Loss: 0.7058\n",
      "Epoch [62/100], Loss: 0.7011\n",
      "Epoch [63/100], Loss: 0.7065\n",
      "Epoch [64/100], Loss: 0.7045\n",
      "Epoch [65/100], Loss: 0.6884\n",
      "Epoch [66/100], Loss: 0.6957\n",
      "Epoch [67/100], Loss: 0.6897\n",
      "Epoch [68/100], Loss: 0.6898\n",
      "Epoch [69/100], Loss: 0.6880\n",
      "Epoch [70/100], Loss: 0.6878\n",
      "Epoch [71/100], Loss: 0.6899\n",
      "Epoch [72/100], Loss: 0.6833\n",
      "Epoch [73/100], Loss: 0.6773\n",
      "Epoch [74/100], Loss: 0.6815\n",
      "Epoch [75/100], Loss: 0.6813\n",
      "Epoch [76/100], Loss: 0.6695\n",
      "Epoch [77/100], Loss: 0.6694\n",
      "Epoch [78/100], Loss: 0.6727\n",
      "Epoch [79/100], Loss: 0.6639\n",
      "Epoch [80/100], Loss: 0.6652\n",
      "Epoch [81/100], Loss: 0.6670\n",
      "Epoch [82/100], Loss: 0.6615\n",
      "Epoch [83/100], Loss: 0.6703\n",
      "Epoch [84/100], Loss: 0.6612\n",
      "Epoch [85/100], Loss: 0.6639\n",
      "Epoch [86/100], Loss: 0.6621\n",
      "Epoch [87/100], Loss: 0.6556\n",
      "Epoch [88/100], Loss: 0.6546\n",
      "Epoch [89/100], Loss: 0.6609\n",
      "Epoch [90/100], Loss: 0.6558\n",
      "Epoch [91/100], Loss: 0.6551\n",
      "Epoch [92/100], Loss: 0.6589\n",
      "Epoch [93/100], Loss: 0.6493\n",
      "Epoch [94/100], Loss: 0.6426\n",
      "Epoch [95/100], Loss: 0.6505\n",
      "Epoch [96/100], Loss: 0.6527\n",
      "Epoch [97/100], Loss: 0.6483\n",
      "Epoch [98/100], Loss: 0.6468\n",
      "Epoch [99/100], Loss: 0.6556\n",
      "Epoch [100/100], Loss: 0.6411\n",
      "Subset 10000, Epoch [100/100], Loss: 0.6411\n",
      "Test Accuracy Base Logit: 64.82%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 1.5401\n",
      "Epoch [2/100], Loss: 1.1127\n",
      "Epoch [3/100], Loss: 1.0443\n",
      "Epoch [4/100], Loss: 1.0150\n",
      "Epoch [5/100], Loss: 0.9863\n",
      "Epoch [6/100], Loss: 0.9686\n",
      "Epoch [7/100], Loss: 0.9327\n",
      "Epoch [8/100], Loss: 0.9173\n",
      "Epoch [9/100], Loss: 0.9232\n",
      "Epoch [10/100], Loss: 0.9088\n",
      "Epoch [11/100], Loss: 0.9013\n",
      "Epoch [12/100], Loss: 0.8790\n",
      "Epoch [13/100], Loss: 0.8730\n",
      "Epoch [14/100], Loss: 0.8676\n",
      "Epoch [15/100], Loss: 0.8593\n",
      "Epoch [16/100], Loss: 0.8504\n",
      "Epoch [17/100], Loss: 0.8394\n",
      "Epoch [18/100], Loss: 0.8420\n",
      "Epoch [19/100], Loss: 0.8187\n",
      "Epoch [20/100], Loss: 0.8129\n",
      "Epoch [21/100], Loss: 0.8181\n",
      "Epoch [22/100], Loss: 0.8118\n",
      "Epoch [23/100], Loss: 0.8044\n",
      "Epoch [24/100], Loss: 0.8027\n",
      "Epoch [25/100], Loss: 0.7947\n",
      "Epoch [26/100], Loss: 0.7917\n",
      "Epoch [27/100], Loss: 0.7851\n",
      "Epoch [28/100], Loss: 0.7908\n",
      "Epoch [29/100], Loss: 0.7869\n",
      "Epoch [30/100], Loss: 0.7874\n",
      "Epoch [31/100], Loss: 0.7895\n",
      "Epoch [32/100], Loss: 0.7665\n",
      "Epoch [33/100], Loss: 0.7667\n",
      "Epoch [34/100], Loss: 0.7627\n",
      "Epoch [35/100], Loss: 0.7556\n",
      "Epoch [36/100], Loss: 0.7496\n",
      "Epoch [37/100], Loss: 0.7490\n",
      "Epoch [38/100], Loss: 0.7603\n",
      "Epoch [39/100], Loss: 0.7469\n",
      "Epoch [40/100], Loss: 0.7605\n",
      "Epoch [41/100], Loss: 0.7431\n",
      "Epoch [42/100], Loss: 0.7427\n",
      "Epoch [43/100], Loss: 0.7478\n",
      "Epoch [44/100], Loss: 0.7350\n",
      "Epoch [45/100], Loss: 0.7269\n",
      "Epoch [46/100], Loss: 0.7227\n",
      "Epoch [47/100], Loss: 0.7216\n",
      "Epoch [48/100], Loss: 0.7187\n",
      "Epoch [49/100], Loss: 0.7199\n",
      "Epoch [50/100], Loss: 0.7138\n",
      "Epoch [51/100], Loss: 0.7142\n",
      "Epoch [52/100], Loss: 0.7099\n",
      "Epoch [53/100], Loss: 0.7034\n",
      "Epoch [54/100], Loss: 0.7102\n",
      "Epoch [55/100], Loss: 0.7052\n",
      "Epoch [56/100], Loss: 0.7072\n",
      "Epoch [57/100], Loss: 0.7052\n",
      "Epoch [58/100], Loss: 0.6985\n",
      "Epoch [59/100], Loss: 0.7012\n",
      "Epoch [60/100], Loss: 0.6999\n",
      "Epoch [61/100], Loss: 0.6920\n",
      "Epoch [62/100], Loss: 0.6868\n",
      "Epoch [63/100], Loss: 0.7020\n",
      "Epoch [64/100], Loss: 0.6905\n",
      "Epoch [65/100], Loss: 0.6868\n",
      "Epoch [66/100], Loss: 0.6899\n",
      "Epoch [67/100], Loss: 0.6817\n",
      "Epoch [68/100], Loss: 0.6815\n",
      "Epoch [69/100], Loss: 0.6725\n",
      "Epoch [70/100], Loss: 0.6750\n",
      "Epoch [71/100], Loss: 0.6784\n",
      "Epoch [72/100], Loss: 0.6705\n",
      "Epoch [73/100], Loss: 0.6730\n",
      "Epoch [74/100], Loss: 0.6698\n",
      "Epoch [75/100], Loss: 0.6711\n",
      "Epoch [76/100], Loss: 0.6711\n",
      "Epoch [77/100], Loss: 0.6637\n",
      "Epoch [78/100], Loss: 0.6640\n",
      "Epoch [79/100], Loss: 0.6610\n",
      "Epoch [80/100], Loss: 0.6605\n",
      "Epoch [81/100], Loss: 0.6635\n",
      "Epoch [82/100], Loss: 0.6673\n",
      "Stopping early at epoch 82 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [82/100], Loss: 0.6673\n",
      "Test Accuracy Base Logit: 65.23%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 1.5424\n",
      "Epoch [2/100], Loss: 1.1032\n",
      "Epoch [3/100], Loss: 1.0388\n",
      "Epoch [4/100], Loss: 0.9917\n",
      "Epoch [5/100], Loss: 0.9828\n",
      "Epoch [6/100], Loss: 0.9485\n",
      "Epoch [7/100], Loss: 0.9403\n",
      "Epoch [8/100], Loss: 0.9150\n",
      "Epoch [9/100], Loss: 0.9075\n",
      "Epoch [10/100], Loss: 0.8966\n",
      "Epoch [11/100], Loss: 0.8845\n",
      "Epoch [12/100], Loss: 0.8780\n",
      "Epoch [13/100], Loss: 0.8639\n",
      "Epoch [14/100], Loss: 0.8509\n",
      "Epoch [15/100], Loss: 0.8495\n",
      "Epoch [16/100], Loss: 0.8551\n",
      "Epoch [17/100], Loss: 0.8350\n",
      "Epoch [18/100], Loss: 0.8274\n",
      "Epoch [19/100], Loss: 0.8267\n",
      "Epoch [20/100], Loss: 0.8222\n",
      "Epoch [21/100], Loss: 0.8093\n",
      "Epoch [22/100], Loss: 0.7984\n",
      "Epoch [23/100], Loss: 0.8000\n",
      "Epoch [24/100], Loss: 0.8019\n",
      "Epoch [25/100], Loss: 0.8035\n",
      "Stopping early at epoch 25 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [25/100], Loss: 0.8035\n",
      "Test Accuracy Base Logit: 66.76%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 1.8141\n",
      "Epoch [2/100], Loss: 1.1813\n",
      "Epoch [3/100], Loss: 1.0789\n",
      "Epoch [4/100], Loss: 1.0142\n",
      "Epoch [5/100], Loss: 0.9891\n",
      "Epoch [6/100], Loss: 0.9341\n",
      "Epoch [7/100], Loss: 0.9126\n",
      "Epoch [8/100], Loss: 0.8956\n",
      "Epoch [9/100], Loss: 0.8770\n",
      "Epoch [10/100], Loss: 0.8824\n",
      "Epoch [11/100], Loss: 0.8391\n",
      "Epoch [12/100], Loss: 0.8296\n",
      "Epoch [13/100], Loss: 0.8052\n",
      "Epoch [14/100], Loss: 0.7788\n",
      "Epoch [15/100], Loss: 0.7774\n",
      "Epoch [16/100], Loss: 0.7500\n",
      "Epoch [17/100], Loss: 0.7518\n",
      "Epoch [18/100], Loss: 0.7274\n",
      "Epoch [19/100], Loss: 0.7167\n",
      "Epoch [20/100], Loss: 0.7066\n",
      "Epoch [21/100], Loss: 0.6985\n",
      "Epoch [22/100], Loss: 0.7024\n",
      "Epoch [23/100], Loss: 0.6954\n",
      "Epoch [24/100], Loss: 0.7082\n",
      "Epoch [25/100], Loss: 0.6820\n",
      "Epoch [26/100], Loss: 0.6715\n",
      "Epoch [27/100], Loss: 0.6579\n",
      "Epoch [28/100], Loss: 0.6713\n",
      "Epoch [29/100], Loss: 0.6705\n",
      "Epoch [30/100], Loss: 0.6538\n",
      "Epoch [31/100], Loss: 0.6522\n",
      "Epoch [32/100], Loss: 0.6552\n",
      "Epoch [33/100], Loss: 0.6382\n",
      "Epoch [34/100], Loss: 0.6401\n",
      "Epoch [35/100], Loss: 0.6607\n",
      "Epoch [36/100], Loss: 0.6225\n",
      "Epoch [37/100], Loss: 0.6128\n",
      "Epoch [38/100], Loss: 0.6201\n",
      "Epoch [39/100], Loss: 0.6251\n",
      "Epoch [40/100], Loss: 0.5927\n",
      "Epoch [41/100], Loss: 0.5902\n",
      "Epoch [42/100], Loss: 0.5723\n",
      "Epoch [43/100], Loss: 0.5806\n",
      "Epoch [44/100], Loss: 0.5728\n",
      "Epoch [45/100], Loss: 0.5727\n",
      "Epoch [46/100], Loss: 0.5973\n",
      "Epoch [47/100], Loss: 0.5855\n",
      "Epoch [48/100], Loss: 0.5830\n",
      "Epoch [49/100], Loss: 0.5627\n",
      "Epoch [50/100], Loss: 0.5529\n",
      "Epoch [51/100], Loss: 0.5684\n",
      "Epoch [52/100], Loss: 0.5642\n",
      "Epoch [53/100], Loss: 0.5739\n",
      "Epoch [54/100], Loss: 0.5383\n",
      "Epoch [55/100], Loss: 0.5437\n",
      "Epoch [56/100], Loss: 0.5365\n",
      "Epoch [57/100], Loss: 0.5262\n",
      "Epoch [58/100], Loss: 0.5276\n",
      "Epoch [59/100], Loss: 0.5324\n",
      "Epoch [60/100], Loss: 0.5226\n",
      "Epoch [61/100], Loss: 0.5123\n",
      "Epoch [62/100], Loss: 0.5505\n",
      "Epoch [63/100], Loss: 0.5192\n",
      "Epoch [64/100], Loss: 0.5149\n",
      "Epoch [65/100], Loss: 0.5076\n",
      "Epoch [66/100], Loss: 0.5289\n",
      "Epoch [67/100], Loss: 0.5172\n",
      "Epoch [68/100], Loss: 0.5039\n",
      "Epoch [69/100], Loss: 0.5237\n",
      "Epoch [70/100], Loss: 0.5209\n",
      "Epoch [71/100], Loss: 0.5070\n",
      "Epoch [72/100], Loss: 0.4976\n",
      "Epoch [73/100], Loss: 0.5014\n",
      "Epoch [74/100], Loss: 0.5019\n",
      "Epoch [75/100], Loss: 0.4884\n",
      "Epoch [76/100], Loss: 0.4762\n",
      "Epoch [77/100], Loss: 0.4698\n",
      "Epoch [78/100], Loss: 0.4772\n",
      "Epoch [79/100], Loss: 0.4844\n",
      "Epoch [80/100], Loss: 0.4839\n",
      "Stopping early at epoch 80 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [80/100], Loss: 0.4839\n",
      "Test Accuracy Base Logit: 63.27%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 1.8196\n",
      "Epoch [2/100], Loss: 1.1588\n",
      "Epoch [3/100], Loss: 1.0616\n",
      "Epoch [4/100], Loss: 1.0004\n",
      "Epoch [5/100], Loss: 0.9905\n",
      "Epoch [6/100], Loss: 0.9685\n",
      "Epoch [7/100], Loss: 0.9376\n",
      "Epoch [8/100], Loss: 0.8608\n",
      "Epoch [9/100], Loss: 0.8635\n",
      "Epoch [10/100], Loss: 0.8498\n",
      "Epoch [11/100], Loss: 0.8078\n",
      "Epoch [12/100], Loss: 0.8213\n",
      "Epoch [13/100], Loss: 0.8008\n",
      "Epoch [14/100], Loss: 0.7796\n",
      "Epoch [15/100], Loss: 0.7807\n",
      "Epoch [16/100], Loss: 0.7478\n",
      "Epoch [17/100], Loss: 0.7244\n",
      "Epoch [18/100], Loss: 0.7184\n",
      "Epoch [19/100], Loss: 0.7025\n",
      "Epoch [20/100], Loss: 0.7014\n",
      "Epoch [21/100], Loss: 0.7009\n",
      "Epoch [22/100], Loss: 0.7140\n",
      "Epoch [23/100], Loss: 0.6781\n",
      "Epoch [24/100], Loss: 0.6634\n",
      "Epoch [25/100], Loss: 0.6629\n",
      "Epoch [26/100], Loss: 0.6607\n",
      "Epoch [27/100], Loss: 0.6517\n",
      "Epoch [28/100], Loss: 0.6413\n",
      "Epoch [29/100], Loss: 0.6545\n",
      "Epoch [30/100], Loss: 0.6630\n",
      "Epoch [31/100], Loss: 0.6797\n",
      "Stopping early at epoch 31 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [31/100], Loss: 0.6797\n",
      "Test Accuracy Base Logit: 65.06%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 1.8205\n",
      "Epoch [2/100], Loss: 1.1763\n",
      "Epoch [3/100], Loss: 1.0791\n",
      "Epoch [4/100], Loss: 1.0061\n",
      "Epoch [5/100], Loss: 0.9546\n",
      "Epoch [6/100], Loss: 0.9128\n",
      "Epoch [7/100], Loss: 0.8952\n",
      "Epoch [8/100], Loss: 0.9079\n",
      "Epoch [9/100], Loss: 0.8751\n",
      "Epoch [10/100], Loss: 0.8425\n",
      "Epoch [11/100], Loss: 0.8097\n",
      "Epoch [12/100], Loss: 0.8148\n",
      "Epoch [13/100], Loss: 0.8141\n",
      "Epoch [14/100], Loss: 0.8049\n",
      "Epoch [15/100], Loss: 0.7711\n",
      "Epoch [16/100], Loss: 0.7648\n",
      "Epoch [17/100], Loss: 0.7590\n",
      "Epoch [18/100], Loss: 0.7723\n",
      "Epoch [19/100], Loss: 0.8162\n",
      "Epoch [20/100], Loss: 0.7501\n",
      "Epoch [21/100], Loss: 0.7277\n",
      "Epoch [22/100], Loss: 0.7275\n",
      "Epoch [23/100], Loss: 0.7313\n",
      "Epoch [24/100], Loss: 0.7104\n",
      "Epoch [25/100], Loss: 0.7103\n",
      "Epoch [26/100], Loss: 0.7035\n",
      "Epoch [27/100], Loss: 0.6792\n",
      "Epoch [28/100], Loss: 0.6676\n",
      "Epoch [29/100], Loss: 0.6776\n",
      "Epoch [30/100], Loss: 0.6955\n",
      "Epoch [31/100], Loss: 0.6571\n",
      "Epoch [32/100], Loss: 0.6487\n",
      "Epoch [33/100], Loss: 0.6437\n",
      "Epoch [34/100], Loss: 0.6422\n",
      "Epoch [35/100], Loss: 0.6403\n",
      "Epoch [36/100], Loss: 0.6695\n",
      "Epoch [37/100], Loss: 0.6677\n",
      "Epoch [38/100], Loss: 0.6301\n",
      "Epoch [39/100], Loss: 0.6224\n",
      "Epoch [40/100], Loss: 0.6219\n",
      "Epoch [41/100], Loss: 0.6291\n",
      "Epoch [42/100], Loss: 0.6199\n",
      "Epoch [43/100], Loss: 0.6147\n",
      "Epoch [44/100], Loss: 0.6246\n",
      "Epoch [45/100], Loss: 0.6170\n",
      "Epoch [46/100], Loss: 0.5990\n",
      "Epoch [47/100], Loss: 0.5996\n",
      "Epoch [48/100], Loss: 0.5846\n",
      "Epoch [49/100], Loss: 0.5920\n",
      "Epoch [50/100], Loss: 0.5908\n",
      "Epoch [51/100], Loss: 0.5737\n",
      "Epoch [52/100], Loss: 0.5739\n",
      "Epoch [53/100], Loss: 0.5671\n",
      "Epoch [54/100], Loss: 0.5661\n",
      "Epoch [55/100], Loss: 0.5576\n",
      "Epoch [56/100], Loss: 0.5431\n",
      "Epoch [57/100], Loss: 0.5441\n",
      "Epoch [58/100], Loss: 0.5496\n",
      "Epoch [59/100], Loss: 0.5463\n",
      "Epoch [60/100], Loss: 0.5452\n",
      "Epoch [61/100], Loss: 0.5593\n",
      "Epoch [62/100], Loss: 0.5371\n",
      "Epoch [63/100], Loss: 0.5271\n",
      "Epoch [64/100], Loss: 0.5331\n",
      "Epoch [65/100], Loss: 0.5337\n",
      "Epoch [66/100], Loss: 0.5244\n",
      "Epoch [67/100], Loss: 0.5189\n",
      "Epoch [68/100], Loss: 0.5166\n",
      "Epoch [69/100], Loss: 0.5359\n",
      "Epoch [70/100], Loss: 0.5358\n",
      "Epoch [71/100], Loss: 0.5116\n",
      "Epoch [72/100], Loss: 0.5273\n",
      "Epoch [73/100], Loss: 0.5088\n",
      "Epoch [74/100], Loss: 0.5326\n",
      "Epoch [75/100], Loss: 0.5181\n",
      "Epoch [76/100], Loss: 0.5488\n",
      "Epoch [77/100], Loss: 0.5292\n",
      "Epoch [78/100], Loss: 0.4960\n",
      "Epoch [79/100], Loss: 0.5162\n",
      "Epoch [80/100], Loss: 0.4960\n",
      "Epoch [81/100], Loss: 0.4835\n",
      "Epoch [82/100], Loss: 0.4877\n",
      "Epoch [83/100], Loss: 0.4927\n",
      "Epoch [84/100], Loss: 0.4986\n",
      "Stopping early at epoch 84 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [84/100], Loss: 0.4986\n",
      "Test Accuracy Base Logit: 62.86%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 1.8248\n",
      "Epoch [2/100], Loss: 1.1723\n",
      "Epoch [3/100], Loss: 1.0782\n",
      "Epoch [4/100], Loss: 1.0339\n",
      "Epoch [5/100], Loss: 1.0036\n",
      "Epoch [6/100], Loss: 0.9473\n",
      "Epoch [7/100], Loss: 0.9165\n",
      "Epoch [8/100], Loss: 0.8769\n",
      "Epoch [9/100], Loss: 0.8618\n",
      "Epoch [10/100], Loss: 0.8349\n",
      "Epoch [11/100], Loss: 0.8432\n",
      "Epoch [12/100], Loss: 0.8259\n",
      "Epoch [13/100], Loss: 0.8144\n",
      "Epoch [14/100], Loss: 0.7980\n",
      "Epoch [15/100], Loss: 0.7933\n",
      "Epoch [16/100], Loss: 0.7696\n",
      "Epoch [17/100], Loss: 0.7848\n",
      "Epoch [18/100], Loss: 0.7790\n",
      "Epoch [19/100], Loss: 0.7432\n",
      "Epoch [20/100], Loss: 0.7287\n",
      "Epoch [21/100], Loss: 0.7319\n",
      "Epoch [22/100], Loss: 0.7137\n",
      "Epoch [23/100], Loss: 0.6950\n",
      "Epoch [24/100], Loss: 0.6935\n",
      "Epoch [25/100], Loss: 0.6895\n",
      "Epoch [26/100], Loss: 0.7129\n",
      "Epoch [27/100], Loss: 0.6974\n",
      "Epoch [28/100], Loss: 0.6646\n",
      "Epoch [29/100], Loss: 0.6750\n",
      "Epoch [30/100], Loss: 0.6801\n",
      "Epoch [31/100], Loss: 0.6915\n",
      "Stopping early at epoch 31 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [31/100], Loss: 0.6915\n",
      "Test Accuracy Base Logit: 64.16%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 1.8788\n",
      "Epoch [2/100], Loss: 1.2357\n",
      "Epoch [3/100], Loss: 1.1023\n",
      "Epoch [4/100], Loss: 1.0299\n",
      "Epoch [5/100], Loss: 0.9947\n",
      "Epoch [6/100], Loss: 0.9737\n",
      "Epoch [7/100], Loss: 0.9630\n",
      "Epoch [8/100], Loss: 0.9176\n",
      "Epoch [9/100], Loss: 0.9021\n",
      "Epoch [10/100], Loss: 0.9119\n",
      "Epoch [11/100], Loss: 0.8827\n",
      "Epoch [12/100], Loss: 0.8495\n",
      "Epoch [13/100], Loss: 0.8323\n",
      "Epoch [14/100], Loss: 0.8011\n",
      "Epoch [15/100], Loss: 0.8265\n",
      "Epoch [16/100], Loss: 0.7895\n",
      "Epoch [17/100], Loss: 0.7731\n",
      "Epoch [18/100], Loss: 0.7710\n",
      "Epoch [19/100], Loss: 0.7583\n",
      "Epoch [20/100], Loss: 0.7482\n",
      "Epoch [21/100], Loss: 0.7540\n",
      "Epoch [22/100], Loss: 0.7494\n",
      "Epoch [23/100], Loss: 0.7314\n",
      "Epoch [24/100], Loss: 0.7343\n",
      "Epoch [25/100], Loss: 0.7086\n",
      "Epoch [26/100], Loss: 0.7020\n",
      "Epoch [27/100], Loss: 0.7076\n",
      "Epoch [28/100], Loss: 0.6881\n",
      "Epoch [29/100], Loss: 0.7046\n",
      "Epoch [30/100], Loss: 0.6689\n",
      "Epoch [31/100], Loss: 0.6717\n",
      "Epoch [32/100], Loss: 0.6553\n",
      "Epoch [33/100], Loss: 0.6858\n",
      "Epoch [34/100], Loss: 0.6765\n",
      "Epoch [35/100], Loss: 0.6533\n",
      "Epoch [36/100], Loss: 0.6473\n",
      "Epoch [37/100], Loss: 0.6521\n",
      "Epoch [38/100], Loss: 0.6474\n",
      "Epoch [39/100], Loss: 0.6229\n",
      "Epoch [40/100], Loss: 0.6256\n",
      "Epoch [41/100], Loss: 0.6387\n",
      "Epoch [42/100], Loss: 0.6274\n",
      "Epoch [43/100], Loss: 0.6007\n",
      "Epoch [44/100], Loss: 0.6003\n",
      "Epoch [45/100], Loss: 0.6073\n",
      "Epoch [46/100], Loss: 0.5992\n",
      "Epoch [47/100], Loss: 0.6001\n",
      "Epoch [48/100], Loss: 0.5929\n",
      "Epoch [49/100], Loss: 0.5865\n",
      "Epoch [50/100], Loss: 0.5963\n",
      "Epoch [51/100], Loss: 0.5871\n",
      "Epoch [52/100], Loss: 0.5867\n",
      "Epoch [53/100], Loss: 0.5745\n",
      "Epoch [54/100], Loss: 0.6069\n",
      "Epoch [55/100], Loss: 0.5955\n",
      "Epoch [56/100], Loss: 0.5784\n",
      "Epoch [57/100], Loss: 0.5609\n",
      "Epoch [58/100], Loss: 0.5467\n",
      "Epoch [59/100], Loss: 0.5433\n",
      "Epoch [60/100], Loss: 0.5307\n",
      "Epoch [61/100], Loss: 0.5523\n",
      "Epoch [62/100], Loss: 0.5443\n",
      "Epoch [63/100], Loss: 0.5506\n",
      "Epoch [64/100], Loss: 0.5629\n",
      "Epoch [65/100], Loss: 0.5421\n",
      "Epoch [66/100], Loss: 0.5436\n",
      "Epoch [67/100], Loss: 0.5287\n",
      "Epoch [68/100], Loss: 0.5256\n",
      "Epoch [69/100], Loss: 0.5285\n",
      "Epoch [70/100], Loss: 0.5341\n",
      "Epoch [71/100], Loss: 0.5322\n",
      "Epoch [72/100], Loss: 0.5460\n",
      "Epoch [73/100], Loss: 0.5401\n",
      "Epoch [74/100], Loss: 0.5178\n",
      "Epoch [75/100], Loss: 0.5233\n",
      "Epoch [76/100], Loss: 0.5279\n",
      "Epoch [77/100], Loss: 0.5136\n",
      "Epoch [78/100], Loss: 0.5099\n",
      "Epoch [79/100], Loss: 0.4991\n",
      "Epoch [80/100], Loss: 0.4866\n",
      "Epoch [81/100], Loss: 0.4972\n",
      "Epoch [82/100], Loss: 0.4914\n",
      "Epoch [83/100], Loss: 0.5025\n",
      "Epoch [84/100], Loss: 0.5034\n",
      "Epoch [85/100], Loss: 0.4917\n",
      "Epoch [86/100], Loss: 0.4928\n",
      "Epoch [87/100], Loss: 0.4863\n",
      "Epoch [88/100], Loss: 0.4893\n",
      "Epoch [89/100], Loss: 0.4863\n",
      "Epoch [90/100], Loss: 0.4723\n",
      "Epoch [91/100], Loss: 0.4750\n",
      "Epoch [92/100], Loss: 0.4754\n",
      "Epoch [93/100], Loss: 0.4766\n",
      "Stopping early at epoch 93 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [93/100], Loss: 0.4766\n",
      "Test Accuracy Base Logit: 61.64%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 2.8908\n",
      "Epoch [2/100], Loss: 1.7178\n",
      "Epoch [3/100], Loss: 1.1948\n",
      "Epoch [4/100], Loss: 1.0000\n",
      "Epoch [5/100], Loss: 0.8652\n",
      "Epoch [6/100], Loss: 0.7709\n",
      "Epoch [7/100], Loss: 0.7073\n",
      "Epoch [8/100], Loss: 0.6568\n",
      "Epoch [9/100], Loss: 0.6132\n",
      "Epoch [10/100], Loss: 0.5803\n",
      "Epoch [11/100], Loss: 0.5510\n",
      "Epoch [12/100], Loss: 0.5126\n",
      "Epoch [13/100], Loss: 0.4922\n",
      "Epoch [14/100], Loss: 0.4711\n",
      "Epoch [15/100], Loss: 0.4511\n",
      "Epoch [16/100], Loss: 0.4304\n",
      "Epoch [17/100], Loss: 0.4110\n",
      "Epoch [18/100], Loss: 0.3972\n",
      "Epoch [19/100], Loss: 0.3864\n",
      "Epoch [20/100], Loss: 0.3745\n",
      "Epoch [21/100], Loss: 0.3568\n",
      "Epoch [22/100], Loss: 0.3482\n",
      "Epoch [23/100], Loss: 0.3325\n",
      "Epoch [24/100], Loss: 0.3263\n",
      "Epoch [25/100], Loss: 0.3140\n",
      "Epoch [26/100], Loss: 0.3039\n",
      "Epoch [27/100], Loss: 0.2954\n",
      "Epoch [28/100], Loss: 0.2879\n",
      "Epoch [29/100], Loss: 0.2787\n",
      "Epoch [30/100], Loss: 0.2724\n",
      "Epoch [31/100], Loss: 0.2665\n",
      "Epoch [32/100], Loss: 0.2586\n",
      "Epoch [33/100], Loss: 0.2528\n",
      "Epoch [34/100], Loss: 0.2463\n",
      "Epoch [35/100], Loss: 0.2405\n",
      "Epoch [36/100], Loss: 0.2319\n",
      "Epoch [37/100], Loss: 0.2280\n",
      "Epoch [38/100], Loss: 0.2233\n",
      "Epoch [39/100], Loss: 0.2175\n",
      "Epoch [40/100], Loss: 0.2137\n",
      "Epoch [41/100], Loss: 0.2089\n",
      "Epoch [42/100], Loss: 0.2050\n",
      "Epoch [43/100], Loss: 0.1986\n",
      "Epoch [44/100], Loss: 0.1963\n",
      "Epoch [45/100], Loss: 0.1914\n",
      "Epoch [46/100], Loss: 0.1890\n",
      "Epoch [47/100], Loss: 0.1838\n",
      "Epoch [48/100], Loss: 0.1799\n",
      "Epoch [49/100], Loss: 0.1770\n",
      "Epoch [50/100], Loss: 0.1730\n",
      "Epoch [51/100], Loss: 0.1692\n",
      "Epoch [52/100], Loss: 0.1677\n",
      "Epoch [53/100], Loss: 0.1623\n",
      "Epoch [54/100], Loss: 0.1615\n",
      "Epoch [55/100], Loss: 0.1586\n",
      "Epoch [56/100], Loss: 0.1560\n",
      "Epoch [57/100], Loss: 0.1526\n",
      "Epoch [58/100], Loss: 0.1508\n",
      "Epoch [59/100], Loss: 0.1478\n",
      "Epoch [60/100], Loss: 0.1448\n",
      "Epoch [61/100], Loss: 0.1424\n",
      "Epoch [62/100], Loss: 0.1405\n",
      "Epoch [63/100], Loss: 0.1379\n",
      "Epoch [64/100], Loss: 0.1364\n",
      "Epoch [65/100], Loss: 0.1339\n",
      "Epoch [66/100], Loss: 0.1322\n",
      "Epoch [67/100], Loss: 0.1294\n",
      "Epoch [68/100], Loss: 0.1286\n",
      "Epoch [69/100], Loss: 0.1266\n",
      "Epoch [70/100], Loss: 0.1255\n",
      "Epoch [71/100], Loss: 0.1228\n",
      "Epoch [72/100], Loss: 0.1210\n",
      "Epoch [73/100], Loss: 0.1197\n",
      "Epoch [74/100], Loss: 0.1193\n",
      "Epoch [75/100], Loss: 0.1167\n",
      "Epoch [76/100], Loss: 0.1154\n",
      "Epoch [77/100], Loss: 0.1142\n",
      "Epoch [78/100], Loss: 0.1126\n",
      "Epoch [79/100], Loss: 0.1104\n",
      "Epoch [80/100], Loss: 0.1104\n",
      "Epoch [81/100], Loss: 0.1081\n",
      "Epoch [82/100], Loss: 0.1063\n",
      "Epoch [83/100], Loss: 0.1053\n",
      "Epoch [84/100], Loss: 0.1043\n",
      "Epoch [85/100], Loss: 0.1032\n",
      "Epoch [86/100], Loss: 0.1016\n",
      "Epoch [87/100], Loss: 0.1012\n",
      "Epoch [88/100], Loss: 0.1002\n",
      "Epoch [89/100], Loss: 0.0987\n",
      "Epoch [90/100], Loss: 0.0973\n",
      "Epoch [91/100], Loss: 0.0970\n",
      "Epoch [92/100], Loss: 0.0956\n",
      "Epoch [93/100], Loss: 0.0948\n",
      "Epoch [94/100], Loss: 0.0938\n",
      "Epoch [95/100], Loss: 0.0919\n",
      "Epoch [96/100], Loss: 0.0917\n",
      "Epoch [97/100], Loss: 0.0903\n",
      "Epoch [98/100], Loss: 0.0894\n",
      "Epoch [99/100], Loss: 0.0889\n",
      "Epoch [100/100], Loss: 0.0876\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0876\n",
      "Test Accuracy Base Logit: 57.20%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 2.9347\n",
      "Epoch [2/100], Loss: 1.7993\n",
      "Epoch [3/100], Loss: 1.3014\n",
      "Epoch [4/100], Loss: 1.0902\n",
      "Epoch [5/100], Loss: 0.9504\n",
      "Epoch [6/100], Loss: 0.8685\n",
      "Epoch [7/100], Loss: 0.7806\n",
      "Epoch [8/100], Loss: 0.7281\n",
      "Epoch [9/100], Loss: 0.6806\n",
      "Epoch [10/100], Loss: 0.6442\n",
      "Epoch [11/100], Loss: 0.6066\n",
      "Epoch [12/100], Loss: 0.5732\n",
      "Epoch [13/100], Loss: 0.5521\n",
      "Epoch [14/100], Loss: 0.5292\n",
      "Epoch [15/100], Loss: 0.5029\n",
      "Epoch [16/100], Loss: 0.4823\n",
      "Epoch [17/100], Loss: 0.4607\n",
      "Epoch [18/100], Loss: 0.4434\n",
      "Epoch [19/100], Loss: 0.4269\n",
      "Epoch [20/100], Loss: 0.4159\n",
      "Epoch [21/100], Loss: 0.3989\n",
      "Epoch [22/100], Loss: 0.3851\n",
      "Epoch [23/100], Loss: 0.3720\n",
      "Epoch [24/100], Loss: 0.3627\n",
      "Epoch [25/100], Loss: 0.3491\n",
      "Epoch [26/100], Loss: 0.3409\n",
      "Epoch [27/100], Loss: 0.3310\n",
      "Epoch [28/100], Loss: 0.3211\n",
      "Epoch [29/100], Loss: 0.3115\n",
      "Epoch [30/100], Loss: 0.3045\n",
      "Epoch [31/100], Loss: 0.2948\n",
      "Epoch [32/100], Loss: 0.2883\n",
      "Epoch [33/100], Loss: 0.2836\n",
      "Epoch [34/100], Loss: 0.2804\n",
      "Epoch [35/100], Loss: 0.2697\n",
      "Epoch [36/100], Loss: 0.2608\n",
      "Epoch [37/100], Loss: 0.2551\n",
      "Epoch [38/100], Loss: 0.2505\n",
      "Epoch [39/100], Loss: 0.2439\n",
      "Epoch [40/100], Loss: 0.2420\n",
      "Epoch [41/100], Loss: 0.2339\n",
      "Epoch [42/100], Loss: 0.2304\n",
      "Epoch [43/100], Loss: 0.2257\n",
      "Epoch [44/100], Loss: 0.2186\n",
      "Epoch [45/100], Loss: 0.2149\n",
      "Epoch [46/100], Loss: 0.2101\n",
      "Epoch [47/100], Loss: 0.2070\n",
      "Epoch [48/100], Loss: 0.2033\n",
      "Epoch [49/100], Loss: 0.2000\n",
      "Epoch [50/100], Loss: 0.1968\n",
      "Epoch [51/100], Loss: 0.1946\n",
      "Epoch [52/100], Loss: 0.1919\n",
      "Epoch [53/100], Loss: 0.1865\n",
      "Epoch [54/100], Loss: 0.1852\n",
      "Epoch [55/100], Loss: 0.1802\n",
      "Epoch [56/100], Loss: 0.1779\n",
      "Epoch [57/100], Loss: 0.1744\n",
      "Epoch [58/100], Loss: 0.1720\n",
      "Epoch [59/100], Loss: 0.1697\n",
      "Epoch [60/100], Loss: 0.1666\n",
      "Epoch [61/100], Loss: 0.1650\n",
      "Epoch [62/100], Loss: 0.1610\n",
      "Epoch [63/100], Loss: 0.1596\n",
      "Epoch [64/100], Loss: 0.1577\n",
      "Epoch [65/100], Loss: 0.1541\n",
      "Epoch [66/100], Loss: 0.1527\n",
      "Epoch [67/100], Loss: 0.1491\n",
      "Epoch [68/100], Loss: 0.1465\n",
      "Epoch [69/100], Loss: 0.1459\n",
      "Epoch [70/100], Loss: 0.1432\n",
      "Epoch [71/100], Loss: 0.1405\n",
      "Epoch [72/100], Loss: 0.1403\n",
      "Epoch [73/100], Loss: 0.1374\n",
      "Epoch [74/100], Loss: 0.1354\n",
      "Epoch [75/100], Loss: 0.1357\n",
      "Epoch [76/100], Loss: 0.1322\n",
      "Epoch [77/100], Loss: 0.1308\n",
      "Epoch [78/100], Loss: 0.1292\n",
      "Epoch [79/100], Loss: 0.1273\n",
      "Epoch [80/100], Loss: 0.1267\n",
      "Epoch [81/100], Loss: 0.1238\n",
      "Epoch [82/100], Loss: 0.1231\n",
      "Epoch [83/100], Loss: 0.1212\n",
      "Epoch [84/100], Loss: 0.1204\n",
      "Epoch [85/100], Loss: 0.1188\n",
      "Epoch [86/100], Loss: 0.1174\n",
      "Epoch [87/100], Loss: 0.1160\n",
      "Epoch [88/100], Loss: 0.1158\n",
      "Epoch [89/100], Loss: 0.1134\n",
      "Epoch [90/100], Loss: 0.1128\n",
      "Epoch [91/100], Loss: 0.1113\n",
      "Epoch [92/100], Loss: 0.1107\n",
      "Epoch [93/100], Loss: 0.1089\n",
      "Epoch [94/100], Loss: 0.1079\n",
      "Epoch [95/100], Loss: 0.1076\n",
      "Epoch [96/100], Loss: 0.1054\n",
      "Epoch [97/100], Loss: 0.1045\n",
      "Epoch [98/100], Loss: 0.1029\n",
      "Epoch [99/100], Loss: 0.1032\n",
      "Epoch [100/100], Loss: 0.1017\n",
      "Subset 1000, Epoch [100/100], Loss: 0.1017\n",
      "Test Accuracy Base Logit: 56.33%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 2.9267\n",
      "Epoch [2/100], Loss: 1.7941\n",
      "Epoch [3/100], Loss: 1.2791\n",
      "Epoch [4/100], Loss: 1.0783\n",
      "Epoch [5/100], Loss: 0.9337\n",
      "Epoch [6/100], Loss: 0.8391\n",
      "Epoch [7/100], Loss: 0.7633\n",
      "Epoch [8/100], Loss: 0.7076\n",
      "Epoch [9/100], Loss: 0.6656\n",
      "Epoch [10/100], Loss: 0.6246\n",
      "Epoch [11/100], Loss: 0.5942\n",
      "Epoch [12/100], Loss: 0.5619\n",
      "Epoch [13/100], Loss: 0.5325\n",
      "Epoch [14/100], Loss: 0.5106\n",
      "Epoch [15/100], Loss: 0.4862\n",
      "Epoch [16/100], Loss: 0.4700\n",
      "Epoch [17/100], Loss: 0.4535\n",
      "Epoch [18/100], Loss: 0.4341\n",
      "Epoch [19/100], Loss: 0.4180\n",
      "Epoch [20/100], Loss: 0.4070\n",
      "Epoch [21/100], Loss: 0.3948\n",
      "Epoch [22/100], Loss: 0.3794\n",
      "Epoch [23/100], Loss: 0.3704\n",
      "Epoch [24/100], Loss: 0.3524\n",
      "Epoch [25/100], Loss: 0.3462\n",
      "Epoch [26/100], Loss: 0.3340\n",
      "Epoch [27/100], Loss: 0.3230\n",
      "Epoch [28/100], Loss: 0.3133\n",
      "Epoch [29/100], Loss: 0.3070\n",
      "Epoch [30/100], Loss: 0.2963\n",
      "Epoch [31/100], Loss: 0.2906\n",
      "Epoch [32/100], Loss: 0.2829\n",
      "Epoch [33/100], Loss: 0.2813\n",
      "Epoch [34/100], Loss: 0.2722\n",
      "Epoch [35/100], Loss: 0.2627\n",
      "Epoch [36/100], Loss: 0.2599\n",
      "Epoch [37/100], Loss: 0.2500\n",
      "Epoch [38/100], Loss: 0.2496\n",
      "Epoch [39/100], Loss: 0.2412\n",
      "Epoch [40/100], Loss: 0.2338\n",
      "Epoch [41/100], Loss: 0.2295\n",
      "Epoch [42/100], Loss: 0.2252\n",
      "Epoch [43/100], Loss: 0.2219\n",
      "Epoch [44/100], Loss: 0.2160\n",
      "Epoch [45/100], Loss: 0.2138\n",
      "Epoch [46/100], Loss: 0.2062\n",
      "Epoch [47/100], Loss: 0.2041\n",
      "Epoch [48/100], Loss: 0.2018\n",
      "Epoch [49/100], Loss: 0.1957\n",
      "Epoch [50/100], Loss: 0.1957\n",
      "Epoch [51/100], Loss: 0.1938\n",
      "Epoch [52/100], Loss: 0.1899\n",
      "Epoch [53/100], Loss: 0.1835\n",
      "Epoch [54/100], Loss: 0.1813\n",
      "Epoch [55/100], Loss: 0.1784\n",
      "Epoch [56/100], Loss: 0.1757\n",
      "Epoch [57/100], Loss: 0.1727\n",
      "Epoch [58/100], Loss: 0.1702\n",
      "Epoch [59/100], Loss: 0.1669\n",
      "Epoch [60/100], Loss: 0.1645\n",
      "Epoch [61/100], Loss: 0.1613\n",
      "Epoch [62/100], Loss: 0.1598\n",
      "Epoch [63/100], Loss: 0.1577\n",
      "Epoch [64/100], Loss: 0.1560\n",
      "Epoch [65/100], Loss: 0.1540\n",
      "Epoch [66/100], Loss: 0.1530\n",
      "Epoch [67/100], Loss: 0.1494\n",
      "Epoch [68/100], Loss: 0.1490\n",
      "Epoch [69/100], Loss: 0.1445\n",
      "Epoch [70/100], Loss: 0.1433\n",
      "Epoch [71/100], Loss: 0.1423\n",
      "Epoch [72/100], Loss: 0.1402\n",
      "Epoch [73/100], Loss: 0.1387\n",
      "Epoch [74/100], Loss: 0.1362\n",
      "Epoch [75/100], Loss: 0.1346\n",
      "Epoch [76/100], Loss: 0.1340\n",
      "Epoch [77/100], Loss: 0.1308\n",
      "Epoch [78/100], Loss: 0.1304\n",
      "Epoch [79/100], Loss: 0.1287\n",
      "Epoch [80/100], Loss: 0.1277\n",
      "Epoch [81/100], Loss: 0.1257\n",
      "Epoch [82/100], Loss: 0.1241\n",
      "Epoch [83/100], Loss: 0.1235\n",
      "Epoch [84/100], Loss: 0.1218\n",
      "Epoch [85/100], Loss: 0.1203\n",
      "Epoch [86/100], Loss: 0.1185\n",
      "Epoch [87/100], Loss: 0.1188\n",
      "Epoch [88/100], Loss: 0.1164\n",
      "Epoch [89/100], Loss: 0.1154\n",
      "Epoch [90/100], Loss: 0.1159\n",
      "Epoch [91/100], Loss: 0.1129\n",
      "Epoch [92/100], Loss: 0.1117\n",
      "Epoch [93/100], Loss: 0.1107\n",
      "Epoch [94/100], Loss: 0.1110\n",
      "Epoch [95/100], Loss: 0.1091\n",
      "Epoch [96/100], Loss: 0.1083\n",
      "Epoch [97/100], Loss: 0.1085\n",
      "Epoch [98/100], Loss: 0.1056\n",
      "Epoch [99/100], Loss: 0.1056\n",
      "Epoch [100/100], Loss: 0.1057\n",
      "Subset 1000, Epoch [100/100], Loss: 0.1057\n",
      "Test Accuracy Base Logit: 57.63%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 2.9233\n",
      "Epoch [2/100], Loss: 1.7319\n",
      "Epoch [3/100], Loss: 1.2312\n",
      "Epoch [4/100], Loss: 1.0360\n",
      "Epoch [5/100], Loss: 0.9217\n",
      "Epoch [6/100], Loss: 0.8237\n",
      "Epoch [7/100], Loss: 0.7605\n",
      "Epoch [8/100], Loss: 0.7026\n",
      "Epoch [9/100], Loss: 0.6679\n",
      "Epoch [10/100], Loss: 0.6263\n",
      "Epoch [11/100], Loss: 0.5893\n",
      "Epoch [12/100], Loss: 0.5610\n",
      "Epoch [13/100], Loss: 0.5330\n",
      "Epoch [14/100], Loss: 0.5142\n",
      "Epoch [15/100], Loss: 0.4874\n",
      "Epoch [16/100], Loss: 0.4719\n",
      "Epoch [17/100], Loss: 0.4476\n",
      "Epoch [18/100], Loss: 0.4331\n",
      "Epoch [19/100], Loss: 0.4103\n",
      "Epoch [20/100], Loss: 0.3932\n",
      "Epoch [21/100], Loss: 0.3850\n",
      "Epoch [22/100], Loss: 0.3649\n",
      "Epoch [23/100], Loss: 0.3541\n",
      "Epoch [24/100], Loss: 0.3419\n",
      "Epoch [25/100], Loss: 0.3315\n",
      "Epoch [26/100], Loss: 0.3217\n",
      "Epoch [27/100], Loss: 0.3113\n",
      "Epoch [28/100], Loss: 0.3015\n",
      "Epoch [29/100], Loss: 0.2923\n",
      "Epoch [30/100], Loss: 0.2846\n",
      "Epoch [31/100], Loss: 0.2762\n",
      "Epoch [32/100], Loss: 0.2672\n",
      "Epoch [33/100], Loss: 0.2619\n",
      "Epoch [34/100], Loss: 0.2536\n",
      "Epoch [35/100], Loss: 0.2453\n",
      "Epoch [36/100], Loss: 0.2408\n",
      "Epoch [37/100], Loss: 0.2361\n",
      "Epoch [38/100], Loss: 0.2297\n",
      "Epoch [39/100], Loss: 0.2236\n",
      "Epoch [40/100], Loss: 0.2180\n",
      "Epoch [41/100], Loss: 0.2116\n",
      "Epoch [42/100], Loss: 0.2068\n",
      "Epoch [43/100], Loss: 0.2017\n",
      "Epoch [44/100], Loss: 0.1979\n",
      "Epoch [45/100], Loss: 0.1926\n",
      "Epoch [46/100], Loss: 0.1894\n",
      "Epoch [47/100], Loss: 0.1858\n",
      "Epoch [48/100], Loss: 0.1830\n",
      "Epoch [49/100], Loss: 0.1786\n",
      "Epoch [50/100], Loss: 0.1755\n",
      "Epoch [51/100], Loss: 0.1716\n",
      "Epoch [52/100], Loss: 0.1697\n",
      "Epoch [53/100], Loss: 0.1668\n",
      "Epoch [54/100], Loss: 0.1622\n",
      "Epoch [55/100], Loss: 0.1606\n",
      "Epoch [56/100], Loss: 0.1577\n",
      "Epoch [57/100], Loss: 0.1536\n",
      "Epoch [58/100], Loss: 0.1505\n",
      "Epoch [59/100], Loss: 0.1489\n",
      "Epoch [60/100], Loss: 0.1465\n",
      "Epoch [61/100], Loss: 0.1438\n",
      "Epoch [62/100], Loss: 0.1408\n",
      "Epoch [63/100], Loss: 0.1396\n",
      "Epoch [64/100], Loss: 0.1376\n",
      "Epoch [65/100], Loss: 0.1352\n",
      "Epoch [66/100], Loss: 0.1338\n",
      "Epoch [67/100], Loss: 0.1314\n",
      "Epoch [68/100], Loss: 0.1284\n",
      "Epoch [69/100], Loss: 0.1272\n",
      "Epoch [70/100], Loss: 0.1253\n",
      "Epoch [71/100], Loss: 0.1230\n",
      "Epoch [72/100], Loss: 0.1214\n",
      "Epoch [73/100], Loss: 0.1200\n",
      "Epoch [74/100], Loss: 0.1186\n",
      "Epoch [75/100], Loss: 0.1161\n",
      "Epoch [76/100], Loss: 0.1163\n",
      "Epoch [77/100], Loss: 0.1132\n",
      "Epoch [78/100], Loss: 0.1118\n",
      "Epoch [79/100], Loss: 0.1105\n",
      "Epoch [80/100], Loss: 0.1094\n",
      "Epoch [81/100], Loss: 0.1072\n",
      "Epoch [82/100], Loss: 0.1066\n",
      "Epoch [83/100], Loss: 0.1052\n",
      "Epoch [84/100], Loss: 0.1040\n",
      "Epoch [85/100], Loss: 0.1025\n",
      "Epoch [86/100], Loss: 0.1012\n",
      "Epoch [87/100], Loss: 0.1000\n",
      "Epoch [88/100], Loss: 0.0992\n",
      "Epoch [89/100], Loss: 0.0986\n",
      "Epoch [90/100], Loss: 0.0972\n",
      "Epoch [91/100], Loss: 0.0964\n",
      "Epoch [92/100], Loss: 0.0948\n",
      "Epoch [93/100], Loss: 0.0939\n",
      "Epoch [94/100], Loss: 0.0926\n",
      "Epoch [95/100], Loss: 0.0919\n",
      "Epoch [96/100], Loss: 0.0903\n",
      "Epoch [97/100], Loss: 0.0898\n",
      "Epoch [98/100], Loss: 0.0885\n",
      "Epoch [99/100], Loss: 0.0874\n",
      "Epoch [100/100], Loss: 0.0865\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0865\n",
      "Test Accuracy Base Logit: 55.99%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 2.9399\n",
      "Epoch [2/100], Loss: 1.7740\n",
      "Epoch [3/100], Loss: 1.2672\n",
      "Epoch [4/100], Loss: 1.0710\n",
      "Epoch [5/100], Loss: 0.9357\n",
      "Epoch [6/100], Loss: 0.8396\n",
      "Epoch [7/100], Loss: 0.7644\n",
      "Epoch [8/100], Loss: 0.7115\n",
      "Epoch [9/100], Loss: 0.6699\n",
      "Epoch [10/100], Loss: 0.6282\n",
      "Epoch [11/100], Loss: 0.5945\n",
      "Epoch [12/100], Loss: 0.5639\n",
      "Epoch [13/100], Loss: 0.5356\n",
      "Epoch [14/100], Loss: 0.5152\n",
      "Epoch [15/100], Loss: 0.4945\n",
      "Epoch [16/100], Loss: 0.4745\n",
      "Epoch [17/100], Loss: 0.4573\n",
      "Epoch [18/100], Loss: 0.4381\n",
      "Epoch [19/100], Loss: 0.4237\n",
      "Epoch [20/100], Loss: 0.4108\n",
      "Epoch [21/100], Loss: 0.3995\n",
      "Epoch [22/100], Loss: 0.3860\n",
      "Epoch [23/100], Loss: 0.3707\n",
      "Epoch [24/100], Loss: 0.3582\n",
      "Epoch [25/100], Loss: 0.3502\n",
      "Epoch [26/100], Loss: 0.3412\n",
      "Epoch [27/100], Loss: 0.3291\n",
      "Epoch [28/100], Loss: 0.3185\n",
      "Epoch [29/100], Loss: 0.3136\n",
      "Epoch [30/100], Loss: 0.3051\n",
      "Epoch [31/100], Loss: 0.2946\n",
      "Epoch [32/100], Loss: 0.2871\n",
      "Epoch [33/100], Loss: 0.2806\n",
      "Epoch [34/100], Loss: 0.2766\n",
      "Epoch [35/100], Loss: 0.2684\n",
      "Epoch [36/100], Loss: 0.2592\n",
      "Epoch [37/100], Loss: 0.2545\n",
      "Epoch [38/100], Loss: 0.2478\n",
      "Epoch [39/100], Loss: 0.2418\n",
      "Epoch [40/100], Loss: 0.2373\n",
      "Epoch [41/100], Loss: 0.2323\n",
      "Epoch [42/100], Loss: 0.2276\n",
      "Epoch [43/100], Loss: 0.2241\n",
      "Epoch [44/100], Loss: 0.2177\n",
      "Epoch [45/100], Loss: 0.2125\n",
      "Epoch [46/100], Loss: 0.2114\n",
      "Epoch [47/100], Loss: 0.2075\n",
      "Epoch [48/100], Loss: 0.2018\n",
      "Epoch [49/100], Loss: 0.1986\n",
      "Epoch [50/100], Loss: 0.1962\n",
      "Epoch [51/100], Loss: 0.1907\n",
      "Epoch [52/100], Loss: 0.1887\n",
      "Epoch [53/100], Loss: 0.1839\n",
      "Epoch [54/100], Loss: 0.1826\n",
      "Epoch [55/100], Loss: 0.1769\n",
      "Epoch [56/100], Loss: 0.1762\n",
      "Epoch [57/100], Loss: 0.1751\n",
      "Epoch [58/100], Loss: 0.1696\n",
      "Epoch [59/100], Loss: 0.1689\n",
      "Epoch [60/100], Loss: 0.1647\n",
      "Epoch [61/100], Loss: 0.1617\n",
      "Epoch [62/100], Loss: 0.1609\n",
      "Epoch [63/100], Loss: 0.1592\n",
      "Epoch [64/100], Loss: 0.1564\n",
      "Epoch [65/100], Loss: 0.1528\n",
      "Epoch [66/100], Loss: 0.1522\n",
      "Epoch [67/100], Loss: 0.1495\n",
      "Epoch [68/100], Loss: 0.1467\n",
      "Epoch [69/100], Loss: 0.1469\n",
      "Epoch [70/100], Loss: 0.1440\n",
      "Epoch [71/100], Loss: 0.1426\n",
      "Epoch [72/100], Loss: 0.1402\n",
      "Epoch [73/100], Loss: 0.1383\n",
      "Epoch [74/100], Loss: 0.1361\n",
      "Epoch [75/100], Loss: 0.1353\n",
      "Epoch [76/100], Loss: 0.1352\n",
      "Epoch [77/100], Loss: 0.1325\n",
      "Epoch [78/100], Loss: 0.1325\n",
      "Epoch [79/100], Loss: 0.1275\n",
      "Epoch [80/100], Loss: 0.1265\n",
      "Epoch [81/100], Loss: 0.1242\n",
      "Epoch [82/100], Loss: 0.1230\n",
      "Epoch [83/100], Loss: 0.1223\n",
      "Epoch [84/100], Loss: 0.1211\n",
      "Epoch [85/100], Loss: 0.1188\n",
      "Epoch [86/100], Loss: 0.1179\n",
      "Epoch [87/100], Loss: 0.1168\n",
      "Epoch [88/100], Loss: 0.1170\n",
      "Epoch [89/100], Loss: 0.1192\n",
      "Epoch [90/100], Loss: 0.1148\n",
      "Epoch [91/100], Loss: 0.1144\n",
      "Epoch [92/100], Loss: 0.1126\n",
      "Epoch [93/100], Loss: 0.1128\n",
      "Epoch [94/100], Loss: 0.1117\n",
      "Epoch [95/100], Loss: 0.1080\n",
      "Epoch [96/100], Loss: 0.1077\n",
      "Epoch [97/100], Loss: 0.1060\n",
      "Epoch [98/100], Loss: 0.1049\n",
      "Epoch [99/100], Loss: 0.1052\n",
      "Epoch [100/100], Loss: 0.1039\n",
      "Subset 1000, Epoch [100/100], Loss: 0.1039\n",
      "Test Accuracy Base Logit: 55.39%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "logit_accuracy = {}\n",
    "\n",
    "for subset_size in num_subsets:\n",
    "    accuracy_list = []\n",
    "\n",
    "    for _ in range(5):\n",
    "        logit_model = LogisticRegression(input_dim, num_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_sdg = optim.SGD(logit_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "        print(f\"Training with subset size: {subset_size}\")\n",
    "        subset_indices = torch.randperm(train_data.shape[0])[:subset_size]\n",
    "        subset = SubsetRandomSampler(subset_indices)\n",
    "        train_loader = DataLoader(train_set, batch_size=128, sampler=subset)\n",
    "\n",
    "        previous_loss = float('inf')  # Track the best loss\n",
    "        epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            logit_model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = logit_model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer_sdg.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer_sdg.step()\n",
    "\n",
    "                total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "            current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "            # Check for early stopping\n",
    "            if previous_loss - current_loss < tolerance:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "                    break\n",
    "            else:\n",
    "                epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "            previous_loss = current_loss\n",
    "\n",
    "        print(f\"Subset {subset_size}, Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        logit_model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(-1, 28*28)\n",
    "                outputs = logit_model(images)\n",
    "                _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Test Accuracy Base Logit: {accuracy:.2f}%\")\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    logit_accuracy[subset_size] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000    69.615385\n",
       " 50000    68.868269\n",
       " 10000    65.370192\n",
       " 5000     63.398077\n",
       " 1000     56.510577\n",
       " dtype: float64,\n",
       " 75000    0.235037\n",
       " 50000    0.262330\n",
       " 10000    0.790387\n",
       " 5000     1.297638\n",
       " 1000     0.906950\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_df = pd.DataFrame(logit_accuracy)\n",
    "logit_df.mean(), logit_df.std(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipschitz Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 1.1686\n",
      "Epoch [2/100], Loss: 1.0577\n",
      "Epoch [3/100], Loss: 1.0425\n",
      "Epoch [4/100], Loss: 1.0326\n",
      "Epoch [5/100], Loss: 1.0279\n",
      "Epoch [6/100], Loss: 1.0209\n",
      "Epoch [7/100], Loss: 1.0175\n",
      "Epoch [8/100], Loss: 1.0148\n",
      "Epoch [9/100], Loss: 1.0111\n",
      "Epoch [10/100], Loss: 1.0096\n",
      "Epoch [11/100], Loss: 1.0072\n",
      "Epoch [12/100], Loss: 1.0062\n",
      "Epoch [13/100], Loss: 1.0040\n",
      "Epoch [14/100], Loss: 1.0015\n",
      "Epoch [15/100], Loss: 1.0006\n",
      "Epoch [16/100], Loss: 0.9993\n",
      "Epoch [17/100], Loss: 0.9996\n",
      "Epoch [18/100], Loss: 0.9966\n",
      "Epoch [19/100], Loss: 0.9943\n",
      "Epoch [20/100], Loss: 0.9953\n",
      "Epoch [21/100], Loss: 0.9939\n",
      "Epoch [22/100], Loss: 0.9955\n",
      "Epoch [23/100], Loss: 0.9936\n",
      "Epoch [24/100], Loss: 0.9919\n",
      "Epoch [25/100], Loss: 0.9922\n",
      "Epoch [26/100], Loss: 0.9907\n",
      "Epoch [27/100], Loss: 0.9902\n",
      "Epoch [28/100], Loss: 0.9886\n",
      "Epoch [29/100], Loss: 0.9868\n",
      "Epoch [30/100], Loss: 0.9859\n",
      "Epoch [31/100], Loss: 0.9877\n",
      "Epoch [32/100], Loss: 0.9855\n",
      "Epoch [33/100], Loss: 0.9880\n",
      "Epoch [34/100], Loss: 0.9843\n",
      "Epoch [35/100], Loss: 0.9854\n",
      "Epoch [36/100], Loss: 0.9854\n",
      "Epoch [37/100], Loss: 0.9834\n",
      "Epoch [38/100], Loss: 0.9836\n",
      "Epoch [39/100], Loss: 0.9829\n",
      "Epoch [40/100], Loss: 0.9852\n",
      "Stopping early at epoch 40 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [40/100], Loss: 0.9852\n",
      "Test Accuracy Logit Lipschitz: 68.70%\n",
      "Epoch [1/100], Loss: 1.1727\n",
      "Epoch [2/100], Loss: 1.0603\n",
      "Epoch [3/100], Loss: 1.0429\n",
      "Epoch [4/100], Loss: 1.0357\n",
      "Epoch [5/100], Loss: 1.0306\n",
      "Epoch [6/100], Loss: 1.0209\n",
      "Epoch [7/100], Loss: 1.0192\n",
      "Epoch [8/100], Loss: 1.0166\n",
      "Epoch [9/100], Loss: 1.0125\n",
      "Epoch [10/100], Loss: 1.0084\n",
      "Epoch [11/100], Loss: 1.0052\n",
      "Epoch [12/100], Loss: 1.0055\n",
      "Epoch [13/100], Loss: 1.0026\n",
      "Epoch [14/100], Loss: 1.0011\n",
      "Epoch [15/100], Loss: 1.0003\n",
      "Epoch [16/100], Loss: 0.9996\n",
      "Epoch [17/100], Loss: 1.0005\n",
      "Stopping early at epoch 17 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [17/100], Loss: 1.0005\n",
      "Test Accuracy Logit Lipschitz: 70.15%\n",
      "Epoch [1/100], Loss: 1.1613\n",
      "Epoch [2/100], Loss: 1.0538\n",
      "Epoch [3/100], Loss: 1.0330\n",
      "Epoch [4/100], Loss: 1.0251\n",
      "Epoch [5/100], Loss: 1.0190\n",
      "Epoch [6/100], Loss: 1.0140\n",
      "Epoch [7/100], Loss: 1.0106\n",
      "Epoch [8/100], Loss: 1.0093\n",
      "Epoch [9/100], Loss: 1.0038\n",
      "Epoch [10/100], Loss: 1.0000\n",
      "Epoch [11/100], Loss: 0.9992\n",
      "Epoch [12/100], Loss: 0.9994\n",
      "Epoch [13/100], Loss: 0.9965\n",
      "Epoch [14/100], Loss: 0.9949\n",
      "Epoch [15/100], Loss: 0.9949\n",
      "Epoch [16/100], Loss: 0.9912\n",
      "Epoch [17/100], Loss: 0.9911\n",
      "Epoch [18/100], Loss: 0.9885\n",
      "Epoch [19/100], Loss: 0.9905\n",
      "Epoch [20/100], Loss: 0.9863\n",
      "Epoch [21/100], Loss: 0.9867\n",
      "Epoch [22/100], Loss: 0.9861\n",
      "Epoch [23/100], Loss: 0.9831\n",
      "Epoch [24/100], Loss: 0.9832\n",
      "Epoch [25/100], Loss: 0.9817\n",
      "Epoch [26/100], Loss: 0.9808\n",
      "Epoch [27/100], Loss: 0.9862\n",
      "Epoch [28/100], Loss: 0.9820\n",
      "Epoch [29/100], Loss: 0.9808\n",
      "Epoch [30/100], Loss: 0.9788\n",
      "Epoch [31/100], Loss: 0.9808\n",
      "Epoch [32/100], Loss: 0.9808\n",
      "Epoch [33/100], Loss: 0.9788\n",
      "Epoch [34/100], Loss: 0.9775\n",
      "Epoch [35/100], Loss: 0.9778\n",
      "Epoch [36/100], Loss: 0.9805\n",
      "Epoch [37/100], Loss: 0.9772\n",
      "Epoch [38/100], Loss: 0.9779\n",
      "Epoch [39/100], Loss: 0.9763\n",
      "Epoch [40/100], Loss: 0.9764\n",
      "Epoch [41/100], Loss: 0.9754\n",
      "Epoch [42/100], Loss: 0.9759\n",
      "Stopping early at epoch 42 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [42/100], Loss: 0.9759\n",
      "Test Accuracy Logit Lipschitz: 69.79%\n",
      "Epoch [1/100], Loss: 1.1727\n",
      "Epoch [2/100], Loss: 1.0591\n",
      "Epoch [3/100], Loss: 1.0459\n",
      "Epoch [4/100], Loss: 1.0330\n",
      "Epoch [5/100], Loss: 1.0262\n",
      "Epoch [6/100], Loss: 1.0221\n",
      "Epoch [7/100], Loss: 1.0188\n",
      "Epoch [8/100], Loss: 1.0176\n",
      "Epoch [9/100], Loss: 1.0137\n",
      "Epoch [10/100], Loss: 1.0100\n",
      "Epoch [11/100], Loss: 1.0092\n",
      "Epoch [12/100], Loss: 1.0071\n",
      "Epoch [13/100], Loss: 1.0036\n",
      "Epoch [14/100], Loss: 1.0050\n",
      "Epoch [15/100], Loss: 1.0004\n",
      "Epoch [16/100], Loss: 1.0020\n",
      "Epoch [17/100], Loss: 0.9983\n",
      "Epoch [18/100], Loss: 0.9996\n",
      "Epoch [19/100], Loss: 0.9971\n",
      "Epoch [20/100], Loss: 0.9941\n",
      "Epoch [21/100], Loss: 0.9952\n",
      "Epoch [22/100], Loss: 0.9953\n",
      "Epoch [23/100], Loss: 0.9949\n",
      "Stopping early at epoch 23 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [23/100], Loss: 0.9949\n",
      "Test Accuracy Logit Lipschitz: 70.45%\n",
      "Epoch [1/100], Loss: 1.1723\n",
      "Epoch [2/100], Loss: 1.0614\n",
      "Epoch [3/100], Loss: 1.0440\n",
      "Epoch [4/100], Loss: 1.0387\n",
      "Epoch [5/100], Loss: 1.0318\n",
      "Epoch [6/100], Loss: 1.0280\n",
      "Epoch [7/100], Loss: 1.0244\n",
      "Epoch [8/100], Loss: 1.0195\n",
      "Epoch [9/100], Loss: 1.0165\n",
      "Epoch [10/100], Loss: 1.0162\n",
      "Epoch [11/100], Loss: 1.0123\n",
      "Epoch [12/100], Loss: 1.0092\n",
      "Epoch [13/100], Loss: 1.0081\n",
      "Epoch [14/100], Loss: 1.0087\n",
      "Epoch [15/100], Loss: 1.0065\n",
      "Epoch [16/100], Loss: 1.0055\n",
      "Epoch [17/100], Loss: 1.0079\n",
      "Epoch [18/100], Loss: 1.0037\n",
      "Epoch [19/100], Loss: 1.0008\n",
      "Epoch [20/100], Loss: 1.0030\n",
      "Epoch [21/100], Loss: 1.0033\n",
      "Epoch [22/100], Loss: 0.9995\n",
      "Epoch [23/100], Loss: 0.9986\n",
      "Epoch [24/100], Loss: 0.9977\n",
      "Epoch [25/100], Loss: 0.9985\n",
      "Stopping early at epoch 25 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [25/100], Loss: 0.9985\n",
      "Test Accuracy Logit Lipschitz: 69.04%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 1.2202\n",
      "Epoch [2/100], Loss: 1.0690\n",
      "Epoch [3/100], Loss: 1.0490\n",
      "Epoch [4/100], Loss: 1.0364\n",
      "Epoch [5/100], Loss: 1.0261\n",
      "Epoch [6/100], Loss: 1.0187\n",
      "Epoch [7/100], Loss: 1.0157\n",
      "Epoch [8/100], Loss: 1.0101\n",
      "Epoch [9/100], Loss: 1.0086\n",
      "Epoch [10/100], Loss: 1.0013\n",
      "Epoch [11/100], Loss: 0.9999\n",
      "Epoch [12/100], Loss: 0.9976\n",
      "Epoch [13/100], Loss: 0.9945\n",
      "Epoch [14/100], Loss: 0.9920\n",
      "Epoch [15/100], Loss: 0.9862\n",
      "Epoch [16/100], Loss: 0.9876\n",
      "Epoch [17/100], Loss: 0.9865\n",
      "Epoch [18/100], Loss: 0.9841\n",
      "Epoch [19/100], Loss: 0.9830\n",
      "Epoch [20/100], Loss: 0.9823\n",
      "Epoch [21/100], Loss: 0.9816\n",
      "Epoch [22/100], Loss: 0.9792\n",
      "Epoch [23/100], Loss: 0.9807\n",
      "Epoch [24/100], Loss: 0.9775\n",
      "Epoch [25/100], Loss: 0.9756\n",
      "Epoch [26/100], Loss: 0.9731\n",
      "Epoch [27/100], Loss: 0.9732\n",
      "Epoch [28/100], Loss: 0.9709\n",
      "Epoch [29/100], Loss: 0.9693\n",
      "Epoch [30/100], Loss: 0.9675\n",
      "Epoch [31/100], Loss: 0.9687\n",
      "Epoch [32/100], Loss: 0.9712\n",
      "Epoch [33/100], Loss: 0.9674\n",
      "Epoch [34/100], Loss: 0.9649\n",
      "Epoch [35/100], Loss: 0.9694\n",
      "Epoch [36/100], Loss: 0.9669\n",
      "Epoch [37/100], Loss: 0.9643\n",
      "Epoch [38/100], Loss: 0.9624\n",
      "Epoch [39/100], Loss: 0.9639\n",
      "Epoch [40/100], Loss: 0.9667\n",
      "Epoch [41/100], Loss: 0.9641\n",
      "Epoch [42/100], Loss: 0.9626\n",
      "Epoch [43/100], Loss: 0.9616\n",
      "Epoch [44/100], Loss: 0.9621\n",
      "Epoch [45/100], Loss: 0.9580\n",
      "Epoch [46/100], Loss: 0.9591\n",
      "Epoch [47/100], Loss: 0.9590\n",
      "Epoch [48/100], Loss: 0.9575\n",
      "Epoch [49/100], Loss: 0.9586\n",
      "Epoch [50/100], Loss: 0.9604\n",
      "Epoch [51/100], Loss: 0.9582\n",
      "Epoch [52/100], Loss: 0.9594\n",
      "Epoch [53/100], Loss: 0.9554\n",
      "Epoch [54/100], Loss: 0.9558\n",
      "Epoch [55/100], Loss: 0.9560\n",
      "Epoch [56/100], Loss: 0.9522\n",
      "Epoch [57/100], Loss: 0.9554\n",
      "Epoch [58/100], Loss: 0.9537\n",
      "Epoch [59/100], Loss: 0.9558\n",
      "Epoch [60/100], Loss: 0.9537\n",
      "Epoch [61/100], Loss: 0.9541\n",
      "Epoch [62/100], Loss: 0.9522\n",
      "Epoch [63/100], Loss: 0.9541\n",
      "Epoch [64/100], Loss: 0.9532\n",
      "Epoch [65/100], Loss: 0.9504\n",
      "Epoch [66/100], Loss: 0.9519\n",
      "Epoch [67/100], Loss: 0.9515\n",
      "Epoch [68/100], Loss: 0.9524\n",
      "Stopping early at epoch 68 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [68/100], Loss: 0.9524\n",
      "Test Accuracy Logit Lipschitz: 69.03%\n",
      "Epoch [1/100], Loss: 1.2146\n",
      "Epoch [2/100], Loss: 1.0692\n",
      "Epoch [3/100], Loss: 1.0503\n",
      "Epoch [4/100], Loss: 1.0330\n",
      "Epoch [5/100], Loss: 1.0287\n",
      "Epoch [6/100], Loss: 1.0213\n",
      "Epoch [7/100], Loss: 1.0186\n",
      "Epoch [8/100], Loss: 1.0148\n",
      "Epoch [9/100], Loss: 1.0074\n",
      "Epoch [10/100], Loss: 1.0025\n",
      "Epoch [11/100], Loss: 1.0028\n",
      "Epoch [12/100], Loss: 0.9994\n",
      "Epoch [13/100], Loss: 0.9933\n",
      "Epoch [14/100], Loss: 0.9935\n",
      "Epoch [15/100], Loss: 0.9901\n",
      "Epoch [16/100], Loss: 0.9879\n",
      "Epoch [17/100], Loss: 0.9860\n",
      "Epoch [18/100], Loss: 0.9825\n",
      "Epoch [19/100], Loss: 0.9871\n",
      "Epoch [20/100], Loss: 0.9820\n",
      "Epoch [21/100], Loss: 0.9830\n",
      "Epoch [22/100], Loss: 0.9833\n",
      "Epoch [23/100], Loss: 0.9769\n",
      "Epoch [24/100], Loss: 0.9775\n",
      "Epoch [25/100], Loss: 0.9769\n",
      "Epoch [26/100], Loss: 0.9762\n",
      "Stopping early at epoch 26 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [26/100], Loss: 0.9762\n",
      "Test Accuracy Logit Lipschitz: 69.60%\n",
      "Epoch [1/100], Loss: 1.2046\n",
      "Epoch [2/100], Loss: 1.0555\n",
      "Epoch [3/100], Loss: 1.0342\n",
      "Epoch [4/100], Loss: 1.0228\n",
      "Epoch [5/100], Loss: 1.0095\n",
      "Epoch [6/100], Loss: 1.0061\n",
      "Epoch [7/100], Loss: 1.0031\n",
      "Epoch [8/100], Loss: 0.9950\n",
      "Epoch [9/100], Loss: 0.9899\n",
      "Epoch [10/100], Loss: 0.9872\n",
      "Epoch [11/100], Loss: 0.9851\n",
      "Epoch [12/100], Loss: 0.9807\n",
      "Epoch [13/100], Loss: 0.9783\n",
      "Epoch [14/100], Loss: 0.9807\n",
      "Epoch [15/100], Loss: 0.9768\n",
      "Epoch [16/100], Loss: 0.9741\n",
      "Epoch [17/100], Loss: 0.9704\n",
      "Epoch [18/100], Loss: 0.9672\n",
      "Epoch [19/100], Loss: 0.9714\n",
      "Epoch [20/100], Loss: 0.9657\n",
      "Epoch [21/100], Loss: 0.9642\n",
      "Epoch [22/100], Loss: 0.9655\n",
      "Epoch [23/100], Loss: 0.9620\n",
      "Epoch [24/100], Loss: 0.9616\n",
      "Epoch [25/100], Loss: 0.9586\n",
      "Epoch [26/100], Loss: 0.9590\n",
      "Epoch [27/100], Loss: 0.9581\n",
      "Epoch [28/100], Loss: 0.9578\n",
      "Stopping early at epoch 28 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [28/100], Loss: 0.9578\n",
      "Test Accuracy Logit Lipschitz: 69.50%\n",
      "Epoch [1/100], Loss: 1.2138\n",
      "Epoch [2/100], Loss: 1.0667\n",
      "Epoch [3/100], Loss: 1.0439\n",
      "Epoch [4/100], Loss: 1.0327\n",
      "Epoch [5/100], Loss: 1.0217\n",
      "Epoch [6/100], Loss: 1.0173\n",
      "Epoch [7/100], Loss: 1.0122\n",
      "Epoch [8/100], Loss: 1.0087\n",
      "Epoch [9/100], Loss: 1.0054\n",
      "Epoch [10/100], Loss: 0.9981\n",
      "Epoch [11/100], Loss: 0.9975\n",
      "Epoch [12/100], Loss: 0.9939\n",
      "Epoch [13/100], Loss: 0.9919\n",
      "Epoch [14/100], Loss: 0.9894\n",
      "Epoch [15/100], Loss: 0.9907\n",
      "Epoch [16/100], Loss: 0.9863\n",
      "Epoch [17/100], Loss: 0.9838\n",
      "Epoch [18/100], Loss: 0.9832\n",
      "Epoch [19/100], Loss: 0.9806\n",
      "Epoch [20/100], Loss: 0.9813\n",
      "Epoch [21/100], Loss: 0.9795\n",
      "Epoch [22/100], Loss: 0.9779\n",
      "Epoch [23/100], Loss: 0.9781\n",
      "Epoch [24/100], Loss: 0.9738\n",
      "Epoch [25/100], Loss: 0.9735\n",
      "Epoch [26/100], Loss: 0.9702\n",
      "Epoch [27/100], Loss: 0.9690\n",
      "Epoch [28/100], Loss: 0.9712\n",
      "Epoch [29/100], Loss: 0.9695\n",
      "Epoch [30/100], Loss: 0.9691\n",
      "Epoch [31/100], Loss: 0.9686\n",
      "Epoch [32/100], Loss: 0.9626\n",
      "Epoch [33/100], Loss: 0.9670\n",
      "Epoch [34/100], Loss: 0.9650\n",
      "Epoch [35/100], Loss: 0.9649\n",
      "Epoch [36/100], Loss: 0.9642\n",
      "Epoch [37/100], Loss: 0.9586\n",
      "Epoch [38/100], Loss: 0.9575\n",
      "Epoch [39/100], Loss: 0.9608\n",
      "Epoch [40/100], Loss: 0.9605\n",
      "Epoch [41/100], Loss: 0.9609\n",
      "Stopping early at epoch 41 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [41/100], Loss: 0.9609\n",
      "Test Accuracy Logit Lipschitz: 68.92%\n",
      "Epoch [1/100], Loss: 1.2004\n",
      "Epoch [2/100], Loss: 1.0585\n",
      "Epoch [3/100], Loss: 1.0352\n",
      "Epoch [4/100], Loss: 1.0252\n",
      "Epoch [5/100], Loss: 1.0182\n",
      "Epoch [6/100], Loss: 1.0110\n",
      "Epoch [7/100], Loss: 1.0049\n",
      "Epoch [8/100], Loss: 0.9992\n",
      "Epoch [9/100], Loss: 0.9983\n",
      "Epoch [10/100], Loss: 0.9941\n",
      "Epoch [11/100], Loss: 0.9889\n",
      "Epoch [12/100], Loss: 0.9865\n",
      "Epoch [13/100], Loss: 0.9877\n",
      "Epoch [14/100], Loss: 0.9820\n",
      "Epoch [15/100], Loss: 0.9801\n",
      "Epoch [16/100], Loss: 0.9792\n",
      "Epoch [17/100], Loss: 0.9765\n",
      "Epoch [18/100], Loss: 0.9752\n",
      "Epoch [19/100], Loss: 0.9729\n",
      "Epoch [20/100], Loss: 0.9735\n",
      "Epoch [21/100], Loss: 0.9689\n",
      "Epoch [22/100], Loss: 0.9672\n",
      "Epoch [23/100], Loss: 0.9660\n",
      "Epoch [24/100], Loss: 0.9631\n",
      "Epoch [25/100], Loss: 0.9657\n",
      "Epoch [26/100], Loss: 0.9683\n",
      "Epoch [27/100], Loss: 0.9650\n",
      "Epoch [28/100], Loss: 0.9624\n",
      "Epoch [29/100], Loss: 0.9620\n",
      "Epoch [30/100], Loss: 0.9590\n",
      "Epoch [31/100], Loss: 0.9577\n",
      "Epoch [32/100], Loss: 0.9581\n",
      "Epoch [33/100], Loss: 0.9564\n",
      "Epoch [34/100], Loss: 0.9560\n",
      "Epoch [35/100], Loss: 0.9550\n",
      "Epoch [36/100], Loss: 0.9551\n",
      "Stopping early at epoch 36 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [36/100], Loss: 0.9551\n",
      "Test Accuracy Logit Lipschitz: 69.11%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 1.5645\n",
      "Epoch [2/100], Loss: 1.1300\n",
      "Epoch [3/100], Loss: 1.0601\n",
      "Epoch [4/100], Loss: 1.0346\n",
      "Epoch [5/100], Loss: 0.9923\n",
      "Epoch [6/100], Loss: 0.9722\n",
      "Epoch [7/100], Loss: 0.9458\n",
      "Epoch [8/100], Loss: 0.9355\n",
      "Epoch [9/100], Loss: 0.9305\n",
      "Epoch [10/100], Loss: 0.9055\n",
      "Epoch [11/100], Loss: 0.9014\n",
      "Epoch [12/100], Loss: 0.8892\n",
      "Epoch [13/100], Loss: 0.8863\n",
      "Epoch [14/100], Loss: 0.8741\n",
      "Epoch [15/100], Loss: 0.8611\n",
      "Epoch [16/100], Loss: 0.8613\n",
      "Epoch [17/100], Loss: 0.8555\n",
      "Epoch [18/100], Loss: 0.8476\n",
      "Epoch [19/100], Loss: 0.8401\n",
      "Epoch [20/100], Loss: 0.8319\n",
      "Epoch [21/100], Loss: 0.8303\n",
      "Epoch [22/100], Loss: 0.8243\n",
      "Epoch [23/100], Loss: 0.8220\n",
      "Epoch [24/100], Loss: 0.8348\n",
      "Epoch [25/100], Loss: 0.8179\n",
      "Epoch [26/100], Loss: 0.8043\n",
      "Epoch [27/100], Loss: 0.8054\n",
      "Epoch [28/100], Loss: 0.8086\n",
      "Epoch [29/100], Loss: 0.7906\n",
      "Epoch [30/100], Loss: 0.7876\n",
      "Epoch [31/100], Loss: 0.7868\n",
      "Epoch [32/100], Loss: 0.7850\n",
      "Epoch [33/100], Loss: 0.7870\n",
      "Epoch [34/100], Loss: 0.7741\n",
      "Epoch [35/100], Loss: 0.7733\n",
      "Epoch [36/100], Loss: 0.7760\n",
      "Epoch [37/100], Loss: 0.7698\n",
      "Epoch [38/100], Loss: 0.7738\n",
      "Epoch [39/100], Loss: 0.7624\n",
      "Epoch [40/100], Loss: 0.7604\n",
      "Epoch [41/100], Loss: 0.7553\n",
      "Epoch [42/100], Loss: 0.7498\n",
      "Epoch [43/100], Loss: 0.7483\n",
      "Epoch [44/100], Loss: 0.7471\n",
      "Epoch [45/100], Loss: 0.7420\n",
      "Epoch [46/100], Loss: 0.7416\n",
      "Epoch [47/100], Loss: 0.7412\n",
      "Epoch [48/100], Loss: 0.7338\n",
      "Epoch [49/100], Loss: 0.7219\n",
      "Epoch [50/100], Loss: 0.7343\n",
      "Epoch [51/100], Loss: 0.7371\n",
      "Epoch [52/100], Loss: 0.7311\n",
      "Epoch [53/100], Loss: 0.7273\n",
      "Epoch [54/100], Loss: 0.7209\n",
      "Epoch [55/100], Loss: 0.7225\n",
      "Epoch [56/100], Loss: 0.7284\n",
      "Epoch [57/100], Loss: 0.7106\n",
      "Epoch [58/100], Loss: 0.7090\n",
      "Epoch [59/100], Loss: 0.7076\n",
      "Epoch [60/100], Loss: 0.7097\n",
      "Epoch [61/100], Loss: 0.7040\n",
      "Epoch [62/100], Loss: 0.7023\n",
      "Epoch [63/100], Loss: 0.6987\n",
      "Epoch [64/100], Loss: 0.7013\n",
      "Epoch [65/100], Loss: 0.6969\n",
      "Epoch [66/100], Loss: 0.6978\n",
      "Epoch [67/100], Loss: 0.6986\n",
      "Epoch [68/100], Loss: 0.6991\n",
      "Stopping early at epoch 68 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [68/100], Loss: 0.6991\n",
      "Test Accuracy Logit Lipschitz: 65.23%\n",
      "Epoch [1/100], Loss: 1.5438\n",
      "Epoch [2/100], Loss: 1.1265\n",
      "Epoch [3/100], Loss: 1.0471\n",
      "Epoch [4/100], Loss: 1.0090\n",
      "Epoch [5/100], Loss: 0.9828\n",
      "Epoch [6/100], Loss: 0.9614\n",
      "Epoch [7/100], Loss: 0.9397\n",
      "Epoch [8/100], Loss: 0.9333\n",
      "Epoch [9/100], Loss: 0.9171\n",
      "Epoch [10/100], Loss: 0.9051\n",
      "Epoch [11/100], Loss: 0.8944\n",
      "Epoch [12/100], Loss: 0.8860\n",
      "Epoch [13/100], Loss: 0.8825\n",
      "Epoch [14/100], Loss: 0.8703\n",
      "Epoch [15/100], Loss: 0.8619\n",
      "Epoch [16/100], Loss: 0.8589\n",
      "Epoch [17/100], Loss: 0.8418\n",
      "Epoch [18/100], Loss: 0.8375\n",
      "Epoch [19/100], Loss: 0.8300\n",
      "Epoch [20/100], Loss: 0.8228\n",
      "Epoch [21/100], Loss: 0.8274\n",
      "Epoch [22/100], Loss: 0.8280\n",
      "Epoch [23/100], Loss: 0.8185\n",
      "Epoch [24/100], Loss: 0.8127\n",
      "Epoch [25/100], Loss: 0.8034\n",
      "Epoch [26/100], Loss: 0.7999\n",
      "Epoch [27/100], Loss: 0.7860\n",
      "Epoch [28/100], Loss: 0.7938\n",
      "Epoch [29/100], Loss: 0.7881\n",
      "Epoch [30/100], Loss: 0.7900\n",
      "Epoch [31/100], Loss: 0.7821\n",
      "Epoch [32/100], Loss: 0.7701\n",
      "Epoch [33/100], Loss: 0.7696\n",
      "Epoch [34/100], Loss: 0.7584\n",
      "Epoch [35/100], Loss: 0.7603\n",
      "Epoch [36/100], Loss: 0.7571\n",
      "Epoch [37/100], Loss: 0.7528\n",
      "Epoch [38/100], Loss: 0.7501\n",
      "Epoch [39/100], Loss: 0.7540\n",
      "Epoch [40/100], Loss: 0.7476\n",
      "Epoch [41/100], Loss: 0.7427\n",
      "Epoch [42/100], Loss: 0.7427\n",
      "Epoch [43/100], Loss: 0.7502\n",
      "Epoch [44/100], Loss: 0.7416\n",
      "Epoch [45/100], Loss: 0.7410\n",
      "Epoch [46/100], Loss: 0.7371\n",
      "Epoch [47/100], Loss: 0.7237\n",
      "Epoch [48/100], Loss: 0.7294\n",
      "Epoch [49/100], Loss: 0.7269\n",
      "Epoch [50/100], Loss: 0.7215\n",
      "Epoch [51/100], Loss: 0.7199\n",
      "Epoch [52/100], Loss: 0.7188\n",
      "Epoch [53/100], Loss: 0.7158\n",
      "Epoch [54/100], Loss: 0.7060\n",
      "Epoch [55/100], Loss: 0.7199\n",
      "Epoch [56/100], Loss: 0.7080\n",
      "Epoch [57/100], Loss: 0.7097\n",
      "Epoch [58/100], Loss: 0.7071\n",
      "Epoch [59/100], Loss: 0.7075\n",
      "Epoch [60/100], Loss: 0.6940\n",
      "Epoch [61/100], Loss: 0.7005\n",
      "Epoch [62/100], Loss: 0.6879\n",
      "Epoch [63/100], Loss: 0.6875\n",
      "Epoch [64/100], Loss: 0.6900\n",
      "Epoch [65/100], Loss: 0.6844\n",
      "Epoch [66/100], Loss: 0.6871\n",
      "Epoch [67/100], Loss: 0.6888\n",
      "Epoch [68/100], Loss: 0.6926\n",
      "Stopping early at epoch 68 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [68/100], Loss: 0.6926\n",
      "Test Accuracy Logit Lipschitz: 66.10%\n",
      "Epoch [1/100], Loss: 1.5419\n",
      "Epoch [2/100], Loss: 1.1140\n",
      "Epoch [3/100], Loss: 1.0420\n",
      "Epoch [4/100], Loss: 1.0152\n",
      "Epoch [5/100], Loss: 0.9666\n",
      "Epoch [6/100], Loss: 0.9630\n",
      "Epoch [7/100], Loss: 0.9426\n",
      "Epoch [8/100], Loss: 0.9273\n",
      "Epoch [9/100], Loss: 0.9118\n",
      "Epoch [10/100], Loss: 0.9081\n",
      "Epoch [11/100], Loss: 0.8957\n",
      "Epoch [12/100], Loss: 0.8750\n",
      "Epoch [13/100], Loss: 0.8692\n",
      "Epoch [14/100], Loss: 0.8657\n",
      "Epoch [15/100], Loss: 0.8501\n",
      "Epoch [16/100], Loss: 0.8598\n",
      "Epoch [17/100], Loss: 0.8502\n",
      "Epoch [18/100], Loss: 0.8292\n",
      "Epoch [19/100], Loss: 0.8245\n",
      "Epoch [20/100], Loss: 0.8191\n",
      "Epoch [21/100], Loss: 0.8142\n",
      "Epoch [22/100], Loss: 0.8090\n",
      "Epoch [23/100], Loss: 0.8044\n",
      "Epoch [24/100], Loss: 0.8016\n",
      "Epoch [25/100], Loss: 0.7970\n",
      "Epoch [26/100], Loss: 0.7918\n",
      "Epoch [27/100], Loss: 0.7850\n",
      "Epoch [28/100], Loss: 0.7852\n",
      "Epoch [29/100], Loss: 0.7791\n",
      "Epoch [30/100], Loss: 0.7732\n",
      "Epoch [31/100], Loss: 0.7683\n",
      "Epoch [32/100], Loss: 0.7689\n",
      "Epoch [33/100], Loss: 0.7806\n",
      "Epoch [34/100], Loss: 0.7651\n",
      "Epoch [35/100], Loss: 0.7580\n",
      "Epoch [36/100], Loss: 0.7500\n",
      "Epoch [37/100], Loss: 0.7476\n",
      "Epoch [38/100], Loss: 0.7351\n",
      "Epoch [39/100], Loss: 0.7362\n",
      "Epoch [40/100], Loss: 0.7346\n",
      "Epoch [41/100], Loss: 0.7357\n",
      "Epoch [42/100], Loss: 0.7483\n",
      "Epoch [43/100], Loss: 0.7274\n",
      "Epoch [44/100], Loss: 0.7272\n",
      "Epoch [45/100], Loss: 0.7225\n",
      "Epoch [46/100], Loss: 0.7318\n",
      "Epoch [47/100], Loss: 0.7123\n",
      "Epoch [48/100], Loss: 0.7117\n",
      "Epoch [49/100], Loss: 0.7089\n",
      "Epoch [50/100], Loss: 0.7090\n",
      "Epoch [51/100], Loss: 0.7070\n",
      "Epoch [52/100], Loss: 0.7054\n",
      "Epoch [53/100], Loss: 0.7032\n",
      "Epoch [54/100], Loss: 0.7077\n",
      "Epoch [55/100], Loss: 0.6974\n",
      "Epoch [56/100], Loss: 0.6993\n",
      "Epoch [57/100], Loss: 0.6990\n",
      "Epoch [58/100], Loss: 0.6901\n",
      "Epoch [59/100], Loss: 0.6941\n",
      "Epoch [60/100], Loss: 0.6884\n",
      "Epoch [61/100], Loss: 0.6859\n",
      "Epoch [62/100], Loss: 0.6803\n",
      "Epoch [63/100], Loss: 0.6835\n",
      "Epoch [64/100], Loss: 0.6875\n",
      "Epoch [65/100], Loss: 0.6821\n",
      "Epoch [66/100], Loss: 0.6759\n",
      "Epoch [67/100], Loss: 0.6815\n",
      "Epoch [68/100], Loss: 0.6716\n",
      "Epoch [69/100], Loss: 0.6654\n",
      "Epoch [70/100], Loss: 0.6656\n",
      "Epoch [71/100], Loss: 0.6693\n",
      "Epoch [72/100], Loss: 0.6760\n",
      "Stopping early at epoch 72 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [72/100], Loss: 0.6760\n",
      "Test Accuracy Logit Lipschitz: 65.41%\n",
      "Epoch [1/100], Loss: 1.5417\n",
      "Epoch [2/100], Loss: 1.1197\n",
      "Epoch [3/100], Loss: 1.0430\n",
      "Epoch [4/100], Loss: 1.0173\n",
      "Epoch [5/100], Loss: 0.9818\n",
      "Epoch [6/100], Loss: 0.9619\n",
      "Epoch [7/100], Loss: 0.9524\n",
      "Epoch [8/100], Loss: 0.9346\n",
      "Epoch [9/100], Loss: 0.9269\n",
      "Epoch [10/100], Loss: 0.9088\n",
      "Epoch [11/100], Loss: 0.8924\n",
      "Epoch [12/100], Loss: 0.8862\n",
      "Epoch [13/100], Loss: 0.8827\n",
      "Epoch [14/100], Loss: 0.8771\n",
      "Epoch [15/100], Loss: 0.8661\n",
      "Epoch [16/100], Loss: 0.8615\n",
      "Epoch [17/100], Loss: 0.8602\n",
      "Epoch [18/100], Loss: 0.8519\n",
      "Epoch [19/100], Loss: 0.8453\n",
      "Epoch [20/100], Loss: 0.8360\n",
      "Epoch [21/100], Loss: 0.8306\n",
      "Epoch [22/100], Loss: 0.8204\n",
      "Epoch [23/100], Loss: 0.8171\n",
      "Epoch [24/100], Loss: 0.8215\n",
      "Epoch [25/100], Loss: 0.8134\n",
      "Epoch [26/100], Loss: 0.8043\n",
      "Epoch [27/100], Loss: 0.7967\n",
      "Epoch [28/100], Loss: 0.8000\n",
      "Epoch [29/100], Loss: 0.7990\n",
      "Epoch [30/100], Loss: 0.7804\n",
      "Epoch [31/100], Loss: 0.7772\n",
      "Epoch [32/100], Loss: 0.7836\n",
      "Epoch [33/100], Loss: 0.7715\n",
      "Epoch [34/100], Loss: 0.7645\n",
      "Epoch [35/100], Loss: 0.7719\n",
      "Epoch [36/100], Loss: 0.7686\n",
      "Epoch [37/100], Loss: 0.7617\n",
      "Epoch [38/100], Loss: 0.7553\n",
      "Epoch [39/100], Loss: 0.7581\n",
      "Epoch [40/100], Loss: 0.7673\n",
      "Epoch [41/100], Loss: 0.7512\n",
      "Epoch [42/100], Loss: 0.7545\n",
      "Epoch [43/100], Loss: 0.7472\n",
      "Epoch [44/100], Loss: 0.7510\n",
      "Epoch [45/100], Loss: 0.7328\n",
      "Epoch [46/100], Loss: 0.7355\n",
      "Epoch [47/100], Loss: 0.7398\n",
      "Epoch [48/100], Loss: 0.7260\n",
      "Epoch [49/100], Loss: 0.7309\n",
      "Epoch [50/100], Loss: 0.7298\n",
      "Epoch [51/100], Loss: 0.7306\n",
      "Epoch [52/100], Loss: 0.7343\n",
      "Epoch [53/100], Loss: 0.7286\n",
      "Epoch [54/100], Loss: 0.7183\n",
      "Epoch [55/100], Loss: 0.7199\n",
      "Epoch [56/100], Loss: 0.7281\n",
      "Epoch [57/100], Loss: 0.7196\n",
      "Epoch [58/100], Loss: 0.7134\n",
      "Epoch [59/100], Loss: 0.7050\n",
      "Epoch [60/100], Loss: 0.7149\n",
      "Epoch [61/100], Loss: 0.7039\n",
      "Epoch [62/100], Loss: 0.7032\n",
      "Epoch [63/100], Loss: 0.6997\n",
      "Epoch [64/100], Loss: 0.6930\n",
      "Epoch [65/100], Loss: 0.7010\n",
      "Epoch [66/100], Loss: 0.6983\n",
      "Epoch [67/100], Loss: 0.6918\n",
      "Epoch [68/100], Loss: 0.6887\n",
      "Epoch [69/100], Loss: 0.6849\n",
      "Epoch [70/100], Loss: 0.6857\n",
      "Epoch [71/100], Loss: 0.6948\n",
      "Epoch [72/100], Loss: 0.6825\n",
      "Epoch [73/100], Loss: 0.6844\n",
      "Epoch [74/100], Loss: 0.6814\n",
      "Epoch [75/100], Loss: 0.6766\n",
      "Epoch [76/100], Loss: 0.6683\n",
      "Epoch [77/100], Loss: 0.6681\n",
      "Epoch [78/100], Loss: 0.6684\n",
      "Epoch [79/100], Loss: 0.6662\n",
      "Epoch [80/100], Loss: 0.6744\n",
      "Epoch [81/100], Loss: 0.6691\n",
      "Epoch [82/100], Loss: 0.6676\n",
      "Epoch [83/100], Loss: 0.6767\n",
      "Epoch [84/100], Loss: 0.6595\n",
      "Epoch [85/100], Loss: 0.6584\n",
      "Epoch [86/100], Loss: 0.6685\n",
      "Epoch [87/100], Loss: 0.6586\n",
      "Epoch [88/100], Loss: 0.6538\n",
      "Epoch [89/100], Loss: 0.6518\n",
      "Epoch [90/100], Loss: 0.6489\n",
      "Epoch [91/100], Loss: 0.6446\n",
      "Epoch [92/100], Loss: 0.6455\n",
      "Epoch [93/100], Loss: 0.6501\n",
      "Epoch [94/100], Loss: 0.6541\n",
      "Stopping early at epoch 94 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [94/100], Loss: 0.6541\n",
      "Test Accuracy Logit Lipschitz: 64.69%\n",
      "Epoch [1/100], Loss: 1.5412\n",
      "Epoch [2/100], Loss: 1.1155\n",
      "Epoch [3/100], Loss: 1.0499\n",
      "Epoch [4/100], Loss: 1.0163\n",
      "Epoch [5/100], Loss: 0.9827\n",
      "Epoch [6/100], Loss: 0.9639\n",
      "Epoch [7/100], Loss: 0.9460\n",
      "Epoch [8/100], Loss: 0.9258\n",
      "Epoch [9/100], Loss: 0.9142\n",
      "Epoch [10/100], Loss: 0.8997\n",
      "Epoch [11/100], Loss: 0.8968\n",
      "Epoch [12/100], Loss: 0.8849\n",
      "Epoch [13/100], Loss: 0.8724\n",
      "Epoch [14/100], Loss: 0.8672\n",
      "Epoch [15/100], Loss: 0.8663\n",
      "Epoch [16/100], Loss: 0.8520\n",
      "Epoch [17/100], Loss: 0.8525\n",
      "Epoch [18/100], Loss: 0.8468\n",
      "Epoch [19/100], Loss: 0.8377\n",
      "Epoch [20/100], Loss: 0.8277\n",
      "Epoch [21/100], Loss: 0.8261\n",
      "Epoch [22/100], Loss: 0.8180\n",
      "Epoch [23/100], Loss: 0.8125\n",
      "Epoch [24/100], Loss: 0.8050\n",
      "Epoch [25/100], Loss: 0.8001\n",
      "Epoch [26/100], Loss: 0.8062\n",
      "Epoch [27/100], Loss: 0.8074\n",
      "Epoch [28/100], Loss: 0.7955\n",
      "Epoch [29/100], Loss: 0.7907\n",
      "Epoch [30/100], Loss: 0.7876\n",
      "Epoch [31/100], Loss: 0.7839\n",
      "Epoch [32/100], Loss: 0.7724\n",
      "Epoch [33/100], Loss: 0.7719\n",
      "Epoch [34/100], Loss: 0.7711\n",
      "Epoch [35/100], Loss: 0.7649\n",
      "Epoch [36/100], Loss: 0.7582\n",
      "Epoch [37/100], Loss: 0.7618\n",
      "Epoch [38/100], Loss: 0.7647\n",
      "Epoch [39/100], Loss: 0.7597\n",
      "Epoch [40/100], Loss: 0.7636\n",
      "Epoch [41/100], Loss: 0.7585\n",
      "Epoch [42/100], Loss: 0.7513\n",
      "Epoch [43/100], Loss: 0.7562\n",
      "Epoch [44/100], Loss: 0.7484\n",
      "Epoch [45/100], Loss: 0.7387\n",
      "Epoch [46/100], Loss: 0.7396\n",
      "Epoch [47/100], Loss: 0.7447\n",
      "Epoch [48/100], Loss: 0.7392\n",
      "Epoch [49/100], Loss: 0.7314\n",
      "Epoch [50/100], Loss: 0.7333\n",
      "Epoch [51/100], Loss: 0.7332\n",
      "Epoch [52/100], Loss: 0.7213\n",
      "Epoch [53/100], Loss: 0.7301\n",
      "Epoch [54/100], Loss: 0.7296\n",
      "Epoch [55/100], Loss: 0.7173\n",
      "Epoch [56/100], Loss: 0.7274\n",
      "Epoch [57/100], Loss: 0.7270\n",
      "Epoch [58/100], Loss: 0.7156\n",
      "Epoch [59/100], Loss: 0.7158\n",
      "Epoch [60/100], Loss: 0.7096\n",
      "Epoch [61/100], Loss: 0.7079\n",
      "Epoch [62/100], Loss: 0.7025\n",
      "Epoch [63/100], Loss: 0.7038\n",
      "Epoch [64/100], Loss: 0.7014\n",
      "Epoch [65/100], Loss: 0.6990\n",
      "Epoch [66/100], Loss: 0.7035\n",
      "Epoch [67/100], Loss: 0.6984\n",
      "Epoch [68/100], Loss: 0.6927\n",
      "Epoch [69/100], Loss: 0.6868\n",
      "Epoch [70/100], Loss: 0.6835\n",
      "Epoch [71/100], Loss: 0.6806\n",
      "Epoch [72/100], Loss: 0.6837\n",
      "Epoch [73/100], Loss: 0.6863\n",
      "Epoch [74/100], Loss: 0.6799\n",
      "Epoch [75/100], Loss: 0.6882\n",
      "Epoch [76/100], Loss: 0.6789\n",
      "Epoch [77/100], Loss: 0.6780\n",
      "Epoch [78/100], Loss: 0.6759\n",
      "Epoch [79/100], Loss: 0.6714\n",
      "Epoch [80/100], Loss: 0.6723\n",
      "Epoch [81/100], Loss: 0.6736\n",
      "Epoch [82/100], Loss: 0.6656\n",
      "Epoch [83/100], Loss: 0.6715\n",
      "Epoch [84/100], Loss: 0.6768\n",
      "Epoch [85/100], Loss: 0.6657\n",
      "Epoch [86/100], Loss: 0.6707\n",
      "Epoch [87/100], Loss: 0.6649\n",
      "Epoch [88/100], Loss: 0.6655\n",
      "Epoch [89/100], Loss: 0.6620\n",
      "Epoch [90/100], Loss: 0.6573\n",
      "Epoch [91/100], Loss: 0.6813\n",
      "Epoch [92/100], Loss: 0.6572\n",
      "Epoch [93/100], Loss: 0.6538\n",
      "Epoch [94/100], Loss: 0.6484\n",
      "Epoch [95/100], Loss: 0.6540\n",
      "Epoch [96/100], Loss: 0.6601\n",
      "Epoch [97/100], Loss: 0.6532\n",
      "Epoch [98/100], Loss: 0.6632\n",
      "Epoch [99/100], Loss: 0.6472\n",
      "Epoch [100/100], Loss: 0.6459\n",
      "Subset 10000, Epoch [100/100], Loss: 0.6459\n",
      "Test Accuracy Logit Lipschitz: 64.73%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 1.8415\n",
      "Epoch [2/100], Loss: 1.1801\n",
      "Epoch [3/100], Loss: 1.0644\n",
      "Epoch [4/100], Loss: 0.9821\n",
      "Epoch [5/100], Loss: 0.9553\n",
      "Epoch [6/100], Loss: 0.9376\n",
      "Epoch [7/100], Loss: 0.8758\n",
      "Epoch [8/100], Loss: 0.8493\n",
      "Epoch [9/100], Loss: 0.8515\n",
      "Epoch [10/100], Loss: 0.8405\n",
      "Epoch [11/100], Loss: 0.8222\n",
      "Epoch [12/100], Loss: 0.7962\n",
      "Epoch [13/100], Loss: 0.7755\n",
      "Epoch [14/100], Loss: 0.7804\n",
      "Epoch [15/100], Loss: 0.7628\n",
      "Epoch [16/100], Loss: 0.7442\n",
      "Epoch [17/100], Loss: 0.7350\n",
      "Epoch [18/100], Loss: 0.7402\n",
      "Epoch [19/100], Loss: 0.7132\n",
      "Epoch [20/100], Loss: 0.7125\n",
      "Epoch [21/100], Loss: 0.7021\n",
      "Epoch [22/100], Loss: 0.6735\n",
      "Epoch [23/100], Loss: 0.6583\n",
      "Epoch [24/100], Loss: 0.6722\n",
      "Epoch [25/100], Loss: 0.6762\n",
      "Epoch [26/100], Loss: 0.6555\n",
      "Epoch [27/100], Loss: 0.6480\n",
      "Epoch [28/100], Loss: 0.6535\n",
      "Epoch [29/100], Loss: 0.6506\n",
      "Epoch [30/100], Loss: 0.6377\n",
      "Epoch [31/100], Loss: 0.6665\n",
      "Epoch [32/100], Loss: 0.6383\n",
      "Epoch [33/100], Loss: 0.6231\n",
      "Epoch [34/100], Loss: 0.6191\n",
      "Epoch [35/100], Loss: 0.6053\n",
      "Epoch [36/100], Loss: 0.6512\n",
      "Epoch [37/100], Loss: 0.6206\n",
      "Epoch [38/100], Loss: 0.6320\n",
      "Epoch [39/100], Loss: 0.6176\n",
      "Epoch [40/100], Loss: 0.5789\n",
      "Epoch [41/100], Loss: 0.5747\n",
      "Epoch [42/100], Loss: 0.5770\n",
      "Epoch [43/100], Loss: 0.5902\n",
      "Epoch [44/100], Loss: 0.5803\n",
      "Epoch [45/100], Loss: 0.5714\n",
      "Epoch [46/100], Loss: 0.5586\n",
      "Epoch [47/100], Loss: 0.5553\n",
      "Epoch [48/100], Loss: 0.5963\n",
      "Epoch [49/100], Loss: 0.5524\n",
      "Epoch [50/100], Loss: 0.5465\n",
      "Epoch [51/100], Loss: 0.5434\n",
      "Epoch [52/100], Loss: 0.5500\n",
      "Epoch [53/100], Loss: 0.5354\n",
      "Epoch [54/100], Loss: 0.5279\n",
      "Epoch [55/100], Loss: 0.5223\n",
      "Epoch [56/100], Loss: 0.5231\n",
      "Epoch [57/100], Loss: 0.5337\n",
      "Epoch [58/100], Loss: 0.5291\n",
      "Epoch [59/100], Loss: 0.5400\n",
      "Epoch [60/100], Loss: 0.5056\n",
      "Epoch [61/100], Loss: 0.5061\n",
      "Epoch [62/100], Loss: 0.5236\n",
      "Epoch [63/100], Loss: 0.5309\n",
      "Stopping early at epoch 63 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [63/100], Loss: 0.5309\n",
      "Test Accuracy Logit Lipschitz: 64.04%\n",
      "Epoch [1/100], Loss: 1.8199\n",
      "Epoch [2/100], Loss: 1.1728\n",
      "Epoch [3/100], Loss: 1.0864\n",
      "Epoch [4/100], Loss: 0.9954\n",
      "Epoch [5/100], Loss: 0.9658\n",
      "Epoch [6/100], Loss: 0.9372\n",
      "Epoch [7/100], Loss: 0.9104\n",
      "Epoch [8/100], Loss: 0.9037\n",
      "Epoch [9/100], Loss: 0.8744\n",
      "Epoch [10/100], Loss: 0.8570\n",
      "Epoch [11/100], Loss: 0.8400\n",
      "Epoch [12/100], Loss: 0.8140\n",
      "Epoch [13/100], Loss: 0.7854\n",
      "Epoch [14/100], Loss: 0.7898\n",
      "Epoch [15/100], Loss: 0.7776\n",
      "Epoch [16/100], Loss: 0.7636\n",
      "Epoch [17/100], Loss: 0.7531\n",
      "Epoch [18/100], Loss: 0.7185\n",
      "Epoch [19/100], Loss: 0.7091\n",
      "Epoch [20/100], Loss: 0.7202\n",
      "Epoch [21/100], Loss: 0.7444\n",
      "Epoch [22/100], Loss: 0.7380\n",
      "Epoch [23/100], Loss: 0.6924\n",
      "Epoch [24/100], Loss: 0.6801\n",
      "Epoch [25/100], Loss: 0.6676\n",
      "Epoch [26/100], Loss: 0.6673\n",
      "Epoch [27/100], Loss: 0.6719\n",
      "Epoch [28/100], Loss: 0.6555\n",
      "Epoch [29/100], Loss: 0.6399\n",
      "Epoch [30/100], Loss: 0.6339\n",
      "Epoch [31/100], Loss: 0.6519\n",
      "Epoch [32/100], Loss: 0.6471\n",
      "Epoch [33/100], Loss: 0.6378\n",
      "Epoch [34/100], Loss: 0.6177\n",
      "Epoch [35/100], Loss: 0.6098\n",
      "Epoch [36/100], Loss: 0.6092\n",
      "Epoch [37/100], Loss: 0.6053\n",
      "Epoch [38/100], Loss: 0.6051\n",
      "Epoch [39/100], Loss: 0.6060\n",
      "Epoch [40/100], Loss: 0.5850\n",
      "Epoch [41/100], Loss: 0.5756\n",
      "Epoch [42/100], Loss: 0.5802\n",
      "Epoch [43/100], Loss: 0.5737\n",
      "Epoch [44/100], Loss: 0.5778\n",
      "Epoch [45/100], Loss: 0.5779\n",
      "Epoch [46/100], Loss: 0.5737\n",
      "Epoch [47/100], Loss: 0.5641\n",
      "Epoch [48/100], Loss: 0.5758\n",
      "Epoch [49/100], Loss: 0.5849\n",
      "Epoch [50/100], Loss: 0.5627\n",
      "Epoch [51/100], Loss: 0.5488\n",
      "Epoch [52/100], Loss: 0.5401\n",
      "Epoch [53/100], Loss: 0.5374\n",
      "Epoch [54/100], Loss: 0.5261\n",
      "Epoch [55/100], Loss: 0.5258\n",
      "Epoch [56/100], Loss: 0.5243\n",
      "Epoch [57/100], Loss: 0.5259\n",
      "Epoch [58/100], Loss: 0.5212\n",
      "Epoch [59/100], Loss: 0.5259\n",
      "Epoch [60/100], Loss: 0.5213\n",
      "Epoch [61/100], Loss: 0.5212\n",
      "Epoch [62/100], Loss: 0.5187\n",
      "Epoch [63/100], Loss: 0.5249\n",
      "Epoch [64/100], Loss: 0.5096\n",
      "Epoch [65/100], Loss: 0.5013\n",
      "Epoch [66/100], Loss: 0.5033\n",
      "Epoch [67/100], Loss: 0.5172\n",
      "Epoch [68/100], Loss: 0.5101\n",
      "Epoch [69/100], Loss: 0.4998\n",
      "Epoch [70/100], Loss: 0.5025\n",
      "Epoch [71/100], Loss: 0.5264\n",
      "Epoch [72/100], Loss: 0.5102\n",
      "Epoch [73/100], Loss: 0.4905\n",
      "Epoch [74/100], Loss: 0.4769\n",
      "Epoch [75/100], Loss: 0.4818\n",
      "Epoch [76/100], Loss: 0.4716\n",
      "Epoch [77/100], Loss: 0.4783\n",
      "Epoch [78/100], Loss: 0.4842\n",
      "Epoch [79/100], Loss: 0.4922\n",
      "Stopping early at epoch 79 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [79/100], Loss: 0.4922\n",
      "Test Accuracy Logit Lipschitz: 62.62%\n",
      "Epoch [1/100], Loss: 1.8088\n",
      "Epoch [2/100], Loss: 1.1449\n",
      "Epoch [3/100], Loss: 1.0431\n",
      "Epoch [4/100], Loss: 0.9926\n",
      "Epoch [5/100], Loss: 0.9831\n",
      "Epoch [6/100], Loss: 0.9432\n",
      "Epoch [7/100], Loss: 0.9280\n",
      "Epoch [8/100], Loss: 0.8831\n",
      "Epoch [9/100], Loss: 0.8600\n",
      "Epoch [10/100], Loss: 0.8370\n",
      "Epoch [11/100], Loss: 0.8010\n",
      "Epoch [12/100], Loss: 0.7847\n",
      "Epoch [13/100], Loss: 0.7770\n",
      "Epoch [14/100], Loss: 0.7809\n",
      "Epoch [15/100], Loss: 0.7564\n",
      "Epoch [16/100], Loss: 0.7373\n",
      "Epoch [17/100], Loss: 0.7478\n",
      "Epoch [18/100], Loss: 0.7358\n",
      "Epoch [19/100], Loss: 0.7193\n",
      "Epoch [20/100], Loss: 0.7135\n",
      "Epoch [21/100], Loss: 0.7086\n",
      "Epoch [22/100], Loss: 0.6790\n",
      "Epoch [23/100], Loss: 0.6864\n",
      "Epoch [24/100], Loss: 0.7092\n",
      "Epoch [25/100], Loss: 0.7022\n",
      "Epoch [26/100], Loss: 0.6774\n",
      "Epoch [27/100], Loss: 0.6530\n",
      "Epoch [28/100], Loss: 0.6541\n",
      "Epoch [29/100], Loss: 0.6447\n",
      "Epoch [30/100], Loss: 0.6512\n",
      "Epoch [31/100], Loss: 0.6460\n",
      "Epoch [32/100], Loss: 0.6431\n",
      "Epoch [33/100], Loss: 0.6411\n",
      "Epoch [34/100], Loss: 0.6373\n",
      "Epoch [35/100], Loss: 0.6322\n",
      "Epoch [36/100], Loss: 0.6183\n",
      "Epoch [37/100], Loss: 0.5918\n",
      "Epoch [38/100], Loss: 0.5845\n",
      "Epoch [39/100], Loss: 0.5846\n",
      "Epoch [40/100], Loss: 0.5997\n",
      "Epoch [41/100], Loss: 0.5814\n",
      "Epoch [42/100], Loss: 0.5899\n",
      "Epoch [43/100], Loss: 0.5828\n",
      "Epoch [44/100], Loss: 0.5819\n",
      "Epoch [45/100], Loss: 0.5652\n",
      "Epoch [46/100], Loss: 0.5568\n",
      "Epoch [47/100], Loss: 0.5541\n",
      "Epoch [48/100], Loss: 0.5929\n",
      "Epoch [49/100], Loss: 0.5785\n",
      "Epoch [50/100], Loss: 0.5549\n",
      "Epoch [51/100], Loss: 0.5397\n",
      "Epoch [52/100], Loss: 0.5369\n",
      "Epoch [53/100], Loss: 0.5645\n",
      "Epoch [54/100], Loss: 0.5892\n",
      "Epoch [55/100], Loss: 0.5806\n",
      "Epoch [56/100], Loss: 0.5504\n",
      "Epoch [57/100], Loss: 0.5381\n",
      "Epoch [58/100], Loss: 0.5293\n",
      "Epoch [59/100], Loss: 0.5304\n",
      "Epoch [60/100], Loss: 0.5135\n",
      "Epoch [61/100], Loss: 0.5054\n",
      "Epoch [62/100], Loss: 0.5123\n",
      "Epoch [63/100], Loss: 0.5102\n",
      "Epoch [64/100], Loss: 0.5275\n",
      "Epoch [65/100], Loss: 0.4992\n",
      "Epoch [66/100], Loss: 0.4946\n",
      "Epoch [67/100], Loss: 0.4891\n",
      "Epoch [68/100], Loss: 0.4945\n",
      "Epoch [69/100], Loss: 0.4800\n",
      "Epoch [70/100], Loss: 0.4892\n",
      "Epoch [71/100], Loss: 0.5125\n",
      "Epoch [72/100], Loss: 0.5075\n",
      "Epoch [73/100], Loss: 0.4811\n",
      "Epoch [74/100], Loss: 0.4760\n",
      "Epoch [75/100], Loss: 0.4798\n",
      "Epoch [76/100], Loss: 0.4768\n",
      "Epoch [77/100], Loss: 0.4719\n",
      "Epoch [78/100], Loss: 0.4856\n",
      "Epoch [79/100], Loss: 0.4857\n",
      "Epoch [80/100], Loss: 0.4850\n",
      "Stopping early at epoch 80 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [80/100], Loss: 0.4850\n",
      "Test Accuracy Logit Lipschitz: 62.43%\n",
      "Epoch [1/100], Loss: 1.8800\n",
      "Epoch [2/100], Loss: 1.2390\n",
      "Epoch [3/100], Loss: 1.1097\n",
      "Epoch [4/100], Loss: 1.0176\n",
      "Epoch [5/100], Loss: 0.9733\n",
      "Epoch [6/100], Loss: 0.9395\n",
      "Epoch [7/100], Loss: 0.9029\n",
      "Epoch [8/100], Loss: 0.8968\n",
      "Epoch [9/100], Loss: 0.8397\n",
      "Epoch [10/100], Loss: 0.8241\n",
      "Epoch [11/100], Loss: 0.8237\n",
      "Epoch [12/100], Loss: 0.8090\n",
      "Epoch [13/100], Loss: 0.7781\n",
      "Epoch [14/100], Loss: 0.7626\n",
      "Epoch [15/100], Loss: 0.7445\n",
      "Epoch [16/100], Loss: 0.7287\n",
      "Epoch [17/100], Loss: 0.7241\n",
      "Epoch [18/100], Loss: 0.7297\n",
      "Epoch [19/100], Loss: 0.7295\n",
      "Epoch [20/100], Loss: 0.7314\n",
      "Stopping early at epoch 20 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [20/100], Loss: 0.7314\n",
      "Test Accuracy Logit Lipschitz: 65.49%\n",
      "Epoch [1/100], Loss: 1.8222\n",
      "Epoch [2/100], Loss: 1.1711\n",
      "Epoch [3/100], Loss: 1.0794\n",
      "Epoch [4/100], Loss: 0.9886\n",
      "Epoch [5/100], Loss: 0.9268\n",
      "Epoch [6/100], Loss: 0.9141\n",
      "Epoch [7/100], Loss: 0.8831\n",
      "Epoch [8/100], Loss: 0.8531\n",
      "Epoch [9/100], Loss: 0.8491\n",
      "Epoch [10/100], Loss: 0.8554\n",
      "Epoch [11/100], Loss: 0.8446\n",
      "Epoch [12/100], Loss: 0.7834\n",
      "Epoch [13/100], Loss: 0.7869\n",
      "Epoch [14/100], Loss: 0.7848\n",
      "Epoch [15/100], Loss: 0.7625\n",
      "Epoch [16/100], Loss: 0.7703\n",
      "Epoch [17/100], Loss: 0.7506\n",
      "Epoch [18/100], Loss: 0.7261\n",
      "Epoch [19/100], Loss: 0.7096\n",
      "Epoch [20/100], Loss: 0.7003\n",
      "Epoch [21/100], Loss: 0.6984\n",
      "Epoch [22/100], Loss: 0.6885\n",
      "Epoch [23/100], Loss: 0.6742\n",
      "Epoch [24/100], Loss: 0.6879\n",
      "Epoch [25/100], Loss: 0.7009\n",
      "Epoch [26/100], Loss: 0.6723\n",
      "Epoch [27/100], Loss: 0.6642\n",
      "Epoch [28/100], Loss: 0.6454\n",
      "Epoch [29/100], Loss: 0.6386\n",
      "Epoch [30/100], Loss: 0.6403\n",
      "Epoch [31/100], Loss: 0.6302\n",
      "Epoch [32/100], Loss: 0.6334\n",
      "Epoch [33/100], Loss: 0.6294\n",
      "Epoch [34/100], Loss: 0.6110\n",
      "Epoch [35/100], Loss: 0.6298\n",
      "Epoch [36/100], Loss: 0.6237\n",
      "Epoch [37/100], Loss: 0.6068\n",
      "Epoch [38/100], Loss: 0.6000\n",
      "Epoch [39/100], Loss: 0.6126\n",
      "Epoch [40/100], Loss: 0.5813\n",
      "Epoch [41/100], Loss: 0.5723\n",
      "Epoch [42/100], Loss: 0.5793\n",
      "Epoch [43/100], Loss: 0.5813\n",
      "Epoch [44/100], Loss: 0.5691\n",
      "Epoch [45/100], Loss: 0.5671\n",
      "Epoch [46/100], Loss: 0.5908\n",
      "Epoch [47/100], Loss: 0.5963\n",
      "Epoch [48/100], Loss: 0.5720\n",
      "Epoch [49/100], Loss: 0.5605\n",
      "Epoch [50/100], Loss: 0.5796\n",
      "Epoch [51/100], Loss: 0.5681\n",
      "Epoch [52/100], Loss: 0.5785\n",
      "Epoch [53/100], Loss: 0.5499\n",
      "Epoch [54/100], Loss: 0.5466\n",
      "Epoch [55/100], Loss: 0.5366\n",
      "Epoch [56/100], Loss: 0.5234\n",
      "Epoch [57/100], Loss: 0.5176\n",
      "Epoch [58/100], Loss: 0.5177\n",
      "Epoch [59/100], Loss: 0.5350\n",
      "Epoch [60/100], Loss: 0.5365\n",
      "Stopping early at epoch 60 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [60/100], Loss: 0.5365\n",
      "Test Accuracy Logit Lipschitz: 63.38%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 2.9156\n",
      "Epoch [2/100], Loss: 1.7431\n",
      "Epoch [3/100], Loss: 1.2172\n",
      "Epoch [4/100], Loss: 1.0211\n",
      "Epoch [5/100], Loss: 0.9000\n",
      "Epoch [6/100], Loss: 0.8011\n",
      "Epoch [7/100], Loss: 0.7431\n",
      "Epoch [8/100], Loss: 0.6775\n",
      "Epoch [9/100], Loss: 0.6360\n",
      "Epoch [10/100], Loss: 0.5891\n",
      "Epoch [11/100], Loss: 0.5540\n",
      "Epoch [12/100], Loss: 0.5289\n",
      "Epoch [13/100], Loss: 0.4981\n",
      "Epoch [14/100], Loss: 0.4720\n",
      "Epoch [15/100], Loss: 0.4485\n",
      "Epoch [16/100], Loss: 0.4320\n",
      "Epoch [17/100], Loss: 0.4111\n",
      "Epoch [18/100], Loss: 0.3980\n",
      "Epoch [19/100], Loss: 0.3809\n",
      "Epoch [20/100], Loss: 0.3663\n",
      "Epoch [21/100], Loss: 0.3491\n",
      "Epoch [22/100], Loss: 0.3376\n",
      "Epoch [23/100], Loss: 0.3268\n",
      "Epoch [24/100], Loss: 0.3174\n",
      "Epoch [25/100], Loss: 0.3081\n",
      "Epoch [26/100], Loss: 0.2963\n",
      "Epoch [27/100], Loss: 0.2866\n",
      "Epoch [28/100], Loss: 0.2794\n",
      "Epoch [29/100], Loss: 0.2683\n",
      "Epoch [30/100], Loss: 0.2617\n",
      "Epoch [31/100], Loss: 0.2560\n",
      "Epoch [32/100], Loss: 0.2481\n",
      "Epoch [33/100], Loss: 0.2433\n",
      "Epoch [34/100], Loss: 0.2352\n",
      "Epoch [35/100], Loss: 0.2283\n",
      "Epoch [36/100], Loss: 0.2228\n",
      "Epoch [37/100], Loss: 0.2191\n",
      "Epoch [38/100], Loss: 0.2145\n",
      "Epoch [39/100], Loss: 0.2089\n",
      "Epoch [40/100], Loss: 0.2030\n",
      "Epoch [41/100], Loss: 0.2002\n",
      "Epoch [42/100], Loss: 0.1949\n",
      "Epoch [43/100], Loss: 0.1894\n",
      "Epoch [44/100], Loss: 0.1859\n",
      "Epoch [45/100], Loss: 0.1817\n",
      "Epoch [46/100], Loss: 0.1799\n",
      "Epoch [47/100], Loss: 0.1760\n",
      "Epoch [48/100], Loss: 0.1716\n",
      "Epoch [49/100], Loss: 0.1677\n",
      "Epoch [50/100], Loss: 0.1654\n",
      "Epoch [51/100], Loss: 0.1636\n",
      "Epoch [52/100], Loss: 0.1578\n",
      "Epoch [53/100], Loss: 0.1562\n",
      "Epoch [54/100], Loss: 0.1536\n",
      "Epoch [55/100], Loss: 0.1514\n",
      "Epoch [56/100], Loss: 0.1489\n",
      "Epoch [57/100], Loss: 0.1460\n",
      "Epoch [58/100], Loss: 0.1432\n",
      "Epoch [59/100], Loss: 0.1414\n",
      "Epoch [60/100], Loss: 0.1392\n",
      "Epoch [61/100], Loss: 0.1366\n",
      "Epoch [62/100], Loss: 0.1349\n",
      "Epoch [63/100], Loss: 0.1350\n",
      "Epoch [64/100], Loss: 0.1305\n",
      "Epoch [65/100], Loss: 0.1299\n",
      "Epoch [66/100], Loss: 0.1267\n",
      "Epoch [67/100], Loss: 0.1265\n",
      "Epoch [68/100], Loss: 0.1225\n",
      "Epoch [69/100], Loss: 0.1218\n",
      "Epoch [70/100], Loss: 0.1197\n",
      "Epoch [71/100], Loss: 0.1184\n",
      "Epoch [72/100], Loss: 0.1161\n",
      "Epoch [73/100], Loss: 0.1150\n",
      "Epoch [74/100], Loss: 0.1130\n",
      "Epoch [75/100], Loss: 0.1124\n",
      "Epoch [76/100], Loss: 0.1107\n",
      "Epoch [77/100], Loss: 0.1099\n",
      "Epoch [78/100], Loss: 0.1082\n",
      "Epoch [79/100], Loss: 0.1072\n",
      "Epoch [80/100], Loss: 0.1059\n",
      "Epoch [81/100], Loss: 0.1043\n",
      "Epoch [82/100], Loss: 0.1065\n",
      "Epoch [83/100], Loss: 0.1030\n",
      "Epoch [84/100], Loss: 0.1009\n",
      "Epoch [85/100], Loss: 0.1002\n",
      "Epoch [86/100], Loss: 0.0985\n",
      "Epoch [87/100], Loss: 0.0976\n",
      "Epoch [88/100], Loss: 0.0959\n",
      "Epoch [89/100], Loss: 0.0956\n",
      "Epoch [90/100], Loss: 0.0945\n",
      "Epoch [91/100], Loss: 0.0927\n",
      "Epoch [92/100], Loss: 0.0924\n",
      "Epoch [93/100], Loss: 0.0915\n",
      "Epoch [94/100], Loss: 0.0902\n",
      "Epoch [95/100], Loss: 0.0895\n",
      "Epoch [96/100], Loss: 0.0882\n",
      "Epoch [97/100], Loss: 0.0874\n",
      "Epoch [98/100], Loss: 0.0868\n",
      "Epoch [99/100], Loss: 0.0859\n",
      "Stopping early at epoch 99 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [99/100], Loss: 0.0859\n",
      "Test Accuracy Logit Lipschitz: 57.24%\n",
      "Epoch [1/100], Loss: 2.9491\n",
      "Epoch [2/100], Loss: 1.8254\n",
      "Epoch [3/100], Loss: 1.2939\n",
      "Epoch [4/100], Loss: 1.0960\n",
      "Epoch [5/100], Loss: 0.9637\n",
      "Epoch [6/100], Loss: 0.8812\n",
      "Epoch [7/100], Loss: 0.8081\n",
      "Epoch [8/100], Loss: 0.7514\n",
      "Epoch [9/100], Loss: 0.7046\n",
      "Epoch [10/100], Loss: 0.6606\n",
      "Epoch [11/100], Loss: 0.6278\n",
      "Epoch [12/100], Loss: 0.6001\n",
      "Epoch [13/100], Loss: 0.5637\n",
      "Epoch [14/100], Loss: 0.5450\n",
      "Epoch [15/100], Loss: 0.5190\n",
      "Epoch [16/100], Loss: 0.5009\n",
      "Epoch [17/100], Loss: 0.4829\n",
      "Epoch [18/100], Loss: 0.4610\n",
      "Epoch [19/100], Loss: 0.4467\n",
      "Epoch [20/100], Loss: 0.4318\n",
      "Epoch [21/100], Loss: 0.4144\n",
      "Epoch [22/100], Loss: 0.3979\n",
      "Epoch [23/100], Loss: 0.3897\n",
      "Epoch [24/100], Loss: 0.3810\n",
      "Epoch [25/100], Loss: 0.3646\n",
      "Epoch [26/100], Loss: 0.3553\n",
      "Epoch [27/100], Loss: 0.3456\n",
      "Epoch [28/100], Loss: 0.3313\n",
      "Epoch [29/100], Loss: 0.3232\n",
      "Epoch [30/100], Loss: 0.3115\n",
      "Epoch [31/100], Loss: 0.3075\n",
      "Epoch [32/100], Loss: 0.3021\n",
      "Epoch [33/100], Loss: 0.2904\n",
      "Epoch [34/100], Loss: 0.2845\n",
      "Epoch [35/100], Loss: 0.2755\n",
      "Epoch [36/100], Loss: 0.2697\n",
      "Epoch [37/100], Loss: 0.2623\n",
      "Epoch [38/100], Loss: 0.2544\n",
      "Epoch [39/100], Loss: 0.2509\n",
      "Epoch [40/100], Loss: 0.2438\n",
      "Epoch [41/100], Loss: 0.2402\n",
      "Epoch [42/100], Loss: 0.2347\n",
      "Epoch [43/100], Loss: 0.2290\n",
      "Epoch [44/100], Loss: 0.2233\n",
      "Epoch [45/100], Loss: 0.2186\n",
      "Epoch [46/100], Loss: 0.2163\n",
      "Epoch [47/100], Loss: 0.2106\n",
      "Epoch [48/100], Loss: 0.2071\n",
      "Epoch [49/100], Loss: 0.2039\n",
      "Epoch [50/100], Loss: 0.1986\n",
      "Epoch [51/100], Loss: 0.1947\n",
      "Epoch [52/100], Loss: 0.1912\n",
      "Epoch [53/100], Loss: 0.1890\n",
      "Epoch [54/100], Loss: 0.1845\n",
      "Epoch [55/100], Loss: 0.1821\n",
      "Epoch [56/100], Loss: 0.1795\n",
      "Epoch [57/100], Loss: 0.1755\n",
      "Epoch [58/100], Loss: 0.1733\n",
      "Epoch [59/100], Loss: 0.1688\n",
      "Epoch [60/100], Loss: 0.1667\n",
      "Epoch [61/100], Loss: 0.1649\n",
      "Epoch [62/100], Loss: 0.1630\n",
      "Epoch [63/100], Loss: 0.1605\n",
      "Epoch [64/100], Loss: 0.1577\n",
      "Epoch [65/100], Loss: 0.1545\n",
      "Epoch [66/100], Loss: 0.1529\n",
      "Epoch [67/100], Loss: 0.1506\n",
      "Epoch [68/100], Loss: 0.1494\n",
      "Epoch [69/100], Loss: 0.1457\n",
      "Epoch [70/100], Loss: 0.1436\n",
      "Epoch [71/100], Loss: 0.1422\n",
      "Epoch [72/100], Loss: 0.1397\n",
      "Epoch [73/100], Loss: 0.1374\n",
      "Epoch [74/100], Loss: 0.1368\n",
      "Epoch [75/100], Loss: 0.1358\n",
      "Epoch [76/100], Loss: 0.1340\n",
      "Epoch [77/100], Loss: 0.1318\n",
      "Epoch [78/100], Loss: 0.1302\n",
      "Epoch [79/100], Loss: 0.1278\n",
      "Epoch [80/100], Loss: 0.1264\n",
      "Epoch [81/100], Loss: 0.1261\n",
      "Epoch [82/100], Loss: 0.1233\n",
      "Epoch [83/100], Loss: 0.1221\n",
      "Epoch [84/100], Loss: 0.1209\n",
      "Epoch [85/100], Loss: 0.1192\n",
      "Epoch [86/100], Loss: 0.1187\n",
      "Epoch [87/100], Loss: 0.1164\n",
      "Epoch [88/100], Loss: 0.1154\n",
      "Epoch [89/100], Loss: 0.1144\n",
      "Epoch [90/100], Loss: 0.1134\n",
      "Epoch [91/100], Loss: 0.1117\n",
      "Epoch [92/100], Loss: 0.1107\n",
      "Epoch [93/100], Loss: 0.1086\n",
      "Epoch [94/100], Loss: 0.1083\n",
      "Epoch [95/100], Loss: 0.1067\n",
      "Epoch [96/100], Loss: 0.1058\n",
      "Epoch [97/100], Loss: 0.1050\n",
      "Epoch [98/100], Loss: 0.1040\n",
      "Stopping early at epoch 98 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [98/100], Loss: 0.1040\n",
      "Test Accuracy Logit Lipschitz: 56.85%\n",
      "Epoch [1/100], Loss: 2.9369\n",
      "Epoch [2/100], Loss: 1.8259\n",
      "Epoch [3/100], Loss: 1.3445\n",
      "Epoch [4/100], Loss: 1.1315\n",
      "Epoch [5/100], Loss: 0.9998\n",
      "Epoch [6/100], Loss: 0.8940\n",
      "Epoch [7/100], Loss: 0.8152\n",
      "Epoch [8/100], Loss: 0.7507\n",
      "Epoch [9/100], Loss: 0.7114\n",
      "Epoch [10/100], Loss: 0.6664\n",
      "Epoch [11/100], Loss: 0.6273\n",
      "Epoch [12/100], Loss: 0.6015\n",
      "Epoch [13/100], Loss: 0.5687\n",
      "Epoch [14/100], Loss: 0.5427\n",
      "Epoch [15/100], Loss: 0.5204\n",
      "Epoch [16/100], Loss: 0.4990\n",
      "Epoch [17/100], Loss: 0.4785\n",
      "Epoch [18/100], Loss: 0.4609\n",
      "Epoch [19/100], Loss: 0.4482\n",
      "Epoch [20/100], Loss: 0.4274\n",
      "Epoch [21/100], Loss: 0.4174\n",
      "Epoch [22/100], Loss: 0.3998\n",
      "Epoch [23/100], Loss: 0.3902\n",
      "Epoch [24/100], Loss: 0.3736\n",
      "Epoch [25/100], Loss: 0.3650\n",
      "Epoch [26/100], Loss: 0.3546\n",
      "Epoch [27/100], Loss: 0.3461\n",
      "Epoch [28/100], Loss: 0.3352\n",
      "Epoch [29/100], Loss: 0.3290\n",
      "Epoch [30/100], Loss: 0.3209\n",
      "Epoch [31/100], Loss: 0.3091\n",
      "Epoch [32/100], Loss: 0.2996\n",
      "Epoch [33/100], Loss: 0.2957\n",
      "Epoch [34/100], Loss: 0.2835\n",
      "Epoch [35/100], Loss: 0.2831\n",
      "Epoch [36/100], Loss: 0.2742\n",
      "Epoch [37/100], Loss: 0.2664\n",
      "Epoch [38/100], Loss: 0.2626\n",
      "Epoch [39/100], Loss: 0.2548\n",
      "Epoch [40/100], Loss: 0.2512\n",
      "Epoch [41/100], Loss: 0.2490\n",
      "Epoch [42/100], Loss: 0.2412\n",
      "Epoch [43/100], Loss: 0.2365\n",
      "Epoch [44/100], Loss: 0.2297\n",
      "Epoch [45/100], Loss: 0.2254\n",
      "Epoch [46/100], Loss: 0.2219\n",
      "Epoch [47/100], Loss: 0.2178\n",
      "Epoch [48/100], Loss: 0.2154\n",
      "Epoch [49/100], Loss: 0.2122\n",
      "Epoch [50/100], Loss: 0.2082\n",
      "Epoch [51/100], Loss: 0.2042\n",
      "Epoch [52/100], Loss: 0.2012\n",
      "Epoch [53/100], Loss: 0.1964\n",
      "Epoch [54/100], Loss: 0.1946\n",
      "Epoch [55/100], Loss: 0.1895\n",
      "Epoch [56/100], Loss: 0.1887\n",
      "Epoch [57/100], Loss: 0.1854\n",
      "Epoch [58/100], Loss: 0.1815\n",
      "Epoch [59/100], Loss: 0.1815\n",
      "Epoch [60/100], Loss: 0.1758\n",
      "Epoch [61/100], Loss: 0.1734\n",
      "Epoch [62/100], Loss: 0.1700\n",
      "Epoch [63/100], Loss: 0.1685\n",
      "Epoch [64/100], Loss: 0.1662\n",
      "Epoch [65/100], Loss: 0.1641\n",
      "Epoch [66/100], Loss: 0.1621\n",
      "Epoch [67/100], Loss: 0.1605\n",
      "Epoch [68/100], Loss: 0.1577\n",
      "Epoch [69/100], Loss: 0.1567\n",
      "Epoch [70/100], Loss: 0.1531\n",
      "Epoch [71/100], Loss: 0.1511\n",
      "Epoch [72/100], Loss: 0.1491\n",
      "Epoch [73/100], Loss: 0.1487\n",
      "Epoch [74/100], Loss: 0.1468\n",
      "Epoch [75/100], Loss: 0.1430\n",
      "Epoch [76/100], Loss: 0.1422\n",
      "Epoch [77/100], Loss: 0.1399\n",
      "Epoch [78/100], Loss: 0.1404\n",
      "Epoch [79/100], Loss: 0.1376\n",
      "Epoch [80/100], Loss: 0.1374\n",
      "Epoch [81/100], Loss: 0.1350\n",
      "Epoch [82/100], Loss: 0.1328\n",
      "Epoch [83/100], Loss: 0.1314\n",
      "Epoch [84/100], Loss: 0.1300\n",
      "Epoch [85/100], Loss: 0.1286\n",
      "Epoch [86/100], Loss: 0.1275\n",
      "Epoch [87/100], Loss: 0.1270\n",
      "Epoch [88/100], Loss: 0.1248\n",
      "Epoch [89/100], Loss: 0.1252\n",
      "Epoch [90/100], Loss: 0.1240\n",
      "Epoch [91/100], Loss: 0.1223\n",
      "Epoch [92/100], Loss: 0.1212\n",
      "Epoch [93/100], Loss: 0.1190\n",
      "Epoch [94/100], Loss: 0.1175\n",
      "Epoch [95/100], Loss: 0.1175\n",
      "Epoch [96/100], Loss: 0.1146\n",
      "Epoch [97/100], Loss: 0.1145\n",
      "Epoch [98/100], Loss: 0.1133\n",
      "Epoch [99/100], Loss: 0.1118\n",
      "Epoch [100/100], Loss: 0.1108\n",
      "Subset 1000, Epoch [100/100], Loss: 0.1108\n",
      "Test Accuracy Logit Lipschitz: 56.21%\n",
      "Epoch [1/100], Loss: 2.9158\n",
      "Epoch [2/100], Loss: 1.7687\n",
      "Epoch [3/100], Loss: 1.2679\n",
      "Epoch [4/100], Loss: 1.0327\n",
      "Epoch [5/100], Loss: 0.9054\n",
      "Epoch [6/100], Loss: 0.8038\n",
      "Epoch [7/100], Loss: 0.7363\n",
      "Epoch [8/100], Loss: 0.6816\n",
      "Epoch [9/100], Loss: 0.6405\n",
      "Epoch [10/100], Loss: 0.6001\n",
      "Epoch [11/100], Loss: 0.5632\n",
      "Epoch [12/100], Loss: 0.5398\n",
      "Epoch [13/100], Loss: 0.5085\n",
      "Epoch [14/100], Loss: 0.4875\n",
      "Epoch [15/100], Loss: 0.4637\n",
      "Epoch [16/100], Loss: 0.4447\n",
      "Epoch [17/100], Loss: 0.4257\n",
      "Epoch [18/100], Loss: 0.4090\n",
      "Epoch [19/100], Loss: 0.3959\n",
      "Epoch [20/100], Loss: 0.3805\n",
      "Epoch [21/100], Loss: 0.3692\n",
      "Epoch [22/100], Loss: 0.3580\n",
      "Epoch [23/100], Loss: 0.3471\n",
      "Epoch [24/100], Loss: 0.3299\n",
      "Epoch [25/100], Loss: 0.3226\n",
      "Epoch [26/100], Loss: 0.3128\n",
      "Epoch [27/100], Loss: 0.3039\n",
      "Epoch [28/100], Loss: 0.2948\n",
      "Epoch [29/100], Loss: 0.2850\n",
      "Epoch [30/100], Loss: 0.2783\n",
      "Epoch [31/100], Loss: 0.2708\n",
      "Epoch [32/100], Loss: 0.2639\n",
      "Epoch [33/100], Loss: 0.2597\n",
      "Epoch [34/100], Loss: 0.2529\n",
      "Epoch [35/100], Loss: 0.2479\n",
      "Epoch [36/100], Loss: 0.2393\n",
      "Epoch [37/100], Loss: 0.2359\n",
      "Epoch [38/100], Loss: 0.2281\n",
      "Epoch [39/100], Loss: 0.2231\n",
      "Epoch [40/100], Loss: 0.2195\n",
      "Epoch [41/100], Loss: 0.2128\n",
      "Epoch [42/100], Loss: 0.2109\n",
      "Epoch [43/100], Loss: 0.2081\n",
      "Epoch [44/100], Loss: 0.2026\n",
      "Epoch [45/100], Loss: 0.1963\n",
      "Epoch [46/100], Loss: 0.1928\n",
      "Epoch [47/100], Loss: 0.1900\n",
      "Epoch [48/100], Loss: 0.1870\n",
      "Epoch [49/100], Loss: 0.1845\n",
      "Epoch [50/100], Loss: 0.1801\n",
      "Epoch [51/100], Loss: 0.1780\n",
      "Epoch [52/100], Loss: 0.1739\n",
      "Epoch [53/100], Loss: 0.1699\n",
      "Epoch [54/100], Loss: 0.1683\n",
      "Epoch [55/100], Loss: 0.1640\n",
      "Epoch [56/100], Loss: 0.1626\n",
      "Epoch [57/100], Loss: 0.1604\n",
      "Epoch [58/100], Loss: 0.1577\n",
      "Epoch [59/100], Loss: 0.1565\n",
      "Epoch [60/100], Loss: 0.1512\n",
      "Epoch [61/100], Loss: 0.1511\n",
      "Epoch [62/100], Loss: 0.1467\n",
      "Epoch [63/100], Loss: 0.1465\n",
      "Epoch [64/100], Loss: 0.1433\n",
      "Epoch [65/100], Loss: 0.1412\n",
      "Epoch [66/100], Loss: 0.1394\n",
      "Epoch [67/100], Loss: 0.1360\n",
      "Epoch [68/100], Loss: 0.1354\n",
      "Epoch [69/100], Loss: 0.1331\n",
      "Epoch [70/100], Loss: 0.1313\n",
      "Epoch [71/100], Loss: 0.1292\n",
      "Epoch [72/100], Loss: 0.1288\n",
      "Epoch [73/100], Loss: 0.1279\n",
      "Epoch [74/100], Loss: 0.1233\n",
      "Epoch [75/100], Loss: 0.1239\n",
      "Epoch [76/100], Loss: 0.1228\n",
      "Epoch [77/100], Loss: 0.1203\n",
      "Epoch [78/100], Loss: 0.1180\n",
      "Epoch [79/100], Loss: 0.1178\n",
      "Epoch [80/100], Loss: 0.1160\n",
      "Epoch [81/100], Loss: 0.1132\n",
      "Epoch [82/100], Loss: 0.1147\n",
      "Epoch [83/100], Loss: 0.1113\n",
      "Epoch [84/100], Loss: 0.1102\n",
      "Epoch [85/100], Loss: 0.1087\n",
      "Epoch [86/100], Loss: 0.1080\n",
      "Epoch [87/100], Loss: 0.1061\n",
      "Epoch [88/100], Loss: 0.1050\n",
      "Epoch [89/100], Loss: 0.1036\n",
      "Epoch [90/100], Loss: 0.1026\n",
      "Epoch [91/100], Loss: 0.1019\n",
      "Epoch [92/100], Loss: 0.1004\n",
      "Epoch [93/100], Loss: 0.0993\n",
      "Epoch [94/100], Loss: 0.0993\n",
      "Epoch [95/100], Loss: 0.0976\n",
      "Epoch [96/100], Loss: 0.0966\n",
      "Epoch [97/100], Loss: 0.0972\n",
      "Epoch [98/100], Loss: 0.0952\n",
      "Epoch [99/100], Loss: 0.0946\n",
      "Epoch [100/100], Loss: 0.0939\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0939\n",
      "Test Accuracy Logit Lipschitz: 57.01%\n",
      "Epoch [1/100], Loss: 2.9284\n",
      "Epoch [2/100], Loss: 1.7978\n",
      "Epoch [3/100], Loss: 1.3096\n",
      "Epoch [4/100], Loss: 1.1104\n",
      "Epoch [5/100], Loss: 0.9803\n",
      "Epoch [6/100], Loss: 0.8823\n",
      "Epoch [7/100], Loss: 0.7992\n",
      "Epoch [8/100], Loss: 0.7473\n",
      "Epoch [9/100], Loss: 0.6937\n",
      "Epoch [10/100], Loss: 0.6562\n",
      "Epoch [11/100], Loss: 0.6196\n",
      "Epoch [12/100], Loss: 0.5937\n",
      "Epoch [13/100], Loss: 0.5623\n",
      "Epoch [14/100], Loss: 0.5354\n",
      "Epoch [15/100], Loss: 0.5129\n",
      "Epoch [16/100], Loss: 0.4868\n",
      "Epoch [17/100], Loss: 0.4703\n",
      "Epoch [18/100], Loss: 0.4485\n",
      "Epoch [19/100], Loss: 0.4320\n",
      "Epoch [20/100], Loss: 0.4184\n",
      "Epoch [21/100], Loss: 0.4083\n",
      "Epoch [22/100], Loss: 0.3925\n",
      "Epoch [23/100], Loss: 0.3777\n",
      "Epoch [24/100], Loss: 0.3646\n",
      "Epoch [25/100], Loss: 0.3509\n",
      "Epoch [26/100], Loss: 0.3409\n",
      "Epoch [27/100], Loss: 0.3309\n",
      "Epoch [28/100], Loss: 0.3203\n",
      "Epoch [29/100], Loss: 0.3096\n",
      "Epoch [30/100], Loss: 0.3032\n",
      "Epoch [31/100], Loss: 0.2962\n",
      "Epoch [32/100], Loss: 0.2865\n",
      "Epoch [33/100], Loss: 0.2796\n",
      "Epoch [34/100], Loss: 0.2737\n",
      "Epoch [35/100], Loss: 0.2646\n",
      "Epoch [36/100], Loss: 0.2589\n",
      "Epoch [37/100], Loss: 0.2525\n",
      "Epoch [38/100], Loss: 0.2464\n",
      "Epoch [39/100], Loss: 0.2404\n",
      "Epoch [40/100], Loss: 0.2348\n",
      "Epoch [41/100], Loss: 0.2284\n",
      "Epoch [42/100], Loss: 0.2253\n",
      "Epoch [43/100], Loss: 0.2187\n",
      "Epoch [44/100], Loss: 0.2145\n",
      "Epoch [45/100], Loss: 0.2093\n",
      "Epoch [46/100], Loss: 0.2054\n",
      "Epoch [47/100], Loss: 0.2004\n",
      "Epoch [48/100], Loss: 0.1960\n",
      "Epoch [49/100], Loss: 0.1920\n",
      "Epoch [50/100], Loss: 0.1907\n",
      "Epoch [51/100], Loss: 0.1866\n",
      "Epoch [52/100], Loss: 0.1832\n",
      "Epoch [53/100], Loss: 0.1808\n",
      "Epoch [54/100], Loss: 0.1770\n",
      "Epoch [55/100], Loss: 0.1742\n",
      "Epoch [56/100], Loss: 0.1703\n",
      "Epoch [57/100], Loss: 0.1661\n",
      "Epoch [58/100], Loss: 0.1644\n",
      "Epoch [59/100], Loss: 0.1611\n",
      "Epoch [60/100], Loss: 0.1594\n",
      "Epoch [61/100], Loss: 0.1565\n",
      "Epoch [62/100], Loss: 0.1532\n",
      "Epoch [63/100], Loss: 0.1524\n",
      "Epoch [64/100], Loss: 0.1494\n",
      "Epoch [65/100], Loss: 0.1463\n",
      "Epoch [66/100], Loss: 0.1441\n",
      "Epoch [67/100], Loss: 0.1415\n",
      "Epoch [68/100], Loss: 0.1401\n",
      "Epoch [69/100], Loss: 0.1382\n",
      "Epoch [70/100], Loss: 0.1354\n",
      "Epoch [71/100], Loss: 0.1352\n",
      "Epoch [72/100], Loss: 0.1327\n",
      "Epoch [73/100], Loss: 0.1309\n",
      "Epoch [74/100], Loss: 0.1284\n",
      "Epoch [75/100], Loss: 0.1267\n",
      "Epoch [76/100], Loss: 0.1256\n",
      "Epoch [77/100], Loss: 0.1237\n",
      "Epoch [78/100], Loss: 0.1221\n",
      "Epoch [79/100], Loss: 0.1203\n",
      "Epoch [80/100], Loss: 0.1188\n",
      "Epoch [81/100], Loss: 0.1182\n",
      "Epoch [82/100], Loss: 0.1164\n",
      "Epoch [83/100], Loss: 0.1160\n",
      "Epoch [84/100], Loss: 0.1134\n",
      "Epoch [85/100], Loss: 0.1123\n",
      "Epoch [86/100], Loss: 0.1103\n",
      "Epoch [87/100], Loss: 0.1093\n",
      "Epoch [88/100], Loss: 0.1084\n",
      "Epoch [89/100], Loss: 0.1071\n",
      "Epoch [90/100], Loss: 0.1054\n",
      "Epoch [91/100], Loss: 0.1044\n",
      "Epoch [92/100], Loss: 0.1038\n",
      "Epoch [93/100], Loss: 0.1032\n",
      "Stopping early at epoch 93 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [93/100], Loss: 0.1032\n",
      "Test Accuracy Logit Lipschitz: 55.58%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "\n",
    "logit_accuracy_lipschitz = {}\n",
    "\n",
    "for subset_size in num_subsets:\n",
    "    accuracy_list = []\n",
    "    print(f\"Training with subset size: {subset_size}\")\n",
    "\n",
    "    for _ in range(5):\n",
    "        logit_model_lipschitz = LogisticRegression(input_dim, num_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_sdg = optim.SGD(logit_model_lipschitz.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        subset_indices = torch.randperm(train_data.shape[0])[:subset_size]\n",
    "        subset = SubsetRandomSampler(subset_indices)\n",
    "        train_loader = DataLoader(train_set, batch_size=128, sampler=subset)\n",
    "\n",
    "        previous_loss = float('inf')  # Track the best loss\n",
    "        epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            logit_model_lipschitz.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = logit_model_lipschitz(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer_sdg.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "\n",
    "                L = max(param.grad.abs().max().item() for param in logit_model_lipschitz.parameters() if param.grad is not None)\n",
    "                adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "                # Update optimizer's learning rate\n",
    "                for param_group in optimizer_sdg.param_groups:\n",
    "                    param_group['lr'] = adaptive_lr\n",
    "                optimizer_sdg.step()\n",
    "\n",
    "                total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "            current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "            # Check for early stopping\n",
    "            if previous_loss - current_loss < tolerance:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "                    break\n",
    "            else:\n",
    "                epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "            previous_loss = current_loss\n",
    "\n",
    "        print(f\"Subset {subset_size}, Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        logit_model_lipschitz.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                images = images.view(-1, 28*28)\n",
    "                outputs = logit_model_lipschitz(images)\n",
    "                _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Test Accuracy Logit Lipschitz: {accuracy:.2f}%\")\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    logit_accuracy_lipschitz[subset_size] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000    69.626923\n",
       " 50000    69.232692\n",
       " 10000    65.230769\n",
       " 5000     63.593269\n",
       " 1000     56.577885\n",
       " dtype: float64,\n",
       " 75000    0.737010\n",
       " 50000    0.299751\n",
       " 10000    0.577424\n",
       " 5000     1.238593\n",
       " 1000     0.674547\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_accuracy_lipschitz_df = pd.DataFrame(logit_accuracy_lipschitz)\n",
    "logit_accuracy_lipschitz_df.mean(), logit_accuracy_lipschitz_df.std(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with subset size: 75000\n",
      "Epoch [1/75], Loss: 0.6171\n",
      "Epoch [1/75], Val Loss: 0.2986\n",
      "Epoch [2/75], Loss: 0.2586\n",
      "Epoch [2/75], Val Loss: 0.2294\n",
      "Epoch [3/75], Loss: 0.2104\n",
      "Epoch [3/75], Val Loss: 0.1989\n",
      "Epoch [4/75], Loss: 0.1800\n",
      "Epoch [4/75], Val Loss: 0.1714\n",
      "Epoch [5/75], Loss: 0.1603\n",
      "Epoch [5/75], Val Loss: 0.1710\n",
      "Epoch [6/75], Loss: 0.1409\n",
      "Epoch [6/75], Val Loss: 0.1650\n",
      "Epoch [7/75], Loss: 0.1270\n",
      "Epoch [7/75], Val Loss: 0.1456\n",
      "Epoch [8/75], Loss: 0.1148\n",
      "Epoch [8/75], Val Loss: 0.1480\n",
      "Epoch [9/75], Loss: 0.1032\n",
      "Epoch [9/75], Val Loss: 0.1416\n",
      "Epoch [10/75], Loss: 0.0947\n",
      "Epoch [10/75], Val Loss: 0.1387\n",
      "Epoch [11/75], Loss: 0.0876\n",
      "Epoch [11/75], Val Loss: 0.1341\n",
      "Epoch [12/75], Loss: 0.0808\n",
      "Epoch [12/75], Val Loss: 0.1357\n",
      "Epoch [13/75], Loss: 0.0736\n",
      "Epoch [13/75], Val Loss: 0.1434\n",
      "Epoch [14/75], Loss: 0.0673\n",
      "Epoch [14/75], Val Loss: 0.1435\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [14/75], Loss: 0.0673\n",
      "Test Accuracy Base CNN: 92.70%\n",
      "Epoch [1/75], Loss: 0.6515\n",
      "Epoch [1/75], Val Loss: 0.3037\n",
      "Epoch [2/75], Loss: 0.2746\n",
      "Epoch [2/75], Val Loss: 0.2337\n",
      "Epoch [3/75], Loss: 0.2160\n",
      "Epoch [3/75], Val Loss: 0.2011\n",
      "Epoch [4/75], Loss: 0.1841\n",
      "Epoch [4/75], Val Loss: 0.1817\n",
      "Epoch [5/75], Loss: 0.1604\n",
      "Epoch [5/75], Val Loss: 0.1792\n",
      "Epoch [6/75], Loss: 0.1428\n",
      "Epoch [6/75], Val Loss: 0.1767\n",
      "Epoch [7/75], Loss: 0.1294\n",
      "Epoch [7/75], Val Loss: 0.1564\n",
      "Epoch [8/75], Loss: 0.1161\n",
      "Epoch [8/75], Val Loss: 0.1480\n",
      "Epoch [9/75], Loss: 0.1050\n",
      "Epoch [9/75], Val Loss: 0.1507\n",
      "Epoch [10/75], Loss: 0.0966\n",
      "Epoch [10/75], Val Loss: 0.1331\n",
      "Epoch [11/75], Loss: 0.0882\n",
      "Epoch [11/75], Val Loss: 0.1370\n",
      "Epoch [12/75], Loss: 0.0821\n",
      "Epoch [12/75], Val Loss: 0.1351\n",
      "Epoch [13/75], Loss: 0.0747\n",
      "Epoch [13/75], Val Loss: 0.1430\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [13/75], Loss: 0.0747\n",
      "Test Accuracy Base CNN: 92.66%\n",
      "Epoch [1/75], Loss: 0.6632\n",
      "Epoch [1/75], Val Loss: 0.3011\n",
      "Epoch [2/75], Loss: 0.2727\n",
      "Epoch [2/75], Val Loss: 0.2425\n",
      "Epoch [3/75], Loss: 0.2203\n",
      "Epoch [3/75], Val Loss: 0.2043\n",
      "Epoch [4/75], Loss: 0.1885\n",
      "Epoch [4/75], Val Loss: 0.1777\n",
      "Epoch [5/75], Loss: 0.1650\n",
      "Epoch [5/75], Val Loss: 0.1692\n",
      "Epoch [6/75], Loss: 0.1472\n",
      "Epoch [6/75], Val Loss: 0.1549\n",
      "Epoch [7/75], Loss: 0.1316\n",
      "Epoch [7/75], Val Loss: 0.1468\n",
      "Epoch [8/75], Loss: 0.1203\n",
      "Epoch [8/75], Val Loss: 0.1669\n",
      "Epoch [9/75], Loss: 0.1082\n",
      "Epoch [9/75], Val Loss: 0.1384\n",
      "Epoch [10/75], Loss: 0.0992\n",
      "Epoch [10/75], Val Loss: 0.1393\n",
      "Epoch [11/75], Loss: 0.0909\n",
      "Epoch [11/75], Val Loss: 0.1404\n",
      "Epoch [12/75], Loss: 0.0823\n",
      "Epoch [12/75], Val Loss: 0.1352\n",
      "Epoch [13/75], Loss: 0.0762\n",
      "Epoch [13/75], Val Loss: 0.1454\n",
      "Epoch [14/75], Loss: 0.0706\n",
      "Epoch [14/75], Val Loss: 0.1331\n",
      "Epoch [15/75], Loss: 0.0643\n",
      "Epoch [15/75], Val Loss: 0.1394\n",
      "Epoch [16/75], Loss: 0.0619\n",
      "Epoch [16/75], Val Loss: 0.1400\n",
      "Epoch [17/75], Loss: 0.0563\n",
      "Epoch [17/75], Val Loss: 0.1357\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [17/75], Loss: 0.0563\n",
      "Test Accuracy Base CNN: 93.13%\n",
      "Epoch [1/75], Loss: 0.6281\n",
      "Epoch [1/75], Val Loss: 0.3071\n",
      "Epoch [2/75], Loss: 0.2650\n",
      "Epoch [2/75], Val Loss: 0.2290\n",
      "Epoch [3/75], Loss: 0.2082\n",
      "Epoch [3/75], Val Loss: 0.2038\n",
      "Epoch [4/75], Loss: 0.1787\n",
      "Epoch [4/75], Val Loss: 0.1907\n",
      "Epoch [5/75], Loss: 0.1576\n",
      "Epoch [5/75], Val Loss: 0.1680\n",
      "Epoch [6/75], Loss: 0.1376\n",
      "Epoch [6/75], Val Loss: 0.1595\n",
      "Epoch [7/75], Loss: 0.1231\n",
      "Epoch [7/75], Val Loss: 0.1554\n",
      "Epoch [8/75], Loss: 0.1119\n",
      "Epoch [8/75], Val Loss: 0.1413\n",
      "Epoch [9/75], Loss: 0.0998\n",
      "Epoch [9/75], Val Loss: 0.1419\n",
      "Epoch [10/75], Loss: 0.0922\n",
      "Epoch [10/75], Val Loss: 0.1429\n",
      "Epoch [11/75], Loss: 0.0850\n",
      "Epoch [11/75], Val Loss: 0.1482\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [11/75], Loss: 0.0850\n",
      "Test Accuracy Base CNN: 92.61%\n",
      "Epoch [1/75], Loss: 0.6345\n",
      "Epoch [1/75], Val Loss: 0.3077\n",
      "Epoch [2/75], Loss: 0.2713\n",
      "Epoch [2/75], Val Loss: 0.2411\n",
      "Epoch [3/75], Loss: 0.2184\n",
      "Epoch [3/75], Val Loss: 0.1979\n",
      "Epoch [4/75], Loss: 0.1869\n",
      "Epoch [4/75], Val Loss: 0.1919\n",
      "Epoch [5/75], Loss: 0.1643\n",
      "Epoch [5/75], Val Loss: 0.1799\n",
      "Epoch [6/75], Loss: 0.1473\n",
      "Epoch [6/75], Val Loss: 0.1595\n",
      "Epoch [7/75], Loss: 0.1326\n",
      "Epoch [7/75], Val Loss: 0.1626\n",
      "Epoch [8/75], Loss: 0.1187\n",
      "Epoch [8/75], Val Loss: 0.1501\n",
      "Epoch [9/75], Loss: 0.1080\n",
      "Epoch [9/75], Val Loss: 0.1425\n",
      "Epoch [10/75], Loss: 0.1001\n",
      "Epoch [10/75], Val Loss: 0.1421\n",
      "Epoch [11/75], Loss: 0.0900\n",
      "Epoch [11/75], Val Loss: 0.1390\n",
      "Epoch [12/75], Loss: 0.0831\n",
      "Epoch [12/75], Val Loss: 0.1493\n",
      "Epoch [13/75], Loss: 0.0755\n",
      "Epoch [13/75], Val Loss: 0.1319\n",
      "Epoch [14/75], Loss: 0.0703\n",
      "Epoch [14/75], Val Loss: 0.1501\n",
      "Epoch [15/75], Loss: 0.0668\n",
      "Epoch [15/75], Val Loss: 0.1348\n",
      "Epoch [16/75], Loss: 0.0589\n",
      "Epoch [16/75], Val Loss: 0.1454\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [16/75], Loss: 0.0589\n",
      "Test Accuracy Base CNN: 92.91%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/75], Loss: 0.7665\n",
      "Epoch [1/75], Val Loss: 0.3489\n",
      "Epoch [2/75], Loss: 0.3040\n",
      "Epoch [2/75], Val Loss: 0.2616\n",
      "Epoch [3/75], Loss: 0.2351\n",
      "Epoch [3/75], Val Loss: 0.2480\n",
      "Epoch [4/75], Loss: 0.1941\n",
      "Epoch [4/75], Val Loss: 0.2210\n",
      "Epoch [5/75], Loss: 0.1707\n",
      "Epoch [5/75], Val Loss: 0.2047\n",
      "Epoch [6/75], Loss: 0.1473\n",
      "Epoch [6/75], Val Loss: 0.1871\n",
      "Epoch [7/75], Loss: 0.1293\n",
      "Epoch [7/75], Val Loss: 0.1944\n",
      "Epoch [8/75], Loss: 0.1179\n",
      "Epoch [8/75], Val Loss: 0.1925\n",
      "Epoch [9/75], Loss: 0.1008\n",
      "Epoch [9/75], Val Loss: 0.2018\n",
      "Stopping early at epoch 9 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [9/75], Loss: 0.1008\n",
      "Test Accuracy Base CNN: 92.44%\n",
      "Epoch [1/75], Loss: 0.7465\n",
      "Epoch [1/75], Val Loss: 0.3390\n",
      "Epoch [2/75], Loss: 0.3065\n",
      "Epoch [2/75], Val Loss: 0.2834\n",
      "Epoch [3/75], Loss: 0.2375\n",
      "Epoch [3/75], Val Loss: 0.2437\n",
      "Epoch [4/75], Loss: 0.1979\n",
      "Epoch [4/75], Val Loss: 0.2255\n",
      "Epoch [5/75], Loss: 0.1714\n",
      "Epoch [5/75], Val Loss: 0.2008\n",
      "Epoch [6/75], Loss: 0.1502\n",
      "Epoch [6/75], Val Loss: 0.2032\n",
      "Epoch [7/75], Loss: 0.1342\n",
      "Epoch [7/75], Val Loss: 0.1939\n",
      "Epoch [8/75], Loss: 0.1199\n",
      "Epoch [8/75], Val Loss: 0.1864\n",
      "Epoch [9/75], Loss: 0.1047\n",
      "Epoch [9/75], Val Loss: 0.1985\n",
      "Epoch [10/75], Loss: 0.0944\n",
      "Epoch [10/75], Val Loss: 0.1924\n",
      "Epoch [11/75], Loss: 0.0874\n",
      "Epoch [11/75], Val Loss: 0.1812\n",
      "Epoch [12/75], Loss: 0.0797\n",
      "Epoch [12/75], Val Loss: 0.1765\n",
      "Epoch [13/75], Loss: 0.0709\n",
      "Epoch [13/75], Val Loss: 0.1885\n",
      "Epoch [14/75], Loss: 0.0672\n",
      "Epoch [14/75], Val Loss: 0.1864\n",
      "Epoch [15/75], Loss: 0.0614\n",
      "Epoch [15/75], Val Loss: 0.1948\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [15/75], Loss: 0.0614\n",
      "Test Accuracy Base CNN: 92.31%\n",
      "Epoch [1/75], Loss: 0.7956\n",
      "Epoch [1/75], Val Loss: 0.3928\n",
      "Epoch [2/75], Loss: 0.3087\n",
      "Epoch [2/75], Val Loss: 0.2805\n",
      "Epoch [3/75], Loss: 0.2426\n",
      "Epoch [3/75], Val Loss: 0.2454\n",
      "Epoch [4/75], Loss: 0.2016\n",
      "Epoch [4/75], Val Loss: 0.2255\n",
      "Epoch [5/75], Loss: 0.1735\n",
      "Epoch [5/75], Val Loss: 0.1930\n",
      "Epoch [6/75], Loss: 0.1513\n",
      "Epoch [6/75], Val Loss: 0.2191\n",
      "Epoch [7/75], Loss: 0.1324\n",
      "Epoch [7/75], Val Loss: 0.1884\n",
      "Epoch [8/75], Loss: 0.1144\n",
      "Epoch [8/75], Val Loss: 0.1929\n",
      "Epoch [9/75], Loss: 0.1029\n",
      "Epoch [9/75], Val Loss: 0.1861\n",
      "Epoch [10/75], Loss: 0.0930\n",
      "Epoch [10/75], Val Loss: 0.1889\n",
      "Epoch [11/75], Loss: 0.0841\n",
      "Epoch [11/75], Val Loss: 0.1907\n",
      "Epoch [12/75], Loss: 0.0759\n",
      "Epoch [12/75], Val Loss: 0.1866\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [12/75], Loss: 0.0759\n",
      "Test Accuracy Base CNN: 92.41%\n",
      "Epoch [1/75], Loss: 0.7484\n",
      "Epoch [1/75], Val Loss: 0.3543\n",
      "Epoch [2/75], Loss: 0.3051\n",
      "Epoch [2/75], Val Loss: 0.2810\n",
      "Epoch [3/75], Loss: 0.2398\n",
      "Epoch [3/75], Val Loss: 0.2379\n",
      "Epoch [4/75], Loss: 0.2008\n",
      "Epoch [4/75], Val Loss: 0.2287\n",
      "Epoch [5/75], Loss: 0.1739\n",
      "Epoch [5/75], Val Loss: 0.2113\n",
      "Epoch [6/75], Loss: 0.1517\n",
      "Epoch [6/75], Val Loss: 0.2008\n",
      "Epoch [7/75], Loss: 0.1347\n",
      "Epoch [7/75], Val Loss: 0.1910\n",
      "Epoch [8/75], Loss: 0.1220\n",
      "Epoch [8/75], Val Loss: 0.1838\n",
      "Epoch [9/75], Loss: 0.1091\n",
      "Epoch [9/75], Val Loss: 0.1921\n",
      "Epoch [10/75], Loss: 0.0964\n",
      "Epoch [10/75], Val Loss: 0.1792\n",
      "Epoch [11/75], Loss: 0.0896\n",
      "Epoch [11/75], Val Loss: 0.1893\n",
      "Epoch [12/75], Loss: 0.0828\n",
      "Epoch [12/75], Val Loss: 0.1890\n",
      "Epoch [13/75], Loss: 0.0720\n",
      "Epoch [13/75], Val Loss: 0.1903\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [13/75], Loss: 0.0720\n",
      "Test Accuracy Base CNN: 92.25%\n",
      "Epoch [1/75], Loss: 0.7673\n",
      "Epoch [1/75], Val Loss: 0.3426\n",
      "Epoch [2/75], Loss: 0.3045\n",
      "Epoch [2/75], Val Loss: 0.2586\n",
      "Epoch [3/75], Loss: 0.2351\n",
      "Epoch [3/75], Val Loss: 0.2339\n",
      "Epoch [4/75], Loss: 0.1967\n",
      "Epoch [4/75], Val Loss: 0.2154\n",
      "Epoch [5/75], Loss: 0.1688\n",
      "Epoch [5/75], Val Loss: 0.2006\n",
      "Epoch [6/75], Loss: 0.1471\n",
      "Epoch [6/75], Val Loss: 0.2015\n",
      "Epoch [7/75], Loss: 0.1297\n",
      "Epoch [7/75], Val Loss: 0.1931\n",
      "Epoch [8/75], Loss: 0.1148\n",
      "Epoch [8/75], Val Loss: 0.1778\n",
      "Epoch [9/75], Loss: 0.1052\n",
      "Epoch [9/75], Val Loss: 0.1782\n",
      "Epoch [10/75], Loss: 0.0950\n",
      "Epoch [10/75], Val Loss: 0.1791\n",
      "Epoch [11/75], Loss: 0.0819\n",
      "Epoch [11/75], Val Loss: 0.1741\n",
      "Epoch [12/75], Loss: 0.0766\n",
      "Epoch [12/75], Val Loss: 0.1816\n",
      "Epoch [13/75], Loss: 0.0702\n",
      "Epoch [13/75], Val Loss: 0.1857\n",
      "Epoch [14/75], Loss: 0.0631\n",
      "Epoch [14/75], Val Loss: 0.1900\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [14/75], Loss: 0.0631\n",
      "Test Accuracy Base CNN: 92.20%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/75], Loss: 1.6771\n",
      "Epoch [1/75], Val Loss: 0.9327\n",
      "Epoch [2/75], Loss: 0.7141\n",
      "Epoch [2/75], Val Loss: 0.6137\n",
      "Epoch [3/75], Loss: 0.4849\n",
      "Epoch [3/75], Val Loss: 0.5659\n",
      "Epoch [4/75], Loss: 0.3792\n",
      "Epoch [4/75], Val Loss: 0.4537\n",
      "Epoch [5/75], Loss: 0.3008\n",
      "Epoch [5/75], Val Loss: 0.3990\n",
      "Epoch [6/75], Loss: 0.2361\n",
      "Epoch [6/75], Val Loss: 0.3945\n",
      "Epoch [7/75], Loss: 0.1978\n",
      "Epoch [7/75], Val Loss: 0.3808\n",
      "Epoch [8/75], Loss: 0.1648\n",
      "Epoch [8/75], Val Loss: 0.3790\n",
      "Epoch [9/75], Loss: 0.1349\n",
      "Epoch [9/75], Val Loss: 0.3706\n",
      "Epoch [10/75], Loss: 0.1104\n",
      "Epoch [10/75], Val Loss: 0.3738\n",
      "Epoch [11/75], Loss: 0.1032\n",
      "Epoch [11/75], Val Loss: 0.3714\n",
      "Epoch [12/75], Loss: 0.0819\n",
      "Epoch [12/75], Val Loss: 0.3936\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [12/75], Loss: 0.0819\n",
      "Test Accuracy Base CNN: 88.69%\n",
      "Epoch [1/75], Loss: 1.8037\n",
      "Epoch [1/75], Val Loss: 0.9901\n",
      "Epoch [2/75], Loss: 0.7246\n",
      "Epoch [2/75], Val Loss: 0.6224\n",
      "Epoch [3/75], Loss: 0.4817\n",
      "Epoch [3/75], Val Loss: 0.4905\n",
      "Epoch [4/75], Loss: 0.3600\n",
      "Epoch [4/75], Val Loss: 0.4357\n",
      "Epoch [5/75], Loss: 0.2930\n",
      "Epoch [5/75], Val Loss: 0.4012\n",
      "Epoch [6/75], Loss: 0.2534\n",
      "Epoch [6/75], Val Loss: 0.3990\n",
      "Epoch [7/75], Loss: 0.2301\n",
      "Epoch [7/75], Val Loss: 0.3919\n",
      "Epoch [8/75], Loss: 0.1672\n",
      "Epoch [8/75], Val Loss: 0.3955\n",
      "Epoch [9/75], Loss: 0.1303\n",
      "Epoch [9/75], Val Loss: 0.4118\n",
      "Epoch [10/75], Loss: 0.1196\n",
      "Epoch [10/75], Val Loss: 0.4188\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [10/75], Loss: 0.1196\n",
      "Test Accuracy Base CNN: 87.16%\n",
      "Epoch [1/75], Loss: 1.6331\n",
      "Epoch [1/75], Val Loss: 0.9247\n",
      "Epoch [2/75], Loss: 0.6629\n",
      "Epoch [2/75], Val Loss: 0.5547\n",
      "Epoch [3/75], Loss: 0.4373\n",
      "Epoch [3/75], Val Loss: 0.4568\n",
      "Epoch [4/75], Loss: 0.3373\n",
      "Epoch [4/75], Val Loss: 0.4223\n",
      "Epoch [5/75], Loss: 0.2582\n",
      "Epoch [5/75], Val Loss: 0.3974\n",
      "Epoch [6/75], Loss: 0.2323\n",
      "Epoch [6/75], Val Loss: 0.3723\n",
      "Epoch [7/75], Loss: 0.2068\n",
      "Epoch [7/75], Val Loss: 0.4122\n",
      "Epoch [8/75], Loss: 0.1761\n",
      "Epoch [8/75], Val Loss: 0.4286\n",
      "Epoch [9/75], Loss: 0.1361\n",
      "Epoch [9/75], Val Loss: 0.3548\n",
      "Epoch [10/75], Loss: 0.1056\n",
      "Epoch [10/75], Val Loss: 0.3889\n",
      "Epoch [11/75], Loss: 0.0946\n",
      "Epoch [11/75], Val Loss: 0.3762\n",
      "Epoch [12/75], Loss: 0.0808\n",
      "Epoch [12/75], Val Loss: 0.4045\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [12/75], Loss: 0.0808\n",
      "Test Accuracy Base CNN: 88.41%\n",
      "Epoch [1/75], Loss: 1.6977\n",
      "Epoch [1/75], Val Loss: 0.8884\n",
      "Epoch [2/75], Loss: 0.6870\n",
      "Epoch [2/75], Val Loss: 0.5954\n",
      "Epoch [3/75], Loss: 0.4519\n",
      "Epoch [3/75], Val Loss: 0.4571\n",
      "Epoch [4/75], Loss: 0.3391\n",
      "Epoch [4/75], Val Loss: 0.4315\n",
      "Epoch [5/75], Loss: 0.2840\n",
      "Epoch [5/75], Val Loss: 0.4025\n",
      "Epoch [6/75], Loss: 0.2514\n",
      "Epoch [6/75], Val Loss: 0.3931\n",
      "Epoch [7/75], Loss: 0.2018\n",
      "Epoch [7/75], Val Loss: 0.3835\n",
      "Epoch [8/75], Loss: 0.1544\n",
      "Epoch [8/75], Val Loss: 0.3885\n",
      "Epoch [9/75], Loss: 0.1303\n",
      "Epoch [9/75], Val Loss: 0.4002\n",
      "Epoch [10/75], Loss: 0.1223\n",
      "Epoch [10/75], Val Loss: 0.3702\n",
      "Epoch [11/75], Loss: 0.0984\n",
      "Epoch [11/75], Val Loss: 0.3852\n",
      "Epoch [12/75], Loss: 0.0816\n",
      "Epoch [12/75], Val Loss: 0.3838\n",
      "Epoch [13/75], Loss: 0.0672\n",
      "Epoch [13/75], Val Loss: 0.4036\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [13/75], Loss: 0.0672\n",
      "Test Accuracy Base CNN: 88.62%\n",
      "Epoch [1/75], Loss: 1.6687\n",
      "Epoch [1/75], Val Loss: 0.9182\n",
      "Epoch [2/75], Loss: 0.6675\n",
      "Epoch [2/75], Val Loss: 0.6195\n",
      "Epoch [3/75], Loss: 0.4684\n",
      "Epoch [3/75], Val Loss: 0.5125\n",
      "Epoch [4/75], Loss: 0.3698\n",
      "Epoch [4/75], Val Loss: 0.4382\n",
      "Epoch [5/75], Loss: 0.2704\n",
      "Epoch [5/75], Val Loss: 0.3814\n",
      "Epoch [6/75], Loss: 0.2262\n",
      "Epoch [6/75], Val Loss: 0.3700\n",
      "Epoch [7/75], Loss: 0.1939\n",
      "Epoch [7/75], Val Loss: 0.3826\n",
      "Epoch [8/75], Loss: 0.1486\n",
      "Epoch [8/75], Val Loss: 0.3692\n",
      "Epoch [9/75], Loss: 0.1387\n",
      "Epoch [9/75], Val Loss: 0.3834\n",
      "Epoch [10/75], Loss: 0.1124\n",
      "Epoch [10/75], Val Loss: 0.3819\n",
      "Epoch [11/75], Loss: 0.1028\n",
      "Epoch [11/75], Val Loss: 0.3677\n",
      "Epoch [12/75], Loss: 0.0800\n",
      "Epoch [12/75], Val Loss: 0.3655\n",
      "Epoch [13/75], Loss: 0.0636\n",
      "Epoch [13/75], Val Loss: 0.3805\n",
      "Epoch [14/75], Loss: 0.0628\n",
      "Epoch [14/75], Val Loss: 0.3819\n",
      "Epoch [15/75], Loss: 0.0481\n",
      "Epoch [15/75], Val Loss: 0.3950\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [15/75], Loss: 0.0481\n",
      "Test Accuracy Base CNN: 89.37%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/75], Loss: 2.3349\n",
      "Epoch [1/75], Val Loss: 1.7305\n",
      "Epoch [2/75], Loss: 1.2166\n",
      "Epoch [2/75], Val Loss: 1.0284\n",
      "Epoch [3/75], Loss: 0.8339\n",
      "Epoch [3/75], Val Loss: 0.7889\n",
      "Epoch [4/75], Loss: 0.6286\n",
      "Epoch [4/75], Val Loss: 0.6509\n",
      "Epoch [5/75], Loss: 0.4508\n",
      "Epoch [5/75], Val Loss: 0.5831\n",
      "Epoch [6/75], Loss: 0.3809\n",
      "Epoch [6/75], Val Loss: 0.5350\n",
      "Epoch [7/75], Loss: 0.2910\n",
      "Epoch [7/75], Val Loss: 0.5334\n",
      "Epoch [8/75], Loss: 0.2224\n",
      "Epoch [8/75], Val Loss: 0.5670\n",
      "Epoch [9/75], Loss: 0.2383\n",
      "Epoch [9/75], Val Loss: 0.5101\n",
      "Epoch [10/75], Loss: 0.1777\n",
      "Epoch [10/75], Val Loss: 0.5073\n",
      "Epoch [11/75], Loss: 0.1409\n",
      "Epoch [11/75], Val Loss: 0.4827\n",
      "Epoch [12/75], Loss: 0.1731\n",
      "Epoch [12/75], Val Loss: 0.5695\n",
      "Epoch [13/75], Loss: 0.1850\n",
      "Epoch [13/75], Val Loss: 0.4732\n",
      "Epoch [14/75], Loss: 0.1429\n",
      "Epoch [14/75], Val Loss: 0.4826\n",
      "Epoch [15/75], Loss: 0.0747\n",
      "Epoch [15/75], Val Loss: 0.4816\n",
      "Epoch [16/75], Loss: 0.0600\n",
      "Epoch [16/75], Val Loss: 0.5283\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [16/75], Loss: 0.0600\n",
      "Test Accuracy Base CNN: 86.56%\n",
      "Epoch [1/75], Loss: 2.1609\n",
      "Epoch [1/75], Val Loss: 1.5526\n",
      "Epoch [2/75], Loss: 1.0743\n",
      "Epoch [2/75], Val Loss: 0.9516\n",
      "Epoch [3/75], Loss: 0.7560\n",
      "Epoch [3/75], Val Loss: 0.7298\n",
      "Epoch [4/75], Loss: 0.5879\n",
      "Epoch [4/75], Val Loss: 0.6151\n",
      "Epoch [5/75], Loss: 0.4184\n",
      "Epoch [5/75], Val Loss: 0.6147\n",
      "Epoch [6/75], Loss: 0.3565\n",
      "Epoch [6/75], Val Loss: 0.5291\n",
      "Epoch [7/75], Loss: 0.2846\n",
      "Epoch [7/75], Val Loss: 0.4839\n",
      "Epoch [8/75], Loss: 0.2287\n",
      "Epoch [8/75], Val Loss: 0.4905\n",
      "Epoch [9/75], Loss: 0.1574\n",
      "Epoch [9/75], Val Loss: 0.4888\n",
      "Epoch [10/75], Loss: 0.1247\n",
      "Epoch [10/75], Val Loss: 0.4859\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [10/75], Loss: 0.1247\n",
      "Test Accuracy Base CNN: 85.59%\n",
      "Epoch [1/75], Loss: 2.2234\n",
      "Epoch [1/75], Val Loss: 1.4512\n",
      "Epoch [2/75], Loss: 1.0570\n",
      "Epoch [2/75], Val Loss: 1.0570\n",
      "Epoch [3/75], Loss: 0.7472\n",
      "Epoch [3/75], Val Loss: 0.8104\n",
      "Epoch [4/75], Loss: 0.5802\n",
      "Epoch [4/75], Val Loss: 0.6637\n",
      "Epoch [5/75], Loss: 0.4414\n",
      "Epoch [5/75], Val Loss: 0.6444\n",
      "Epoch [6/75], Loss: 0.3977\n",
      "Epoch [6/75], Val Loss: 0.5490\n",
      "Epoch [7/75], Loss: 0.3177\n",
      "Epoch [7/75], Val Loss: 0.5153\n",
      "Epoch [8/75], Loss: 0.2355\n",
      "Epoch [8/75], Val Loss: 0.4936\n",
      "Epoch [9/75], Loss: 0.1860\n",
      "Epoch [9/75], Val Loss: 0.5393\n",
      "Epoch [10/75], Loss: 0.1512\n",
      "Epoch [10/75], Val Loss: 0.4739\n",
      "Epoch [11/75], Loss: 0.1105\n",
      "Epoch [11/75], Val Loss: 0.5378\n",
      "Epoch [12/75], Loss: 0.1057\n",
      "Epoch [12/75], Val Loss: 0.4923\n",
      "Epoch [13/75], Loss: 0.0925\n",
      "Epoch [13/75], Val Loss: 0.5122\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [13/75], Loss: 0.0925\n",
      "Test Accuracy Base CNN: 85.43%\n",
      "Epoch [1/75], Loss: 2.2117\n",
      "Epoch [1/75], Val Loss: 1.4739\n",
      "Epoch [2/75], Loss: 1.1260\n",
      "Epoch [2/75], Val Loss: 0.9766\n",
      "Epoch [3/75], Loss: 0.7449\n",
      "Epoch [3/75], Val Loss: 0.7682\n",
      "Epoch [4/75], Loss: 0.5458\n",
      "Epoch [4/75], Val Loss: 0.6441\n",
      "Epoch [5/75], Loss: 0.4505\n",
      "Epoch [5/75], Val Loss: 0.5791\n",
      "Epoch [6/75], Loss: 0.3574\n",
      "Epoch [6/75], Val Loss: 0.5695\n",
      "Epoch [7/75], Loss: 0.2515\n",
      "Epoch [7/75], Val Loss: 0.5030\n",
      "Epoch [8/75], Loss: 0.2522\n",
      "Epoch [8/75], Val Loss: 0.4832\n",
      "Epoch [9/75], Loss: 0.2014\n",
      "Epoch [9/75], Val Loss: 0.5899\n",
      "Epoch [10/75], Loss: 0.2286\n",
      "Epoch [10/75], Val Loss: 0.4838\n",
      "Epoch [11/75], Loss: 0.1267\n",
      "Epoch [11/75], Val Loss: 0.5310\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [11/75], Loss: 0.1267\n",
      "Test Accuracy Base CNN: 84.75%\n",
      "Epoch [1/75], Loss: 2.1632\n",
      "Epoch [1/75], Val Loss: 1.4427\n",
      "Epoch [2/75], Loss: 1.0806\n",
      "Epoch [2/75], Val Loss: 0.9420\n",
      "Epoch [3/75], Loss: 0.7855\n",
      "Epoch [3/75], Val Loss: 0.7404\n",
      "Epoch [4/75], Loss: 0.5539\n",
      "Epoch [4/75], Val Loss: 0.6093\n",
      "Epoch [5/75], Loss: 0.4113\n",
      "Epoch [5/75], Val Loss: 0.5593\n",
      "Epoch [6/75], Loss: 0.3209\n",
      "Epoch [6/75], Val Loss: 0.5186\n",
      "Epoch [7/75], Loss: 0.2505\n",
      "Epoch [7/75], Val Loss: 0.5227\n",
      "Epoch [8/75], Loss: 0.1977\n",
      "Epoch [8/75], Val Loss: 0.4675\n",
      "Epoch [9/75], Loss: 0.1584\n",
      "Epoch [9/75], Val Loss: 0.5275\n",
      "Epoch [10/75], Loss: 0.1388\n",
      "Epoch [10/75], Val Loss: 0.5165\n",
      "Epoch [11/75], Loss: 0.1002\n",
      "Epoch [11/75], Val Loss: 0.5206\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [11/75], Loss: 0.1002\n",
      "Test Accuracy Base CNN: 86.09%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/75], Loss: 3.2259\n",
      "Epoch [1/75], Val Loss: 2.9712\n",
      "Epoch [2/75], Loss: 2.5906\n",
      "Epoch [2/75], Val Loss: 2.2158\n",
      "Epoch [3/75], Loss: 1.7271\n",
      "Epoch [3/75], Val Loss: 1.9001\n",
      "Epoch [4/75], Loss: 1.6252\n",
      "Epoch [4/75], Val Loss: 1.5425\n",
      "Epoch [5/75], Loss: 1.2486\n",
      "Epoch [5/75], Val Loss: 1.4703\n",
      "Epoch [6/75], Loss: 1.0326\n",
      "Epoch [6/75], Val Loss: 1.3304\n",
      "Epoch [7/75], Loss: 0.8156\n",
      "Epoch [7/75], Val Loss: 1.2488\n",
      "Epoch [8/75], Loss: 0.6876\n",
      "Epoch [8/75], Val Loss: 1.2503\n",
      "Epoch [9/75], Loss: 0.6117\n",
      "Epoch [9/75], Val Loss: 1.1945\n",
      "Epoch [10/75], Loss: 0.4710\n",
      "Epoch [10/75], Val Loss: 1.2668\n",
      "Epoch [11/75], Loss: 0.4327\n",
      "Epoch [11/75], Val Loss: 1.2404\n",
      "Epoch [12/75], Loss: 0.3675\n",
      "Epoch [12/75], Val Loss: 1.1827\n",
      "Epoch [13/75], Loss: 0.2542\n",
      "Epoch [13/75], Val Loss: 1.1578\n",
      "Epoch [14/75], Loss: 0.1999\n",
      "Epoch [14/75], Val Loss: 1.1915\n",
      "Epoch [15/75], Loss: 0.1554\n",
      "Epoch [15/75], Val Loss: 1.2506\n",
      "Epoch [16/75], Loss: 0.1194\n",
      "Epoch [16/75], Val Loss: 1.2749\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [16/75], Loss: 0.1194\n",
      "Test Accuracy Base CNN: 69.66%\n",
      "Epoch [1/75], Loss: 3.2261\n",
      "Epoch [1/75], Val Loss: 3.0713\n",
      "Epoch [2/75], Loss: 2.7732\n",
      "Epoch [2/75], Val Loss: 2.4540\n",
      "Epoch [3/75], Loss: 2.0060\n",
      "Epoch [3/75], Val Loss: 1.9262\n",
      "Epoch [4/75], Loss: 1.4711\n",
      "Epoch [4/75], Val Loss: 1.7119\n",
      "Epoch [5/75], Loss: 1.2539\n",
      "Epoch [5/75], Val Loss: 1.5985\n",
      "Epoch [6/75], Loss: 1.0268\n",
      "Epoch [6/75], Val Loss: 1.2867\n",
      "Epoch [7/75], Loss: 0.8148\n",
      "Epoch [7/75], Val Loss: 1.2710\n",
      "Epoch [8/75], Loss: 0.6591\n",
      "Epoch [8/75], Val Loss: 1.1413\n",
      "Epoch [9/75], Loss: 0.5231\n",
      "Epoch [9/75], Val Loss: 1.1611\n",
      "Epoch [10/75], Loss: 0.4500\n",
      "Epoch [10/75], Val Loss: 1.2045\n",
      "Epoch [11/75], Loss: 0.3723\n",
      "Epoch [11/75], Val Loss: 1.1101\n",
      "Epoch [12/75], Loss: 0.2821\n",
      "Epoch [12/75], Val Loss: 1.1524\n",
      "Epoch [13/75], Loss: 0.2179\n",
      "Epoch [13/75], Val Loss: 1.1809\n",
      "Epoch [14/75], Loss: 0.1702\n",
      "Epoch [14/75], Val Loss: 1.1500\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [14/75], Loss: 0.1702\n",
      "Test Accuracy Base CNN: 70.68%\n",
      "Epoch [1/75], Loss: 3.2486\n",
      "Epoch [1/75], Val Loss: 3.1121\n",
      "Epoch [2/75], Loss: 2.8216\n",
      "Epoch [2/75], Val Loss: 2.5159\n",
      "Epoch [3/75], Loss: 2.0446\n",
      "Epoch [3/75], Val Loss: 2.1104\n",
      "Epoch [4/75], Loss: 1.8722\n",
      "Epoch [4/75], Val Loss: 1.6660\n",
      "Epoch [5/75], Loss: 1.3383\n",
      "Epoch [5/75], Val Loss: 1.5078\n",
      "Epoch [6/75], Loss: 1.0616\n",
      "Epoch [6/75], Val Loss: 1.2690\n",
      "Epoch [7/75], Loss: 0.8303\n",
      "Epoch [7/75], Val Loss: 1.1939\n",
      "Epoch [8/75], Loss: 0.6746\n",
      "Epoch [8/75], Val Loss: 1.1483\n",
      "Epoch [9/75], Loss: 0.5635\n",
      "Epoch [9/75], Val Loss: 1.0922\n",
      "Epoch [10/75], Loss: 0.4864\n",
      "Epoch [10/75], Val Loss: 1.0712\n",
      "Epoch [11/75], Loss: 0.3784\n",
      "Epoch [11/75], Val Loss: 1.1253\n",
      "Epoch [12/75], Loss: 0.3444\n",
      "Epoch [12/75], Val Loss: 1.1455\n",
      "Epoch [13/75], Loss: 0.2775\n",
      "Epoch [13/75], Val Loss: 1.1207\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [13/75], Loss: 0.2775\n",
      "Test Accuracy Base CNN: 69.57%\n",
      "Epoch [1/75], Loss: 3.2538\n",
      "Epoch [1/75], Val Loss: 3.0132\n",
      "Epoch [2/75], Loss: 2.6423\n",
      "Epoch [2/75], Val Loss: 2.1579\n",
      "Epoch [3/75], Loss: 1.8029\n",
      "Epoch [3/75], Val Loss: 1.9295\n",
      "Epoch [4/75], Loss: 1.4696\n",
      "Epoch [4/75], Val Loss: 1.6337\n",
      "Epoch [5/75], Loss: 1.2099\n",
      "Epoch [5/75], Val Loss: 1.3620\n",
      "Epoch [6/75], Loss: 0.8995\n",
      "Epoch [6/75], Val Loss: 1.3322\n",
      "Epoch [7/75], Loss: 0.7923\n",
      "Epoch [7/75], Val Loss: 1.3944\n",
      "Epoch [8/75], Loss: 0.6345\n",
      "Epoch [8/75], Val Loss: 1.1989\n",
      "Epoch [9/75], Loss: 0.5109\n",
      "Epoch [9/75], Val Loss: 1.2051\n",
      "Epoch [10/75], Loss: 0.3841\n",
      "Epoch [10/75], Val Loss: 1.2855\n",
      "Epoch [11/75], Loss: 0.3261\n",
      "Epoch [11/75], Val Loss: 1.2643\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [11/75], Loss: 0.3261\n",
      "Test Accuracy Base CNN: 66.69%\n",
      "Epoch [1/75], Loss: 3.2801\n",
      "Epoch [1/75], Val Loss: 3.1303\n",
      "Epoch [2/75], Loss: 2.9529\n",
      "Epoch [2/75], Val Loss: 2.6613\n",
      "Epoch [3/75], Loss: 2.2175\n",
      "Epoch [3/75], Val Loss: 2.0000\n",
      "Epoch [4/75], Loss: 1.6738\n",
      "Epoch [4/75], Val Loss: 1.6306\n",
      "Epoch [5/75], Loss: 1.3835\n",
      "Epoch [5/75], Val Loss: 1.6563\n",
      "Epoch [6/75], Loss: 1.1808\n",
      "Epoch [6/75], Val Loss: 1.4217\n",
      "Epoch [7/75], Loss: 0.9501\n",
      "Epoch [7/75], Val Loss: 1.2496\n",
      "Epoch [8/75], Loss: 0.7867\n",
      "Epoch [8/75], Val Loss: 1.1950\n",
      "Epoch [9/75], Loss: 0.6298\n",
      "Epoch [9/75], Val Loss: 1.0866\n",
      "Epoch [10/75], Loss: 0.4873\n",
      "Epoch [10/75], Val Loss: 1.1222\n",
      "Epoch [11/75], Loss: 0.3885\n",
      "Epoch [11/75], Val Loss: 1.0974\n",
      "Epoch [12/75], Loss: 0.3203\n",
      "Epoch [12/75], Val Loss: 1.0879\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [12/75], Loss: 0.3203\n",
      "Test Accuracy Base CNN: 70.10%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 75\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "logit_accuracy_cnn = {}\n",
    "\n",
    "for subset_size in num_subsets:\n",
    "    accuracy_list = []\n",
    "    print(f\"Training with subset size: {subset_size}\")\n",
    "\n",
    "    for _ in range(5):\n",
    "        cnn_model = EMNIST_CNN(num_classes).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_sdg = optim.SGD(cnn_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "\n",
    "        def init_weights(m):\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)  # Set all weights to zero\n",
    "                    m.bias.data.fill_(0.01)\n",
    "        cnn_model.apply(init_weights)\n",
    "\n",
    "        subset_indices = torch.randperm(train_data.shape[0])[:subset_size]\n",
    "        subset = SubsetRandomSampler(subset_indices)\n",
    "        train_loader = DataLoader(train_set, batch_size=128, sampler=subset)\n",
    "\n",
    "        best_val_loss = float('inf')  # Track the best loss\n",
    "        epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            cnn_model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = cnn_model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer_sdg.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer_sdg.step()\n",
    "\n",
    "                total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "            current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "            # Validation loop\n",
    "            cnn_model.eval()\n",
    "            val_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader_cnn:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                    outputs = cnn_model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader_cnn)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "                    break\n",
    "\n",
    "        print(f\"Subset {subset_size}, Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        cnn_model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = cnn_model(images)\n",
    "                _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Test Accuracy Base CNN: {accuracy:.2f}%\")\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    logit_accuracy_cnn[subset_size] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000    92.802885\n",
       " 50000    92.323077\n",
       " 10000    88.449038\n",
       " 5000     85.684615\n",
       " 1000     69.338462\n",
       " dtype: float64,\n",
       " 75000    0.216266\n",
       " 50000    0.102066\n",
       " 10000    0.803148\n",
       " 5000     0.682003\n",
       " 1000     1.545797\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_accuracy_cnn_df = pd.DataFrame(logit_accuracy_cnn)\n",
    "logit_accuracy_cnn_df.mean(), logit_accuracy_cnn_df.std(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Lipschitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with subset size: 75000\n",
      "Epoch [1/75], Loss: 0.6324\n",
      "Epoch [1/75], Val Loss: 0.2941\n",
      "Epoch [2/75], Loss: 0.2677\n",
      "Epoch [2/75], Val Loss: 0.2286\n",
      "Epoch [3/75], Loss: 0.2153\n",
      "Epoch [3/75], Val Loss: 0.2023\n",
      "Epoch [4/75], Loss: 0.1836\n",
      "Epoch [4/75], Val Loss: 0.1796\n",
      "Epoch [5/75], Loss: 0.1609\n",
      "Epoch [5/75], Val Loss: 0.1754\n",
      "Epoch [6/75], Loss: 0.1430\n",
      "Epoch [6/75], Val Loss: 0.1668\n",
      "Epoch [7/75], Loss: 0.1278\n",
      "Epoch [7/75], Val Loss: 0.1706\n",
      "Epoch [8/75], Loss: 0.1165\n",
      "Epoch [8/75], Val Loss: 0.1474\n",
      "Epoch [9/75], Loss: 0.1062\n",
      "Epoch [9/75], Val Loss: 0.1433\n",
      "Epoch [10/75], Loss: 0.0970\n",
      "Epoch [10/75], Val Loss: 0.1418\n",
      "Epoch [11/75], Loss: 0.0871\n",
      "Epoch [11/75], Val Loss: 0.1434\n",
      "Epoch [12/75], Loss: 0.0837\n",
      "Epoch [12/75], Val Loss: 0.1447\n",
      "Epoch [13/75], Loss: 0.0750\n",
      "Epoch [13/75], Val Loss: 0.1378\n",
      "Epoch [14/75], Loss: 0.0689\n",
      "Epoch [14/75], Val Loss: 0.1400\n",
      "Epoch [15/75], Loss: 0.0645\n",
      "Epoch [15/75], Val Loss: 0.1389\n",
      "Epoch [16/75], Loss: 0.0615\n",
      "Epoch [16/75], Val Loss: 0.1392\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [16/75], Loss: 0.0615\n",
      "Test Accuracy CNN Lipschitz: 93.00%\n",
      "Epoch [1/75], Loss: 0.6600\n",
      "Epoch [1/75], Val Loss: 0.2898\n",
      "Epoch [2/75], Loss: 0.2651\n",
      "Epoch [2/75], Val Loss: 0.2327\n",
      "Epoch [3/75], Loss: 0.2107\n",
      "Epoch [3/75], Val Loss: 0.2039\n",
      "Epoch [4/75], Loss: 0.1823\n",
      "Epoch [4/75], Val Loss: 0.1784\n",
      "Epoch [5/75], Loss: 0.1585\n",
      "Epoch [5/75], Val Loss: 0.1714\n",
      "Epoch [6/75], Loss: 0.1424\n",
      "Epoch [6/75], Val Loss: 0.1601\n",
      "Epoch [7/75], Loss: 0.1270\n",
      "Epoch [7/75], Val Loss: 0.1485\n",
      "Epoch [8/75], Loss: 0.1152\n",
      "Epoch [8/75], Val Loss: 0.1437\n",
      "Epoch [9/75], Loss: 0.1061\n",
      "Epoch [9/75], Val Loss: 0.1421\n",
      "Epoch [10/75], Loss: 0.0948\n",
      "Epoch [10/75], Val Loss: 0.1468\n",
      "Epoch [11/75], Loss: 0.0869\n",
      "Epoch [11/75], Val Loss: 0.1367\n",
      "Epoch [12/75], Loss: 0.0805\n",
      "Epoch [12/75], Val Loss: 0.1423\n",
      "Epoch [13/75], Loss: 0.0740\n",
      "Epoch [13/75], Val Loss: 0.1416\n",
      "Epoch [14/75], Loss: 0.0687\n",
      "Epoch [14/75], Val Loss: 0.1279\n",
      "Epoch [15/75], Loss: 0.0643\n",
      "Epoch [15/75], Val Loss: 0.1396\n",
      "Epoch [16/75], Loss: 0.0600\n",
      "Epoch [16/75], Val Loss: 0.1392\n",
      "Epoch [17/75], Loss: 0.0577\n",
      "Epoch [17/75], Val Loss: 0.1391\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [17/75], Loss: 0.0577\n",
      "Test Accuracy CNN Lipschitz: 92.92%\n",
      "Epoch [1/75], Loss: 0.6294\n",
      "Epoch [1/75], Val Loss: 0.2931\n",
      "Epoch [2/75], Loss: 0.2644\n",
      "Epoch [2/75], Val Loss: 0.2388\n",
      "Epoch [3/75], Loss: 0.2113\n",
      "Epoch [3/75], Val Loss: 0.2124\n",
      "Epoch [4/75], Loss: 0.1818\n",
      "Epoch [4/75], Val Loss: 0.1834\n",
      "Epoch [5/75], Loss: 0.1594\n",
      "Epoch [5/75], Val Loss: 0.1750\n",
      "Epoch [6/75], Loss: 0.1440\n",
      "Epoch [6/75], Val Loss: 0.1663\n",
      "Epoch [7/75], Loss: 0.1262\n",
      "Epoch [7/75], Val Loss: 0.1526\n",
      "Epoch [8/75], Loss: 0.1147\n",
      "Epoch [8/75], Val Loss: 0.1494\n",
      "Epoch [9/75], Loss: 0.1044\n",
      "Epoch [9/75], Val Loss: 0.1452\n",
      "Epoch [10/75], Loss: 0.0947\n",
      "Epoch [10/75], Val Loss: 0.1402\n",
      "Epoch [11/75], Loss: 0.0875\n",
      "Epoch [11/75], Val Loss: 0.1489\n",
      "Epoch [12/75], Loss: 0.0798\n",
      "Epoch [12/75], Val Loss: 0.1430\n",
      "Epoch [13/75], Loss: 0.0752\n",
      "Epoch [13/75], Val Loss: 0.1523\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [13/75], Loss: 0.0752\n",
      "Test Accuracy CNN Lipschitz: 92.62%\n",
      "Epoch [1/75], Loss: 0.6390\n",
      "Epoch [1/75], Val Loss: 0.3012\n",
      "Epoch [2/75], Loss: 0.2694\n",
      "Epoch [2/75], Val Loss: 0.2352\n",
      "Epoch [3/75], Loss: 0.2134\n",
      "Epoch [3/75], Val Loss: 0.2041\n",
      "Epoch [4/75], Loss: 0.1815\n",
      "Epoch [4/75], Val Loss: 0.1884\n",
      "Epoch [5/75], Loss: 0.1596\n",
      "Epoch [5/75], Val Loss: 0.1600\n",
      "Epoch [6/75], Loss: 0.1411\n",
      "Epoch [6/75], Val Loss: 0.1604\n",
      "Epoch [7/75], Loss: 0.1266\n",
      "Epoch [7/75], Val Loss: 0.1456\n",
      "Epoch [8/75], Loss: 0.1141\n",
      "Epoch [8/75], Val Loss: 0.1411\n",
      "Epoch [9/75], Loss: 0.1030\n",
      "Epoch [9/75], Val Loss: 0.1395\n",
      "Epoch [10/75], Loss: 0.0956\n",
      "Epoch [10/75], Val Loss: 0.1348\n",
      "Epoch [11/75], Loss: 0.0865\n",
      "Epoch [11/75], Val Loss: 0.1389\n",
      "Epoch [12/75], Loss: 0.0792\n",
      "Epoch [12/75], Val Loss: 0.1305\n",
      "Epoch [13/75], Loss: 0.0739\n",
      "Epoch [13/75], Val Loss: 0.1351\n",
      "Epoch [14/75], Loss: 0.0681\n",
      "Epoch [14/75], Val Loss: 0.1337\n",
      "Epoch [15/75], Loss: 0.0645\n",
      "Epoch [15/75], Val Loss: 0.1278\n",
      "Epoch [16/75], Loss: 0.0594\n",
      "Epoch [16/75], Val Loss: 0.1505\n",
      "Epoch [17/75], Loss: 0.0562\n",
      "Epoch [17/75], Val Loss: 0.1314\n",
      "Epoch [18/75], Loss: 0.0514\n",
      "Epoch [18/75], Val Loss: 0.1394\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [18/75], Loss: 0.0514\n",
      "Test Accuracy CNN Lipschitz: 92.66%\n",
      "Epoch [1/75], Loss: 0.6229\n",
      "Epoch [1/75], Val Loss: 0.3168\n",
      "Epoch [2/75], Loss: 0.2635\n",
      "Epoch [2/75], Val Loss: 0.2379\n",
      "Epoch [3/75], Loss: 0.2085\n",
      "Epoch [3/75], Val Loss: 0.2027\n",
      "Epoch [4/75], Loss: 0.1800\n",
      "Epoch [4/75], Val Loss: 0.1819\n",
      "Epoch [5/75], Loss: 0.1566\n",
      "Epoch [5/75], Val Loss: 0.1707\n",
      "Epoch [6/75], Loss: 0.1386\n",
      "Epoch [6/75], Val Loss: 0.1620\n",
      "Epoch [7/75], Loss: 0.1238\n",
      "Epoch [7/75], Val Loss: 0.1487\n",
      "Epoch [8/75], Loss: 0.1123\n",
      "Epoch [8/75], Val Loss: 0.1438\n",
      "Epoch [9/75], Loss: 0.1007\n",
      "Epoch [9/75], Val Loss: 0.1454\n",
      "Epoch [10/75], Loss: 0.0925\n",
      "Epoch [10/75], Val Loss: 0.1450\n",
      "Epoch [11/75], Loss: 0.0841\n",
      "Epoch [11/75], Val Loss: 0.1304\n",
      "Epoch [12/75], Loss: 0.0748\n",
      "Epoch [12/75], Val Loss: 0.1329\n",
      "Epoch [13/75], Loss: 0.0706\n",
      "Epoch [13/75], Val Loss: 0.1412\n",
      "Epoch [14/75], Loss: 0.0663\n",
      "Epoch [14/75], Val Loss: 0.1480\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [14/75], Loss: 0.0663\n",
      "Test Accuracy CNN Lipschitz: 92.24%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/75], Loss: 0.7649\n",
      "Epoch [1/75], Val Loss: 0.3515\n",
      "Epoch [2/75], Loss: 0.2983\n",
      "Epoch [2/75], Val Loss: 0.2702\n",
      "Epoch [3/75], Loss: 0.2359\n",
      "Epoch [3/75], Val Loss: 0.2314\n",
      "Epoch [4/75], Loss: 0.1958\n",
      "Epoch [4/75], Val Loss: 0.2168\n",
      "Epoch [5/75], Loss: 0.1695\n",
      "Epoch [5/75], Val Loss: 0.1935\n",
      "Epoch [6/75], Loss: 0.1494\n",
      "Epoch [6/75], Val Loss: 0.2056\n",
      "Epoch [7/75], Loss: 0.1323\n",
      "Epoch [7/75], Val Loss: 0.1899\n",
      "Epoch [8/75], Loss: 0.1160\n",
      "Epoch [8/75], Val Loss: 0.1921\n",
      "Epoch [9/75], Loss: 0.1069\n",
      "Epoch [9/75], Val Loss: 0.1860\n",
      "Epoch [10/75], Loss: 0.0977\n",
      "Epoch [10/75], Val Loss: 0.1855\n",
      "Epoch [11/75], Loss: 0.0855\n",
      "Epoch [11/75], Val Loss: 0.1900\n",
      "Epoch [12/75], Loss: 0.0783\n",
      "Epoch [12/75], Val Loss: 0.1769\n",
      "Epoch [13/75], Loss: 0.0728\n",
      "Epoch [13/75], Val Loss: 0.1906\n",
      "Epoch [14/75], Loss: 0.0668\n",
      "Epoch [14/75], Val Loss: 0.1968\n",
      "Epoch [15/75], Loss: 0.0596\n",
      "Epoch [15/75], Val Loss: 0.2061\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [15/75], Loss: 0.0596\n",
      "Test Accuracy CNN Lipschitz: 92.01%\n",
      "Epoch [1/75], Loss: 0.7758\n",
      "Epoch [1/75], Val Loss: 0.3622\n",
      "Epoch [2/75], Loss: 0.3069\n",
      "Epoch [2/75], Val Loss: 0.2945\n",
      "Epoch [3/75], Loss: 0.2370\n",
      "Epoch [3/75], Val Loss: 0.2324\n",
      "Epoch [4/75], Loss: 0.1963\n",
      "Epoch [4/75], Val Loss: 0.2185\n",
      "Epoch [5/75], Loss: 0.1691\n",
      "Epoch [5/75], Val Loss: 0.2227\n",
      "Epoch [6/75], Loss: 0.1488\n",
      "Epoch [6/75], Val Loss: 0.1979\n",
      "Epoch [7/75], Loss: 0.1317\n",
      "Epoch [7/75], Val Loss: 0.1946\n",
      "Epoch [8/75], Loss: 0.1190\n",
      "Epoch [8/75], Val Loss: 0.1956\n",
      "Epoch [9/75], Loss: 0.1047\n",
      "Epoch [9/75], Val Loss: 0.1900\n",
      "Epoch [10/75], Loss: 0.0935\n",
      "Epoch [10/75], Val Loss: 0.1995\n",
      "Epoch [11/75], Loss: 0.0845\n",
      "Epoch [11/75], Val Loss: 0.2011\n",
      "Epoch [12/75], Loss: 0.0803\n",
      "Epoch [12/75], Val Loss: 0.1934\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [12/75], Loss: 0.0803\n",
      "Test Accuracy CNN Lipschitz: 92.37%\n",
      "Epoch [1/75], Loss: 0.7763\n",
      "Epoch [1/75], Val Loss: 0.3609\n",
      "Epoch [2/75], Loss: 0.3053\n",
      "Epoch [2/75], Val Loss: 0.2737\n",
      "Epoch [3/75], Loss: 0.2357\n",
      "Epoch [3/75], Val Loss: 0.2266\n",
      "Epoch [4/75], Loss: 0.1962\n",
      "Epoch [4/75], Val Loss: 0.2251\n",
      "Epoch [5/75], Loss: 0.1699\n",
      "Epoch [5/75], Val Loss: 0.2028\n",
      "Epoch [6/75], Loss: 0.1476\n",
      "Epoch [6/75], Val Loss: 0.2045\n",
      "Epoch [7/75], Loss: 0.1267\n",
      "Epoch [7/75], Val Loss: 0.1886\n",
      "Epoch [8/75], Loss: 0.1126\n",
      "Epoch [8/75], Val Loss: 0.1926\n",
      "Epoch [9/75], Loss: 0.1038\n",
      "Epoch [9/75], Val Loss: 0.1859\n",
      "Epoch [10/75], Loss: 0.0917\n",
      "Epoch [10/75], Val Loss: 0.1772\n",
      "Epoch [11/75], Loss: 0.0842\n",
      "Epoch [11/75], Val Loss: 0.1860\n",
      "Epoch [12/75], Loss: 0.0727\n",
      "Epoch [12/75], Val Loss: 0.1840\n",
      "Epoch [13/75], Loss: 0.0679\n",
      "Epoch [13/75], Val Loss: 0.1861\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [13/75], Loss: 0.0679\n",
      "Test Accuracy CNN Lipschitz: 92.66%\n",
      "Epoch [1/75], Loss: 0.7364\n",
      "Epoch [1/75], Val Loss: 0.3553\n",
      "Epoch [2/75], Loss: 0.2993\n",
      "Epoch [2/75], Val Loss: 0.2845\n",
      "Epoch [3/75], Loss: 0.2326\n",
      "Epoch [3/75], Val Loss: 0.2353\n",
      "Epoch [4/75], Loss: 0.1928\n",
      "Epoch [4/75], Val Loss: 0.2265\n",
      "Epoch [5/75], Loss: 0.1671\n",
      "Epoch [5/75], Val Loss: 0.2175\n",
      "Epoch [6/75], Loss: 0.1433\n",
      "Epoch [6/75], Val Loss: 0.2065\n",
      "Epoch [7/75], Loss: 0.1287\n",
      "Epoch [7/75], Val Loss: 0.1901\n",
      "Epoch [8/75], Loss: 0.1133\n",
      "Epoch [8/75], Val Loss: 0.1938\n",
      "Epoch [9/75], Loss: 0.1012\n",
      "Epoch [9/75], Val Loss: 0.1902\n",
      "Epoch [10/75], Loss: 0.0918\n",
      "Epoch [10/75], Val Loss: 0.1819\n",
      "Epoch [11/75], Loss: 0.0818\n",
      "Epoch [11/75], Val Loss: 0.1861\n",
      "Epoch [12/75], Loss: 0.0750\n",
      "Epoch [12/75], Val Loss: 0.1929\n",
      "Epoch [13/75], Loss: 0.0688\n",
      "Epoch [13/75], Val Loss: 0.1860\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [13/75], Loss: 0.0688\n",
      "Test Accuracy CNN Lipschitz: 92.46%\n",
      "Epoch [1/75], Loss: 0.7940\n",
      "Epoch [1/75], Val Loss: 0.3514\n",
      "Epoch [2/75], Loss: 0.3043\n",
      "Epoch [2/75], Val Loss: 0.2671\n",
      "Epoch [3/75], Loss: 0.2352\n",
      "Epoch [3/75], Val Loss: 0.2363\n",
      "Epoch [4/75], Loss: 0.1943\n",
      "Epoch [4/75], Val Loss: 0.2097\n",
      "Epoch [5/75], Loss: 0.1690\n",
      "Epoch [5/75], Val Loss: 0.2010\n",
      "Epoch [6/75], Loss: 0.1477\n",
      "Epoch [6/75], Val Loss: 0.1968\n",
      "Epoch [7/75], Loss: 0.1312\n",
      "Epoch [7/75], Val Loss: 0.1981\n",
      "Epoch [8/75], Loss: 0.1164\n",
      "Epoch [8/75], Val Loss: 0.1972\n",
      "Epoch [9/75], Loss: 0.0995\n",
      "Epoch [9/75], Val Loss: 0.1983\n",
      "Stopping early at epoch 9 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [9/75], Loss: 0.0995\n",
      "Test Accuracy CNN Lipschitz: 92.07%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/75], Loss: 1.7552\n",
      "Epoch [1/75], Val Loss: 0.9651\n",
      "Epoch [2/75], Loss: 0.7219\n",
      "Epoch [2/75], Val Loss: 0.6096\n",
      "Epoch [3/75], Loss: 0.4728\n",
      "Epoch [3/75], Val Loss: 0.4912\n",
      "Epoch [4/75], Loss: 0.3687\n",
      "Epoch [4/75], Val Loss: 0.4168\n",
      "Epoch [5/75], Loss: 0.2968\n",
      "Epoch [5/75], Val Loss: 0.4137\n",
      "Epoch [6/75], Loss: 0.2380\n",
      "Epoch [6/75], Val Loss: 0.3616\n",
      "Epoch [7/75], Loss: 0.2065\n",
      "Epoch [7/75], Val Loss: 0.4064\n",
      "Epoch [8/75], Loss: 0.1737\n",
      "Epoch [8/75], Val Loss: 0.3625\n",
      "Epoch [9/75], Loss: 0.1342\n",
      "Epoch [9/75], Val Loss: 0.3640\n",
      "Stopping early at epoch 9 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [9/75], Loss: 0.1342\n",
      "Test Accuracy CNN Lipschitz: 88.42%\n",
      "Epoch [1/75], Loss: 1.6733\n",
      "Epoch [1/75], Val Loss: 0.8936\n",
      "Epoch [2/75], Loss: 0.7021\n",
      "Epoch [2/75], Val Loss: 0.6160\n",
      "Epoch [3/75], Loss: 0.4699\n",
      "Epoch [3/75], Val Loss: 0.4671\n",
      "Epoch [4/75], Loss: 0.3483\n",
      "Epoch [4/75], Val Loss: 0.4436\n",
      "Epoch [5/75], Loss: 0.2764\n",
      "Epoch [5/75], Val Loss: 0.3997\n",
      "Epoch [6/75], Loss: 0.2473\n",
      "Epoch [6/75], Val Loss: 0.3771\n",
      "Epoch [7/75], Loss: 0.1791\n",
      "Epoch [7/75], Val Loss: 0.3864\n",
      "Epoch [8/75], Loss: 0.1688\n",
      "Epoch [8/75], Val Loss: 0.3788\n",
      "Epoch [9/75], Loss: 0.1271\n",
      "Epoch [9/75], Val Loss: 0.3887\n",
      "Stopping early at epoch 9 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [9/75], Loss: 0.1271\n",
      "Test Accuracy CNN Lipschitz: 87.54%\n",
      "Epoch [1/75], Loss: 1.6885\n",
      "Epoch [1/75], Val Loss: 0.9978\n",
      "Epoch [2/75], Loss: 0.7595\n",
      "Epoch [2/75], Val Loss: 0.6453\n",
      "Epoch [3/75], Loss: 0.5199\n",
      "Epoch [3/75], Val Loss: 0.5258\n",
      "Epoch [4/75], Loss: 0.3992\n",
      "Epoch [4/75], Val Loss: 0.4832\n",
      "Epoch [5/75], Loss: 0.2994\n",
      "Epoch [5/75], Val Loss: 0.4065\n",
      "Epoch [6/75], Loss: 0.2490\n",
      "Epoch [6/75], Val Loss: 0.4113\n",
      "Epoch [7/75], Loss: 0.2139\n",
      "Epoch [7/75], Val Loss: 0.3564\n",
      "Epoch [8/75], Loss: 0.1781\n",
      "Epoch [8/75], Val Loss: 0.3726\n",
      "Epoch [9/75], Loss: 0.1444\n",
      "Epoch [9/75], Val Loss: 0.4078\n",
      "Epoch [10/75], Loss: 0.1397\n",
      "Epoch [10/75], Val Loss: 0.3790\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [10/75], Loss: 0.1397\n",
      "Test Accuracy CNN Lipschitz: 88.06%\n",
      "Epoch [1/75], Loss: 1.7271\n",
      "Epoch [1/75], Val Loss: 0.8827\n",
      "Epoch [2/75], Loss: 0.6892\n",
      "Epoch [2/75], Val Loss: 0.6044\n",
      "Epoch [3/75], Loss: 0.4712\n",
      "Epoch [3/75], Val Loss: 0.4900\n",
      "Epoch [4/75], Loss: 0.3693\n",
      "Epoch [4/75], Val Loss: 0.4335\n",
      "Epoch [5/75], Loss: 0.2941\n",
      "Epoch [5/75], Val Loss: 0.3999\n",
      "Epoch [6/75], Loss: 0.2287\n",
      "Epoch [6/75], Val Loss: 0.4316\n",
      "Epoch [7/75], Loss: 0.1836\n",
      "Epoch [7/75], Val Loss: 0.3957\n",
      "Epoch [8/75], Loss: 0.1491\n",
      "Epoch [8/75], Val Loss: 0.3984\n",
      "Epoch [9/75], Loss: 0.1283\n",
      "Epoch [9/75], Val Loss: 0.4001\n",
      "Epoch [10/75], Loss: 0.1153\n",
      "Epoch [10/75], Val Loss: 0.4261\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [10/75], Loss: 0.1153\n",
      "Test Accuracy CNN Lipschitz: 87.57%\n",
      "Epoch [1/75], Loss: 1.7680\n",
      "Epoch [1/75], Val Loss: 0.9746\n",
      "Epoch [2/75], Loss: 0.7648\n",
      "Epoch [2/75], Val Loss: 0.6721\n",
      "Epoch [3/75], Loss: 0.5134\n",
      "Epoch [3/75], Val Loss: 0.5638\n",
      "Epoch [4/75], Loss: 0.4074\n",
      "Epoch [4/75], Val Loss: 0.4433\n",
      "Epoch [5/75], Loss: 0.3294\n",
      "Epoch [5/75], Val Loss: 0.4135\n",
      "Epoch [6/75], Loss: 0.2494\n",
      "Epoch [6/75], Val Loss: 0.3637\n",
      "Epoch [7/75], Loss: 0.2116\n",
      "Epoch [7/75], Val Loss: 0.3809\n",
      "Epoch [8/75], Loss: 0.1859\n",
      "Epoch [8/75], Val Loss: 0.3885\n",
      "Epoch [9/75], Loss: 0.1527\n",
      "Epoch [9/75], Val Loss: 0.3562\n",
      "Epoch [10/75], Loss: 0.1288\n",
      "Epoch [10/75], Val Loss: 0.3770\n",
      "Epoch [11/75], Loss: 0.1116\n",
      "Epoch [11/75], Val Loss: 0.3529\n",
      "Epoch [12/75], Loss: 0.0903\n",
      "Epoch [12/75], Val Loss: 0.3648\n",
      "Epoch [13/75], Loss: 0.0790\n",
      "Epoch [13/75], Val Loss: 0.3917\n",
      "Epoch [14/75], Loss: 0.0668\n",
      "Epoch [14/75], Val Loss: 0.3942\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [14/75], Loss: 0.0668\n",
      "Test Accuracy CNN Lipschitz: 88.75%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/75], Loss: 2.3416\n",
      "Epoch [1/75], Val Loss: 1.4797\n",
      "Epoch [2/75], Loss: 1.0461\n",
      "Epoch [2/75], Val Loss: 0.9233\n",
      "Epoch [3/75], Loss: 0.7092\n",
      "Epoch [3/75], Val Loss: 0.7214\n",
      "Epoch [4/75], Loss: 0.5263\n",
      "Epoch [4/75], Val Loss: 0.6851\n",
      "Epoch [5/75], Loss: 0.4294\n",
      "Epoch [5/75], Val Loss: 0.6573\n",
      "Epoch [6/75], Loss: 0.3910\n",
      "Epoch [6/75], Val Loss: 0.5615\n",
      "Epoch [7/75], Loss: 0.3381\n",
      "Epoch [7/75], Val Loss: 0.5958\n",
      "Epoch [8/75], Loss: 0.3283\n",
      "Epoch [8/75], Val Loss: 0.5362\n",
      "Epoch [9/75], Loss: 0.2832\n",
      "Epoch [9/75], Val Loss: 0.5137\n",
      "Epoch [10/75], Loss: 0.1690\n",
      "Epoch [10/75], Val Loss: 0.5090\n",
      "Epoch [11/75], Loss: 0.1838\n",
      "Epoch [11/75], Val Loss: 0.5322\n",
      "Epoch [12/75], Loss: 0.1215\n",
      "Epoch [12/75], Val Loss: 0.5252\n",
      "Epoch [13/75], Loss: 0.0857\n",
      "Epoch [13/75], Val Loss: 0.5435\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [13/75], Loss: 0.0857\n",
      "Test Accuracy CNN Lipschitz: 85.11%\n",
      "Epoch [1/75], Loss: 2.3169\n",
      "Epoch [1/75], Val Loss: 1.4675\n",
      "Epoch [2/75], Loss: 1.0689\n",
      "Epoch [2/75], Val Loss: 0.9700\n",
      "Epoch [3/75], Loss: 0.7670\n",
      "Epoch [3/75], Val Loss: 0.7477\n",
      "Epoch [4/75], Loss: 0.5650\n",
      "Epoch [4/75], Val Loss: 0.5926\n",
      "Epoch [5/75], Loss: 0.4121\n",
      "Epoch [5/75], Val Loss: 0.5989\n",
      "Epoch [6/75], Loss: 0.3963\n",
      "Epoch [6/75], Val Loss: 0.4919\n",
      "Epoch [7/75], Loss: 0.3058\n",
      "Epoch [7/75], Val Loss: 0.5375\n",
      "Epoch [8/75], Loss: 0.2594\n",
      "Epoch [8/75], Val Loss: 0.5320\n",
      "Epoch [9/75], Loss: 0.2696\n",
      "Epoch [9/75], Val Loss: 0.4682\n",
      "Epoch [10/75], Loss: 0.1585\n",
      "Epoch [10/75], Val Loss: 0.5292\n",
      "Epoch [11/75], Loss: 0.2138\n",
      "Epoch [11/75], Val Loss: 0.4659\n",
      "Epoch [12/75], Loss: 0.1068\n",
      "Epoch [12/75], Val Loss: 0.4819\n",
      "Epoch [13/75], Loss: 0.0734\n",
      "Epoch [13/75], Val Loss: 0.5361\n",
      "Epoch [14/75], Loss: 0.0955\n",
      "Epoch [14/75], Val Loss: 0.5214\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [14/75], Loss: 0.0955\n",
      "Test Accuracy CNN Lipschitz: 85.60%\n",
      "Epoch [1/75], Loss: 2.3206\n",
      "Epoch [1/75], Val Loss: 1.4514\n",
      "Epoch [2/75], Loss: 1.1429\n",
      "Epoch [2/75], Val Loss: 0.9873\n",
      "Epoch [3/75], Loss: 0.8144\n",
      "Epoch [3/75], Val Loss: 0.7746\n",
      "Epoch [4/75], Loss: 0.6153\n",
      "Epoch [4/75], Val Loss: 0.6111\n",
      "Epoch [5/75], Loss: 0.4558\n",
      "Epoch [5/75], Val Loss: 0.5561\n",
      "Epoch [6/75], Loss: 0.3576\n",
      "Epoch [6/75], Val Loss: 0.5007\n",
      "Epoch [7/75], Loss: 0.2853\n",
      "Epoch [7/75], Val Loss: 0.4881\n",
      "Epoch [8/75], Loss: 0.2748\n",
      "Epoch [8/75], Val Loss: 0.5231\n",
      "Epoch [9/75], Loss: 0.3421\n",
      "Epoch [9/75], Val Loss: 0.4883\n",
      "Epoch [10/75], Loss: 0.2360\n",
      "Epoch [10/75], Val Loss: 0.4970\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [10/75], Loss: 0.2360\n",
      "Test Accuracy CNN Lipschitz: 85.55%\n",
      "Epoch [1/75], Loss: 2.2016\n",
      "Epoch [1/75], Val Loss: 1.8010\n",
      "Epoch [2/75], Loss: 1.1913\n",
      "Epoch [2/75], Val Loss: 0.9283\n",
      "Epoch [3/75], Loss: 0.7361\n",
      "Epoch [3/75], Val Loss: 0.7818\n",
      "Epoch [4/75], Loss: 0.6066\n",
      "Epoch [4/75], Val Loss: 0.6233\n",
      "Epoch [5/75], Loss: 0.4521\n",
      "Epoch [5/75], Val Loss: 0.5795\n",
      "Epoch [6/75], Loss: 0.3688\n",
      "Epoch [6/75], Val Loss: 0.5513\n",
      "Epoch [7/75], Loss: 0.3087\n",
      "Epoch [7/75], Val Loss: 0.4891\n",
      "Epoch [8/75], Loss: 0.2402\n",
      "Epoch [8/75], Val Loss: 0.5015\n",
      "Epoch [9/75], Loss: 0.2172\n",
      "Epoch [9/75], Val Loss: 0.4455\n",
      "Epoch [10/75], Loss: 0.1695\n",
      "Epoch [10/75], Val Loss: 0.4747\n",
      "Epoch [11/75], Loss: 0.1505\n",
      "Epoch [11/75], Val Loss: 0.5374\n",
      "Epoch [12/75], Loss: 0.1291\n",
      "Epoch [12/75], Val Loss: 0.5066\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [12/75], Loss: 0.1291\n",
      "Test Accuracy CNN Lipschitz: 85.72%\n",
      "Epoch [1/75], Loss: 2.2976\n",
      "Epoch [1/75], Val Loss: 1.5285\n",
      "Epoch [2/75], Loss: 1.1427\n",
      "Epoch [2/75], Val Loss: 0.9122\n",
      "Epoch [3/75], Loss: 0.7549\n",
      "Epoch [3/75], Val Loss: 0.7137\n",
      "Epoch [4/75], Loss: 0.5927\n",
      "Epoch [4/75], Val Loss: 0.6943\n",
      "Epoch [5/75], Loss: 0.4635\n",
      "Epoch [5/75], Val Loss: 0.6113\n",
      "Epoch [6/75], Loss: 0.3534\n",
      "Epoch [6/75], Val Loss: 0.5013\n",
      "Epoch [7/75], Loss: 0.2579\n",
      "Epoch [7/75], Val Loss: 0.4958\n",
      "Epoch [8/75], Loss: 0.2178\n",
      "Epoch [8/75], Val Loss: 0.5779\n",
      "Epoch [9/75], Loss: 0.2868\n",
      "Epoch [9/75], Val Loss: 0.5297\n",
      "Epoch [10/75], Loss: 0.1988\n",
      "Epoch [10/75], Val Loss: 0.4797\n",
      "Epoch [11/75], Loss: 0.1130\n",
      "Epoch [11/75], Val Loss: 0.4717\n",
      "Epoch [12/75], Loss: 0.1159\n",
      "Epoch [12/75], Val Loss: 0.4746\n",
      "Epoch [13/75], Loss: 0.0790\n",
      "Epoch [13/75], Val Loss: 0.4823\n",
      "Epoch [14/75], Loss: 0.0770\n",
      "Epoch [14/75], Val Loss: 0.4980\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [14/75], Loss: 0.0770\n",
      "Test Accuracy CNN Lipschitz: 86.65%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/75], Loss: 3.2300\n",
      "Epoch [1/75], Val Loss: 2.9563\n",
      "Epoch [2/75], Loss: 2.5102\n",
      "Epoch [2/75], Val Loss: 2.0970\n",
      "Epoch [3/75], Loss: 1.7172\n",
      "Epoch [3/75], Val Loss: 1.8033\n",
      "Epoch [4/75], Loss: 1.3532\n",
      "Epoch [4/75], Val Loss: 1.6336\n",
      "Epoch [5/75], Loss: 1.0762\n",
      "Epoch [5/75], Val Loss: 1.4139\n",
      "Epoch [6/75], Loss: 0.9084\n",
      "Epoch [6/75], Val Loss: 1.3559\n",
      "Epoch [7/75], Loss: 0.7028\n",
      "Epoch [7/75], Val Loss: 1.2581\n",
      "Epoch [8/75], Loss: 0.5596\n",
      "Epoch [8/75], Val Loss: 1.2334\n",
      "Epoch [9/75], Loss: 0.4335\n",
      "Epoch [9/75], Val Loss: 1.3160\n",
      "Epoch [10/75], Loss: 0.3627\n",
      "Epoch [10/75], Val Loss: 1.1650\n",
      "Epoch [11/75], Loss: 0.2634\n",
      "Epoch [11/75], Val Loss: 1.2243\n",
      "Epoch [12/75], Loss: 0.2215\n",
      "Epoch [12/75], Val Loss: 1.2757\n",
      "Epoch [13/75], Loss: 0.2006\n",
      "Epoch [13/75], Val Loss: 1.1832\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [13/75], Loss: 0.2006\n",
      "Test Accuracy CNN Lipschitz: 69.99%\n",
      "Epoch [1/75], Loss: 3.2579\n",
      "Epoch [1/75], Val Loss: 3.1134\n",
      "Epoch [2/75], Loss: 2.9180\n",
      "Epoch [2/75], Val Loss: 2.6368\n",
      "Epoch [3/75], Loss: 2.1950\n",
      "Epoch [3/75], Val Loss: 1.8803\n",
      "Epoch [4/75], Loss: 1.5437\n",
      "Epoch [4/75], Val Loss: 1.8086\n",
      "Epoch [5/75], Loss: 1.4344\n",
      "Epoch [5/75], Val Loss: 1.5108\n",
      "Epoch [6/75], Loss: 1.1396\n",
      "Epoch [6/75], Val Loss: 1.3446\n",
      "Epoch [7/75], Loss: 0.8950\n",
      "Epoch [7/75], Val Loss: 1.2856\n",
      "Epoch [8/75], Loss: 0.7253\n",
      "Epoch [8/75], Val Loss: 1.2290\n",
      "Epoch [9/75], Loss: 0.6185\n",
      "Epoch [9/75], Val Loss: 1.1635\n",
      "Epoch [10/75], Loss: 0.5285\n",
      "Epoch [10/75], Val Loss: 1.1341\n",
      "Epoch [11/75], Loss: 0.4136\n",
      "Epoch [11/75], Val Loss: 1.1202\n",
      "Epoch [12/75], Loss: 0.3257\n",
      "Epoch [12/75], Val Loss: 1.0810\n",
      "Epoch [13/75], Loss: 0.2656\n",
      "Epoch [13/75], Val Loss: 1.1124\n",
      "Epoch [14/75], Loss: 0.2208\n",
      "Epoch [14/75], Val Loss: 1.1751\n",
      "Epoch [15/75], Loss: 0.1973\n",
      "Epoch [15/75], Val Loss: 1.1615\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [15/75], Loss: 0.1973\n",
      "Test Accuracy CNN Lipschitz: 70.72%\n",
      "Epoch [1/75], Loss: 3.2381\n",
      "Epoch [1/75], Val Loss: 3.0257\n",
      "Epoch [2/75], Loss: 2.6729\n",
      "Epoch [2/75], Val Loss: 2.2316\n",
      "Epoch [3/75], Loss: 1.8018\n",
      "Epoch [3/75], Val Loss: 1.8877\n",
      "Epoch [4/75], Loss: 1.5124\n",
      "Epoch [4/75], Val Loss: 1.8573\n",
      "Epoch [5/75], Loss: 1.3224\n",
      "Epoch [5/75], Val Loss: 1.3640\n",
      "Epoch [6/75], Loss: 1.0350\n",
      "Epoch [6/75], Val Loss: 1.2688\n",
      "Epoch [7/75], Loss: 0.8066\n",
      "Epoch [7/75], Val Loss: 1.2457\n",
      "Epoch [8/75], Loss: 0.6561\n",
      "Epoch [8/75], Val Loss: 1.1684\n",
      "Epoch [9/75], Loss: 0.5620\n",
      "Epoch [9/75], Val Loss: 1.1507\n",
      "Epoch [10/75], Loss: 0.5037\n",
      "Epoch [10/75], Val Loss: 1.1405\n",
      "Epoch [11/75], Loss: 0.4063\n",
      "Epoch [11/75], Val Loss: 1.1151\n",
      "Epoch [12/75], Loss: 0.3244\n",
      "Epoch [12/75], Val Loss: 1.0835\n",
      "Epoch [13/75], Loss: 0.2413\n",
      "Epoch [13/75], Val Loss: 1.1291\n",
      "Epoch [14/75], Loss: 0.1821\n",
      "Epoch [14/75], Val Loss: 1.1465\n",
      "Epoch [15/75], Loss: 0.1478\n",
      "Epoch [15/75], Val Loss: 1.2475\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [15/75], Loss: 0.1478\n",
      "Test Accuracy CNN Lipschitz: 68.71%\n",
      "Epoch [1/75], Loss: 3.2715\n",
      "Epoch [1/75], Val Loss: 3.1361\n",
      "Epoch [2/75], Loss: 2.9098\n",
      "Epoch [2/75], Val Loss: 2.6066\n",
      "Epoch [3/75], Loss: 2.1224\n",
      "Epoch [3/75], Val Loss: 1.9709\n",
      "Epoch [4/75], Loss: 1.5864\n",
      "Epoch [4/75], Val Loss: 1.9294\n",
      "Epoch [5/75], Loss: 1.2697\n",
      "Epoch [5/75], Val Loss: 1.4248\n",
      "Epoch [6/75], Loss: 0.9897\n",
      "Epoch [6/75], Val Loss: 1.3457\n",
      "Epoch [7/75], Loss: 0.8202\n",
      "Epoch [7/75], Val Loss: 1.2044\n",
      "Epoch [8/75], Loss: 0.6512\n",
      "Epoch [8/75], Val Loss: 1.2294\n",
      "Epoch [9/75], Loss: 0.5617\n",
      "Epoch [9/75], Val Loss: 1.1857\n",
      "Epoch [10/75], Loss: 0.5026\n",
      "Epoch [10/75], Val Loss: 1.1650\n",
      "Epoch [11/75], Loss: 0.3839\n",
      "Epoch [11/75], Val Loss: 1.1333\n",
      "Epoch [12/75], Loss: 0.2822\n",
      "Epoch [12/75], Val Loss: 1.1441\n",
      "Epoch [13/75], Loss: 0.2211\n",
      "Epoch [13/75], Val Loss: 1.1618\n",
      "Epoch [14/75], Loss: 0.1878\n",
      "Epoch [14/75], Val Loss: 1.2421\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [14/75], Loss: 0.1878\n",
      "Test Accuracy CNN Lipschitz: 69.14%\n",
      "Epoch [1/75], Loss: 3.2804\n",
      "Epoch [1/75], Val Loss: 3.1504\n",
      "Epoch [2/75], Loss: 2.9703\n",
      "Epoch [2/75], Val Loss: 2.7387\n",
      "Epoch [3/75], Loss: 2.2913\n",
      "Epoch [3/75], Val Loss: 1.9142\n",
      "Epoch [4/75], Loss: 1.8022\n",
      "Epoch [4/75], Val Loss: 1.7572\n",
      "Epoch [5/75], Loss: 1.5052\n",
      "Epoch [5/75], Val Loss: 1.5809\n",
      "Epoch [6/75], Loss: 1.2486\n",
      "Epoch [6/75], Val Loss: 1.4020\n",
      "Epoch [7/75], Loss: 0.9493\n",
      "Epoch [7/75], Val Loss: 1.2517\n",
      "Epoch [8/75], Loss: 0.7610\n",
      "Epoch [8/75], Val Loss: 1.2279\n",
      "Epoch [9/75], Loss: 0.6401\n",
      "Epoch [9/75], Val Loss: 1.0636\n",
      "Epoch [10/75], Loss: 0.4977\n",
      "Epoch [10/75], Val Loss: 1.0865\n",
      "Epoch [11/75], Loss: 0.4087\n",
      "Epoch [11/75], Val Loss: 1.1035\n",
      "Epoch [12/75], Loss: 0.3594\n",
      "Epoch [12/75], Val Loss: 1.1718\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [12/75], Loss: 0.3594\n",
      "Test Accuracy CNN Lipschitz: 68.65%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 75\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "logit_accuracy_cnn_lipschitz = {}\n",
    "\n",
    "for subset_size in num_subsets:\n",
    "    accuracy_list = []\n",
    "    print(f\"Training with subset size: {subset_size}\")\n",
    "\n",
    "    for _ in range(5):\n",
    "        cnn_model_lipschitz = EMNIST_CNN(num_classes).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_sdg = optim.SGD(cnn_model_lipschitz.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "\n",
    "        def init_weights(m):\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)  # Set all weights to zero\n",
    "                    m.bias.data.fill_(0.01)\n",
    "        cnn_model_lipschitz.apply(init_weights)\n",
    "\n",
    "        subset_indices = torch.randperm(train_data.shape[0])[:subset_size]\n",
    "        subset = SubsetRandomSampler(subset_indices)\n",
    "        train_loader = DataLoader(train_set, batch_size=128, sampler=subset)\n",
    "\n",
    "        best_val_loss = float('inf')  # Track the best loss\n",
    "        epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            cnn_model_lipschitz.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = cnn_model_lipschitz(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer_sdg.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "\n",
    "                L = max(param.grad.abs().max().item() for param in cnn_model_lipschitz.parameters() if param.grad is not None)\n",
    "                adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "                # Update optimizer's learning rate\n",
    "                for param_group in optimizer_sdg.param_groups:\n",
    "                    param_group['lr'] = adaptive_lr\n",
    "\n",
    "                optimizer_sdg.step()\n",
    "\n",
    "                total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "            current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "           # Validation loop\n",
    "            cnn_model_lipschitz.eval()\n",
    "            val_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader_cnn:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                    outputs = cnn_model_lipschitz(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader_cnn)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "                    break\n",
    "\n",
    "        print(f\"Subset {subset_size}, Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        cnn_model_lipschitz.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = cnn_model_lipschitz(images)\n",
    "                _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Test Accuracy CNN Lipschitz: {accuracy:.2f}%\")\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    logit_accuracy_cnn_lipschitz[subset_size] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000    92.687500\n",
       " 50000    92.315385\n",
       " 10000    88.067308\n",
       " 5000     85.725000\n",
       " 1000     69.442308\n",
       " dtype: float64,\n",
       " 75000    0.298406\n",
       " 50000    0.270165\n",
       " 10000    0.528901\n",
       " 5000     0.566568\n",
       " 1000     0.892012\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_accuracy_cnn_lipschitz_df = pd.DataFrame(logit_accuracy_cnn_lipschitz)\n",
    "logit_accuracy_cnn_lipschitz_df.mean(), logit_accuracy_cnn_lipschitz_df.std(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans(Dataset):\n",
    "    def __init__(self, centroids, labels):\n",
    "        \"\"\"\n",
    "        centroids: torch.Tensor or numpy array of shape (N, D) where N is the number of samples, D is the feature size.\n",
    "        labels: torch.Tensor or numpy array of shape (N,) containing class labels.\n",
    "        \"\"\"\n",
    "        self.centroids = centroids.clone().detach().float()\n",
    "        self.labels = labels.clone().detach().long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centroids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.centroids[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 175.4606\n",
      "Epoch [2/100], Loss: 158.9786\n",
      "Epoch [3/100], Loss: 156.4019\n",
      "Epoch [4/100], Loss: 155.2115\n",
      "Epoch [5/100], Loss: 154.2610\n",
      "Epoch [6/100], Loss: 153.4348\n",
      "Epoch [7/100], Loss: 153.0185\n",
      "Epoch [8/100], Loss: 152.6335\n",
      "Epoch [9/100], Loss: 152.0356\n",
      "Epoch [10/100], Loss: 151.7344\n",
      "Epoch [11/100], Loss: 151.5714\n",
      "Epoch [12/100], Loss: 150.6712\n",
      "Epoch [13/100], Loss: 151.0839\n",
      "Epoch [14/100], Loss: 150.6006\n",
      "Epoch [15/100], Loss: 150.1626\n",
      "Epoch [16/100], Loss: 149.9008\n",
      "Epoch [17/100], Loss: 150.2816\n",
      "Epoch [18/100], Loss: 149.7011\n",
      "Epoch [19/100], Loss: 149.4352\n",
      "Epoch [20/100], Loss: 149.3675\n",
      "Epoch [21/100], Loss: 149.4079\n",
      "Epoch [22/100], Loss: 149.1935\n",
      "Epoch [23/100], Loss: 148.8121\n",
      "Epoch [24/100], Loss: 148.7812\n",
      "Epoch [25/100], Loss: 148.8039\n",
      "Epoch [26/100], Loss: 148.6196\n",
      "Epoch [27/100], Loss: 148.8564\n",
      "Epoch [28/100], Loss: 148.4779\n",
      "Epoch [29/100], Loss: 148.5490\n",
      "Epoch [30/100], Loss: 148.5787\n",
      "Epoch [31/100], Loss: 148.5956\n",
      "Stopping early at epoch 31 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Base Logit K Means: 69.46%\n",
      "Epoch [1/100], Loss: 121.2048\n",
      "Epoch [2/100], Loss: 106.6236\n",
      "Epoch [3/100], Loss: 104.1628\n",
      "Epoch [4/100], Loss: 102.9018\n",
      "Epoch [5/100], Loss: 102.2278\n",
      "Epoch [6/100], Loss: 101.6111\n",
      "Epoch [7/100], Loss: 100.9835\n",
      "Epoch [8/100], Loss: 100.3985\n",
      "Epoch [9/100], Loss: 100.0811\n",
      "Epoch [10/100], Loss: 99.9867\n",
      "Epoch [11/100], Loss: 99.6988\n",
      "Epoch [12/100], Loss: 99.1106\n",
      "Epoch [13/100], Loss: 98.8430\n",
      "Epoch [14/100], Loss: 98.7612\n",
      "Epoch [15/100], Loss: 98.4477\n",
      "Epoch [16/100], Loss: 98.4176\n",
      "Epoch [17/100], Loss: 98.1846\n",
      "Epoch [18/100], Loss: 98.2690\n",
      "Epoch [19/100], Loss: 97.8946\n",
      "Epoch [20/100], Loss: 97.8834\n",
      "Epoch [21/100], Loss: 97.7701\n",
      "Epoch [22/100], Loss: 97.7608\n",
      "Epoch [23/100], Loss: 97.3312\n",
      "Epoch [24/100], Loss: 97.3840\n",
      "Epoch [25/100], Loss: 97.2651\n",
      "Epoch [26/100], Loss: 97.0385\n",
      "Epoch [27/100], Loss: 96.7432\n",
      "Epoch [28/100], Loss: 96.6729\n",
      "Epoch [29/100], Loss: 96.6914\n",
      "Epoch [30/100], Loss: 96.7511\n",
      "Epoch [31/100], Loss: 96.6749\n",
      "Epoch [32/100], Loss: 96.2098\n",
      "Epoch [33/100], Loss: 96.6210\n",
      "Epoch [34/100], Loss: 96.3072\n",
      "Epoch [35/100], Loss: 96.1842\n",
      "Epoch [36/100], Loss: 95.8686\n",
      "Epoch [37/100], Loss: 96.1538\n",
      "Epoch [38/100], Loss: 95.9489\n",
      "Epoch [39/100], Loss: 95.6969\n",
      "Epoch [40/100], Loss: 95.7312\n",
      "Epoch [41/100], Loss: 95.7618\n",
      "Epoch [42/100], Loss: 95.4176\n",
      "Epoch [43/100], Loss: 95.8159\n",
      "Epoch [44/100], Loss: 95.3388\n",
      "Epoch [45/100], Loss: 95.3501\n",
      "Epoch [46/100], Loss: 95.4767\n",
      "Epoch [47/100], Loss: 95.2194\n",
      "Epoch [48/100], Loss: 95.2748\n",
      "Epoch [49/100], Loss: 95.1065\n",
      "Epoch [50/100], Loss: 95.2739\n",
      "Epoch [51/100], Loss: 95.3620\n",
      "Epoch [52/100], Loss: 95.2172\n",
      "Epoch [53/100], Loss: 95.0857\n",
      "Epoch [54/100], Loss: 94.8820\n",
      "Epoch [55/100], Loss: 94.9243\n",
      "Epoch [56/100], Loss: 94.7713\n",
      "Epoch [57/100], Loss: 94.8409\n",
      "Epoch [58/100], Loss: 94.8502\n",
      "Epoch [59/100], Loss: 94.8105\n",
      "Epoch [60/100], Loss: 94.8421\n",
      "Epoch [61/100], Loss: 94.9186\n",
      "Epoch [62/100], Loss: 94.4927\n",
      "Epoch [63/100], Loss: 94.1438\n",
      "Epoch [64/100], Loss: 94.3762\n",
      "Epoch [65/100], Loss: 94.6065\n",
      "Epoch [66/100], Loss: 94.4247\n",
      "Epoch [67/100], Loss: 94.5378\n",
      "Epoch [68/100], Loss: 94.2698\n",
      "Epoch [69/100], Loss: 94.3839\n",
      "Epoch [70/100], Loss: 94.5795\n",
      "Epoch [71/100], Loss: 94.4615\n",
      "Epoch [72/100], Loss: 94.1959\n",
      "Epoch [73/100], Loss: 94.2446\n",
      "Epoch [74/100], Loss: 94.0644\n",
      "Epoch [75/100], Loss: 94.3262\n",
      "Epoch [76/100], Loss: 94.1385\n",
      "Epoch [77/100], Loss: 93.9647\n",
      "Epoch [78/100], Loss: 94.3753\n",
      "Epoch [79/100], Loss: 93.9312\n",
      "Epoch [80/100], Loss: 94.0483\n",
      "Epoch [81/100], Loss: 94.0347\n",
      "Epoch [82/100], Loss: 94.0885\n",
      "Epoch [83/100], Loss: 94.0421\n",
      "Epoch [84/100], Loss: 93.6677\n",
      "Epoch [85/100], Loss: 93.8822\n",
      "Epoch [86/100], Loss: 93.5978\n",
      "Epoch [87/100], Loss: 94.1506\n",
      "Epoch [88/100], Loss: 93.9510\n",
      "Epoch [89/100], Loss: 93.8951\n",
      "Epoch [90/100], Loss: 93.6110\n",
      "Epoch [91/100], Loss: 93.8096\n",
      "Epoch [92/100], Loss: 93.7718\n",
      "Epoch [93/100], Loss: 93.8740\n",
      "Epoch [94/100], Loss: 93.7752\n",
      "Epoch [95/100], Loss: 93.5658\n",
      "Epoch [96/100], Loss: 93.6774\n",
      "Epoch [97/100], Loss: 93.7526\n",
      "Epoch [98/100], Loss: 93.6827\n",
      "Epoch [99/100], Loss: 93.6310\n",
      "Epoch [100/100], Loss: 93.2773\n",
      "Test Accuracy Base Logit K Means: 67.89%\n",
      "Epoch [1/100], Loss: 30.0688\n",
      "Epoch [2/100], Loss: 20.5317\n",
      "Epoch [3/100], Loss: 19.1188\n",
      "Epoch [4/100], Loss: 18.3951\n",
      "Epoch [5/100], Loss: 17.7621\n",
      "Epoch [6/100], Loss: 17.4458\n",
      "Epoch [7/100], Loss: 16.9153\n",
      "Epoch [8/100], Loss: 16.6629\n",
      "Epoch [9/100], Loss: 16.4184\n",
      "Epoch [10/100], Loss: 16.1260\n",
      "Epoch [11/100], Loss: 15.9602\n",
      "Epoch [12/100], Loss: 15.8520\n",
      "Epoch [13/100], Loss: 15.7394\n",
      "Epoch [14/100], Loss: 15.5360\n",
      "Epoch [15/100], Loss: 15.4526\n",
      "Epoch [16/100], Loss: 15.1932\n",
      "Epoch [17/100], Loss: 15.0943\n",
      "Epoch [18/100], Loss: 15.0637\n",
      "Epoch [19/100], Loss: 15.0001\n",
      "Epoch [20/100], Loss: 14.8108\n",
      "Epoch [21/100], Loss: 14.7738\n",
      "Epoch [22/100], Loss: 14.5938\n",
      "Epoch [23/100], Loss: 14.4896\n",
      "Epoch [24/100], Loss: 14.4009\n",
      "Epoch [25/100], Loss: 14.3562\n",
      "Epoch [26/100], Loss: 14.3883\n",
      "Epoch [27/100], Loss: 14.4467\n",
      "Epoch [28/100], Loss: 14.2249\n",
      "Epoch [29/100], Loss: 14.1994\n",
      "Epoch [30/100], Loss: 14.0161\n",
      "Epoch [31/100], Loss: 13.9255\n",
      "Epoch [32/100], Loss: 13.8594\n",
      "Epoch [33/100], Loss: 13.8539\n",
      "Epoch [34/100], Loss: 13.7900\n",
      "Epoch [35/100], Loss: 13.7441\n",
      "Epoch [36/100], Loss: 13.8687\n",
      "Epoch [37/100], Loss: 13.7082\n",
      "Epoch [38/100], Loss: 13.6007\n",
      "Epoch [39/100], Loss: 13.7608\n",
      "Epoch [40/100], Loss: 13.5828\n",
      "Epoch [41/100], Loss: 13.4703\n",
      "Epoch [42/100], Loss: 13.4823\n",
      "Epoch [43/100], Loss: 13.3959\n",
      "Epoch [44/100], Loss: 13.4484\n",
      "Epoch [45/100], Loss: 13.2834\n",
      "Epoch [46/100], Loss: 13.3060\n",
      "Epoch [47/100], Loss: 13.1002\n",
      "Epoch [48/100], Loss: 13.1051\n",
      "Epoch [49/100], Loss: 13.0906\n",
      "Epoch [50/100], Loss: 13.1251\n",
      "Epoch [51/100], Loss: 13.0585\n",
      "Epoch [52/100], Loss: 12.9194\n",
      "Epoch [53/100], Loss: 12.8705\n",
      "Epoch [54/100], Loss: 13.0454\n",
      "Epoch [55/100], Loss: 13.0810\n",
      "Epoch [56/100], Loss: 12.8709\n",
      "Epoch [57/100], Loss: 12.8426\n",
      "Epoch [58/100], Loss: 12.8494\n",
      "Epoch [59/100], Loss: 12.6686\n",
      "Epoch [60/100], Loss: 12.6691\n",
      "Epoch [61/100], Loss: 12.7027\n",
      "Epoch [62/100], Loss: 12.8174\n",
      "Stopping early at epoch 62 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Base Logit K Means: 65.20%\n",
      "Epoch [1/100], Loss: 17.8023\n",
      "Epoch [2/100], Loss: 10.2566\n",
      "Epoch [3/100], Loss: 9.0319\n",
      "Epoch [4/100], Loss: 8.4150\n",
      "Epoch [5/100], Loss: 8.0643\n",
      "Epoch [6/100], Loss: 7.8879\n",
      "Epoch [7/100], Loss: 7.5755\n",
      "Epoch [8/100], Loss: 7.3623\n",
      "Epoch [9/100], Loss: 7.0680\n",
      "Epoch [10/100], Loss: 6.9351\n",
      "Epoch [11/100], Loss: 6.9512\n",
      "Epoch [12/100], Loss: 6.8182\n",
      "Epoch [13/100], Loss: 6.7616\n",
      "Epoch [14/100], Loss: 6.6275\n",
      "Epoch [15/100], Loss: 6.6037\n",
      "Epoch [16/100], Loss: 6.5506\n",
      "Epoch [17/100], Loss: 6.5004\n",
      "Epoch [18/100], Loss: 6.5729\n",
      "Epoch [19/100], Loss: 6.3034\n",
      "Epoch [20/100], Loss: 6.0198\n",
      "Epoch [21/100], Loss: 6.1392\n",
      "Epoch [22/100], Loss: 5.9880\n",
      "Epoch [23/100], Loss: 5.8978\n",
      "Epoch [24/100], Loss: 5.9818\n",
      "Epoch [25/100], Loss: 5.7413\n",
      "Epoch [26/100], Loss: 5.8904\n",
      "Epoch [27/100], Loss: 5.9573\n",
      "Epoch [28/100], Loss: 5.8030\n",
      "Epoch [29/100], Loss: 5.8547\n",
      "Epoch [30/100], Loss: 5.5166\n",
      "Epoch [31/100], Loss: 5.5470\n",
      "Epoch [32/100], Loss: 5.6194\n",
      "Epoch [33/100], Loss: 5.5425\n",
      "Epoch [34/100], Loss: 5.4652\n",
      "Epoch [35/100], Loss: 5.3477\n",
      "Epoch [36/100], Loss: 5.2538\n",
      "Epoch [37/100], Loss: 5.2199\n",
      "Epoch [38/100], Loss: 5.2649\n",
      "Epoch [39/100], Loss: 5.2093\n",
      "Epoch [40/100], Loss: 5.2406\n",
      "Epoch [41/100], Loss: 5.3391\n",
      "Epoch [42/100], Loss: 5.2608\n",
      "Epoch [43/100], Loss: 5.0957\n",
      "Epoch [44/100], Loss: 5.1053\n",
      "Epoch [45/100], Loss: 5.2286\n",
      "Epoch [46/100], Loss: 5.0763\n",
      "Epoch [47/100], Loss: 4.9739\n",
      "Epoch [48/100], Loss: 4.9965\n",
      "Epoch [49/100], Loss: 5.0230\n",
      "Epoch [50/100], Loss: 5.0926\n",
      "Stopping early at epoch 50 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Base Logit K Means: 64.96%\n",
      "Epoch [1/100], Loss: 5.8706\n",
      "Epoch [2/100], Loss: 3.5665\n",
      "Epoch [3/100], Loss: 2.3533\n",
      "Epoch [4/100], Loss: 1.8903\n",
      "Epoch [5/100], Loss: 1.6331\n",
      "Epoch [6/100], Loss: 1.4730\n",
      "Epoch [7/100], Loss: 1.3507\n",
      "Epoch [8/100], Loss: 1.2657\n",
      "Epoch [9/100], Loss: 1.1959\n",
      "Epoch [10/100], Loss: 1.1393\n",
      "Epoch [11/100], Loss: 1.0945\n",
      "Epoch [12/100], Loss: 1.0490\n",
      "Epoch [13/100], Loss: 0.9977\n",
      "Epoch [14/100], Loss: 0.9687\n",
      "Epoch [15/100], Loss: 0.9301\n",
      "Epoch [16/100], Loss: 0.8998\n",
      "Epoch [17/100], Loss: 0.8754\n",
      "Epoch [18/100], Loss: 0.8485\n",
      "Epoch [19/100], Loss: 0.8273\n",
      "Epoch [20/100], Loss: 0.8107\n",
      "Epoch [21/100], Loss: 0.7905\n",
      "Epoch [22/100], Loss: 0.7786\n",
      "Epoch [23/100], Loss: 0.7496\n",
      "Epoch [24/100], Loss: 0.7386\n",
      "Epoch [25/100], Loss: 0.7199\n",
      "Epoch [26/100], Loss: 0.7056\n",
      "Epoch [27/100], Loss: 0.6913\n",
      "Epoch [28/100], Loss: 0.6850\n",
      "Epoch [29/100], Loss: 0.6664\n",
      "Epoch [30/100], Loss: 0.6575\n",
      "Epoch [31/100], Loss: 0.6438\n",
      "Epoch [32/100], Loss: 0.6322\n",
      "Epoch [33/100], Loss: 0.6197\n",
      "Epoch [34/100], Loss: 0.6167\n",
      "Epoch [35/100], Loss: 0.6024\n",
      "Epoch [36/100], Loss: 0.5947\n",
      "Epoch [37/100], Loss: 0.5852\n",
      "Epoch [38/100], Loss: 0.5820\n",
      "Epoch [39/100], Loss: 0.5690\n",
      "Epoch [40/100], Loss: 0.5607\n",
      "Epoch [41/100], Loss: 0.5545\n",
      "Epoch [42/100], Loss: 0.5414\n",
      "Epoch [43/100], Loss: 0.5364\n",
      "Epoch [44/100], Loss: 0.5282\n",
      "Epoch [45/100], Loss: 0.5271\n",
      "Epoch [46/100], Loss: 0.5136\n",
      "Epoch [47/100], Loss: 0.5071\n",
      "Epoch [48/100], Loss: 0.5036\n",
      "Epoch [49/100], Loss: 0.4987\n",
      "Epoch [50/100], Loss: 0.4886\n",
      "Epoch [51/100], Loss: 0.4827\n",
      "Epoch [52/100], Loss: 0.4797\n",
      "Epoch [53/100], Loss: 0.4776\n",
      "Epoch [54/100], Loss: 0.4705\n",
      "Epoch [55/100], Loss: 0.4634\n",
      "Epoch [56/100], Loss: 0.4622\n",
      "Epoch [57/100], Loss: 0.4539\n",
      "Epoch [58/100], Loss: 0.4481\n",
      "Epoch [59/100], Loss: 0.4503\n",
      "Epoch [60/100], Loss: 0.4388\n",
      "Epoch [61/100], Loss: 0.4327\n",
      "Epoch [62/100], Loss: 0.4286\n",
      "Epoch [63/100], Loss: 0.4252\n",
      "Epoch [64/100], Loss: 0.4204\n",
      "Epoch [65/100], Loss: 0.4143\n",
      "Epoch [66/100], Loss: 0.4139\n",
      "Epoch [67/100], Loss: 0.4087\n",
      "Epoch [68/100], Loss: 0.4029\n",
      "Epoch [69/100], Loss: 0.4034\n",
      "Epoch [70/100], Loss: 0.3977\n",
      "Epoch [71/100], Loss: 0.3970\n",
      "Epoch [72/100], Loss: 0.3909\n",
      "Epoch [73/100], Loss: 0.3902\n",
      "Epoch [74/100], Loss: 0.3816\n",
      "Epoch [75/100], Loss: 0.3795\n",
      "Epoch [76/100], Loss: 0.3742\n",
      "Epoch [77/100], Loss: 0.3756\n",
      "Epoch [78/100], Loss: 0.3733\n",
      "Epoch [79/100], Loss: 0.3662\n",
      "Epoch [80/100], Loss: 0.3670\n",
      "Epoch [81/100], Loss: 0.3625\n",
      "Epoch [82/100], Loss: 0.3590\n",
      "Epoch [83/100], Loss: 0.3559\n",
      "Epoch [84/100], Loss: 0.3523\n",
      "Epoch [85/100], Loss: 0.3486\n",
      "Epoch [86/100], Loss: 0.3497\n",
      "Epoch [87/100], Loss: 0.3428\n",
      "Epoch [88/100], Loss: 0.3411\n",
      "Epoch [89/100], Loss: 0.3426\n",
      "Epoch [90/100], Loss: 0.3371\n",
      "Epoch [91/100], Loss: 0.3334\n",
      "Epoch [92/100], Loss: 0.3287\n",
      "Epoch [93/100], Loss: 0.3260\n",
      "Epoch [94/100], Loss: 0.3274\n",
      "Epoch [95/100], Loss: 0.3262\n",
      "Epoch [96/100], Loss: 0.3244\n",
      "Epoch [97/100], Loss: 0.3206\n",
      "Epoch [98/100], Loss: 0.3182\n",
      "Epoch [99/100], Loss: 0.3163\n",
      "Epoch [100/100], Loss: 0.3112\n",
      "Test Accuracy Base Logit K Means: 63.06%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "base_logit_kmeans = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    logit_model = LogisticRegression(input_dim, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(logit_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "    previous_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logit_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = logit_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if previous_loss - current_loss < tolerance:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "                break\n",
    "        else:\n",
    "            epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "        previous_loss = current_loss\n",
    "\n",
    "    logit_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 28*28)\n",
    "            outputs = logit_model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy Base Logit K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    base_logit_kmeans[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.461538</td>\n",
       "      <td>67.889423</td>\n",
       "      <td>65.197115</td>\n",
       "      <td>64.961538</td>\n",
       "      <td>63.0625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       75000      50000      10000      5000     1000 \n",
       "0  69.461538  67.889423  65.197115  64.961538  63.0625"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_logit_kmeans_df = pd.DataFrame(base_logit_kmeans)\n",
    "base_logit_kmeans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipschitz Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 174.7797\n",
      "Epoch [2/100], Loss: 158.9948\n",
      "Epoch [3/100], Loss: 156.4363\n",
      "Epoch [4/100], Loss: 155.3280\n",
      "Epoch [5/100], Loss: 154.2844\n",
      "Epoch [6/100], Loss: 153.4449\n",
      "Epoch [7/100], Loss: 152.8901\n",
      "Epoch [8/100], Loss: 152.3302\n",
      "Epoch [9/100], Loss: 152.0780\n",
      "Epoch [10/100], Loss: 151.5151\n",
      "Epoch [11/100], Loss: 151.4044\n",
      "Epoch [12/100], Loss: 151.1122\n",
      "Epoch [13/100], Loss: 150.9331\n",
      "Epoch [14/100], Loss: 150.2972\n",
      "Epoch [15/100], Loss: 150.3928\n",
      "Epoch [16/100], Loss: 150.3375\n",
      "Epoch [17/100], Loss: 150.0230\n",
      "Epoch [18/100], Loss: 149.4260\n",
      "Epoch [19/100], Loss: 149.8679\n",
      "Epoch [20/100], Loss: 149.2008\n",
      "Epoch [21/100], Loss: 149.7504\n",
      "Epoch [22/100], Loss: 149.3839\n",
      "Epoch [23/100], Loss: 149.1636\n",
      "Epoch [24/100], Loss: 149.1011\n",
      "Epoch [25/100], Loss: 149.1429\n",
      "Epoch [26/100], Loss: 148.6327\n",
      "Epoch [27/100], Loss: 148.9050\n",
      "Epoch [28/100], Loss: 148.3092\n",
      "Epoch [29/100], Loss: 148.3335\n",
      "Epoch [30/100], Loss: 148.4233\n",
      "Epoch [31/100], Loss: 148.7612\n",
      "Stopping early at epoch 31 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Lipschitz Logit K Means: 69.41%\n",
      "Epoch [1/100], Loss: 120.9242\n",
      "Epoch [2/100], Loss: 106.2355\n",
      "Epoch [3/100], Loss: 104.4279\n",
      "Epoch [4/100], Loss: 102.9723\n",
      "Epoch [5/100], Loss: 102.3947\n",
      "Epoch [6/100], Loss: 101.7374\n",
      "Epoch [7/100], Loss: 100.8850\n",
      "Epoch [8/100], Loss: 100.6141\n",
      "Epoch [9/100], Loss: 100.3017\n",
      "Epoch [10/100], Loss: 100.2366\n",
      "Epoch [11/100], Loss: 99.5111\n",
      "Epoch [12/100], Loss: 99.2689\n",
      "Epoch [13/100], Loss: 98.7988\n",
      "Epoch [14/100], Loss: 98.8263\n",
      "Epoch [15/100], Loss: 98.5736\n",
      "Epoch [16/100], Loss: 98.8779\n",
      "Epoch [17/100], Loss: 98.0306\n",
      "Epoch [18/100], Loss: 97.9374\n",
      "Epoch [19/100], Loss: 97.8743\n",
      "Epoch [20/100], Loss: 97.7601\n",
      "Epoch [21/100], Loss: 97.4635\n",
      "Epoch [22/100], Loss: 97.2938\n",
      "Epoch [23/100], Loss: 97.4197\n",
      "Epoch [24/100], Loss: 97.1681\n",
      "Epoch [25/100], Loss: 97.3970\n",
      "Epoch [26/100], Loss: 97.0500\n",
      "Epoch [27/100], Loss: 96.8637\n",
      "Epoch [28/100], Loss: 96.8633\n",
      "Epoch [29/100], Loss: 96.6718\n",
      "Epoch [30/100], Loss: 96.4185\n",
      "Epoch [31/100], Loss: 96.2934\n",
      "Epoch [32/100], Loss: 96.3742\n",
      "Epoch [33/100], Loss: 96.4072\n",
      "Epoch [34/100], Loss: 96.1355\n",
      "Epoch [35/100], Loss: 95.9351\n",
      "Epoch [36/100], Loss: 96.0182\n",
      "Epoch [37/100], Loss: 95.8465\n",
      "Epoch [38/100], Loss: 96.2839\n",
      "Epoch [39/100], Loss: 95.9689\n",
      "Epoch [40/100], Loss: 95.9976\n",
      "Epoch [41/100], Loss: 95.7066\n",
      "Epoch [42/100], Loss: 95.4776\n",
      "Epoch [43/100], Loss: 95.3630\n",
      "Epoch [44/100], Loss: 95.2750\n",
      "Epoch [45/100], Loss: 95.6250\n",
      "Epoch [46/100], Loss: 95.4836\n",
      "Epoch [47/100], Loss: 95.1569\n",
      "Epoch [48/100], Loss: 95.2703\n",
      "Epoch [49/100], Loss: 95.1601\n",
      "Epoch [50/100], Loss: 95.1672\n",
      "Epoch [51/100], Loss: 95.0050\n",
      "Epoch [52/100], Loss: 95.1294\n",
      "Epoch [53/100], Loss: 95.0838\n",
      "Epoch [54/100], Loss: 94.9724\n",
      "Epoch [55/100], Loss: 95.1881\n",
      "Epoch [56/100], Loss: 94.6699\n",
      "Epoch [57/100], Loss: 95.2084\n",
      "Epoch [58/100], Loss: 94.8855\n",
      "Epoch [59/100], Loss: 94.8306\n",
      "Epoch [60/100], Loss: 94.7683\n",
      "Epoch [61/100], Loss: 94.8180\n",
      "Epoch [62/100], Loss: 94.5767\n",
      "Epoch [63/100], Loss: 94.7403\n",
      "Epoch [64/100], Loss: 94.2910\n",
      "Epoch [65/100], Loss: 94.5698\n",
      "Epoch [66/100], Loss: 94.4308\n",
      "Epoch [67/100], Loss: 94.2636\n",
      "Epoch [68/100], Loss: 94.2237\n",
      "Epoch [69/100], Loss: 94.4411\n",
      "Epoch [70/100], Loss: 94.5186\n",
      "Epoch [71/100], Loss: 94.2757\n",
      "Epoch [72/100], Loss: 94.0821\n",
      "Epoch [73/100], Loss: 94.5829\n",
      "Epoch [74/100], Loss: 94.3947\n",
      "Epoch [75/100], Loss: 93.7149\n",
      "Epoch [76/100], Loss: 94.2138\n",
      "Epoch [77/100], Loss: 93.7268\n",
      "Epoch [78/100], Loss: 93.8639\n",
      "Epoch [79/100], Loss: 93.9922\n",
      "Epoch [80/100], Loss: 94.1043\n",
      "Stopping early at epoch 80 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Lipschitz Logit K Means: 68.55%\n",
      "Epoch [1/100], Loss: 29.7088\n",
      "Epoch [2/100], Loss: 20.3822\n",
      "Epoch [3/100], Loss: 19.2759\n",
      "Epoch [4/100], Loss: 18.3297\n",
      "Epoch [5/100], Loss: 17.7561\n",
      "Epoch [6/100], Loss: 17.2910\n",
      "Epoch [7/100], Loss: 16.9710\n",
      "Epoch [8/100], Loss: 16.5439\n",
      "Epoch [9/100], Loss: 16.2671\n",
      "Epoch [10/100], Loss: 16.2236\n",
      "Epoch [11/100], Loss: 16.0861\n",
      "Epoch [12/100], Loss: 15.9566\n",
      "Epoch [13/100], Loss: 15.6121\n",
      "Epoch [14/100], Loss: 15.5416\n",
      "Epoch [15/100], Loss: 15.4860\n",
      "Epoch [16/100], Loss: 15.5370\n",
      "Epoch [17/100], Loss: 15.2141\n",
      "Epoch [18/100], Loss: 15.0947\n",
      "Epoch [19/100], Loss: 15.0145\n",
      "Epoch [20/100], Loss: 14.8687\n",
      "Epoch [21/100], Loss: 14.8067\n",
      "Epoch [22/100], Loss: 14.7883\n",
      "Epoch [23/100], Loss: 14.6593\n",
      "Epoch [24/100], Loss: 14.6173\n",
      "Epoch [25/100], Loss: 14.3860\n",
      "Epoch [26/100], Loss: 14.2519\n",
      "Epoch [27/100], Loss: 14.2191\n",
      "Epoch [28/100], Loss: 14.2961\n",
      "Epoch [29/100], Loss: 14.1807\n",
      "Epoch [30/100], Loss: 14.0992\n",
      "Epoch [31/100], Loss: 14.0277\n",
      "Epoch [32/100], Loss: 14.0625\n",
      "Epoch [33/100], Loss: 14.0814\n",
      "Epoch [34/100], Loss: 13.9281\n",
      "Epoch [35/100], Loss: 13.6698\n",
      "Epoch [36/100], Loss: 13.6125\n",
      "Epoch [37/100], Loss: 13.6950\n",
      "Epoch [38/100], Loss: 13.5739\n",
      "Epoch [39/100], Loss: 13.5530\n",
      "Epoch [40/100], Loss: 13.5972\n",
      "Epoch [41/100], Loss: 13.4718\n",
      "Epoch [42/100], Loss: 13.2820\n",
      "Epoch [43/100], Loss: 13.4044\n",
      "Epoch [44/100], Loss: 13.3259\n",
      "Epoch [45/100], Loss: 13.4228\n",
      "Epoch [46/100], Loss: 13.2145\n",
      "Epoch [47/100], Loss: 13.1019\n",
      "Epoch [48/100], Loss: 13.1362\n",
      "Epoch [49/100], Loss: 13.1498\n",
      "Epoch [50/100], Loss: 13.3211\n",
      "Stopping early at epoch 50 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Lipschitz Logit K Means: 65.92%\n",
      "Epoch [1/100], Loss: 17.7899\n",
      "Epoch [2/100], Loss: 10.2299\n",
      "Epoch [3/100], Loss: 9.1737\n",
      "Epoch [4/100], Loss: 8.5578\n",
      "Epoch [5/100], Loss: 8.0596\n",
      "Epoch [6/100], Loss: 7.7611\n",
      "Epoch [7/100], Loss: 7.4068\n",
      "Epoch [8/100], Loss: 7.4857\n",
      "Epoch [9/100], Loss: 7.1122\n",
      "Epoch [10/100], Loss: 6.8915\n",
      "Epoch [11/100], Loss: 6.8507\n",
      "Epoch [12/100], Loss: 6.7225\n",
      "Epoch [13/100], Loss: 6.7583\n",
      "Epoch [14/100], Loss: 6.5604\n",
      "Epoch [15/100], Loss: 6.5745\n",
      "Epoch [16/100], Loss: 6.3471\n",
      "Epoch [17/100], Loss: 6.3090\n",
      "Epoch [18/100], Loss: 6.3066\n",
      "Epoch [19/100], Loss: 6.3097\n",
      "Epoch [20/100], Loss: 6.1035\n",
      "Epoch [21/100], Loss: 6.0533\n",
      "Epoch [22/100], Loss: 6.1723\n",
      "Epoch [23/100], Loss: 6.1760\n",
      "Epoch [24/100], Loss: 5.9566\n",
      "Epoch [25/100], Loss: 5.8514\n",
      "Epoch [26/100], Loss: 5.8690\n",
      "Epoch [27/100], Loss: 5.6536\n",
      "Epoch [28/100], Loss: 5.8039\n",
      "Epoch [29/100], Loss: 6.0965\n",
      "Epoch [30/100], Loss: 5.6405\n",
      "Epoch [31/100], Loss: 5.4986\n",
      "Epoch [32/100], Loss: 5.4972\n",
      "Epoch [33/100], Loss: 5.5038\n",
      "Epoch [34/100], Loss: 5.4047\n",
      "Epoch [35/100], Loss: 5.3679\n",
      "Epoch [36/100], Loss: 5.6453\n",
      "Epoch [37/100], Loss: 5.5095\n",
      "Epoch [38/100], Loss: 5.4495\n",
      "Epoch [39/100], Loss: 5.2357\n",
      "Epoch [40/100], Loss: 5.2275\n",
      "Epoch [41/100], Loss: 5.2317\n",
      "Epoch [42/100], Loss: 5.1360\n",
      "Epoch [43/100], Loss: 5.3674\n",
      "Epoch [44/100], Loss: 5.4289\n",
      "Epoch [45/100], Loss: 5.2755\n",
      "Epoch [46/100], Loss: 5.1038\n",
      "Epoch [47/100], Loss: 4.9509\n",
      "Epoch [48/100], Loss: 4.9996\n",
      "Epoch [49/100], Loss: 4.9637\n",
      "Epoch [50/100], Loss: 4.9051\n",
      "Epoch [51/100], Loss: 4.9365\n",
      "Epoch [52/100], Loss: 4.9449\n",
      "Epoch [53/100], Loss: 5.0772\n",
      "Stopping early at epoch 53 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Lipschitz Logit K Means: 64.72%\n",
      "Epoch [1/100], Loss: 5.8950\n",
      "Epoch [2/100], Loss: 3.5436\n",
      "Epoch [3/100], Loss: 2.3627\n",
      "Epoch [4/100], Loss: 1.8847\n",
      "Epoch [5/100], Loss: 1.6468\n",
      "Epoch [6/100], Loss: 1.4703\n",
      "Epoch [7/100], Loss: 1.3425\n",
      "Epoch [8/100], Loss: 1.2547\n",
      "Epoch [9/100], Loss: 1.1883\n",
      "Epoch [10/100], Loss: 1.1261\n",
      "Epoch [11/100], Loss: 1.0817\n",
      "Epoch [12/100], Loss: 1.0382\n",
      "Epoch [13/100], Loss: 0.9974\n",
      "Epoch [14/100], Loss: 0.9612\n",
      "Epoch [15/100], Loss: 0.9269\n",
      "Epoch [16/100], Loss: 0.9059\n",
      "Epoch [17/100], Loss: 0.8828\n",
      "Epoch [18/100], Loss: 0.8541\n",
      "Epoch [19/100], Loss: 0.8337\n",
      "Epoch [20/100], Loss: 0.8114\n",
      "Epoch [21/100], Loss: 0.7914\n",
      "Epoch [22/100], Loss: 0.7680\n",
      "Epoch [23/100], Loss: 0.7561\n",
      "Epoch [24/100], Loss: 0.7422\n",
      "Epoch [25/100], Loss: 0.7246\n",
      "Epoch [26/100], Loss: 0.7068\n",
      "Epoch [27/100], Loss: 0.7008\n",
      "Epoch [28/100], Loss: 0.6832\n",
      "Epoch [29/100], Loss: 0.6682\n",
      "Epoch [30/100], Loss: 0.6600\n",
      "Epoch [31/100], Loss: 0.6435\n",
      "Epoch [32/100], Loss: 0.6342\n",
      "Epoch [33/100], Loss: 0.6253\n",
      "Epoch [34/100], Loss: 0.6140\n",
      "Epoch [35/100], Loss: 0.6084\n",
      "Epoch [36/100], Loss: 0.5946\n",
      "Epoch [37/100], Loss: 0.5830\n",
      "Epoch [38/100], Loss: 0.5745\n",
      "Epoch [39/100], Loss: 0.5660\n",
      "Epoch [40/100], Loss: 0.5592\n",
      "Epoch [41/100], Loss: 0.5545\n",
      "Epoch [42/100], Loss: 0.5464\n",
      "Epoch [43/100], Loss: 0.5336\n",
      "Epoch [44/100], Loss: 0.5319\n",
      "Epoch [45/100], Loss: 0.5251\n",
      "Epoch [46/100], Loss: 0.5158\n",
      "Epoch [47/100], Loss: 0.5090\n",
      "Epoch [48/100], Loss: 0.5009\n",
      "Epoch [49/100], Loss: 0.5002\n",
      "Epoch [50/100], Loss: 0.4871\n",
      "Epoch [51/100], Loss: 0.4911\n",
      "Epoch [52/100], Loss: 0.4801\n",
      "Epoch [53/100], Loss: 0.4756\n",
      "Epoch [54/100], Loss: 0.4688\n",
      "Epoch [55/100], Loss: 0.4656\n",
      "Epoch [56/100], Loss: 0.4625\n",
      "Epoch [57/100], Loss: 0.4496\n",
      "Epoch [58/100], Loss: 0.4476\n",
      "Epoch [59/100], Loss: 0.4445\n",
      "Epoch [60/100], Loss: 0.4406\n",
      "Epoch [61/100], Loss: 0.4329\n",
      "Epoch [62/100], Loss: 0.4293\n",
      "Epoch [63/100], Loss: 0.4257\n",
      "Epoch [64/100], Loss: 0.4200\n",
      "Epoch [65/100], Loss: 0.4171\n",
      "Epoch [66/100], Loss: 0.4153\n",
      "Epoch [67/100], Loss: 0.4121\n",
      "Epoch [68/100], Loss: 0.4064\n",
      "Epoch [69/100], Loss: 0.4016\n",
      "Epoch [70/100], Loss: 0.3967\n",
      "Epoch [71/100], Loss: 0.3964\n",
      "Epoch [72/100], Loss: 0.3933\n",
      "Epoch [73/100], Loss: 0.3836\n",
      "Epoch [74/100], Loss: 0.3812\n",
      "Epoch [75/100], Loss: 0.3797\n",
      "Epoch [76/100], Loss: 0.3733\n",
      "Epoch [77/100], Loss: 0.3723\n",
      "Epoch [78/100], Loss: 0.3688\n",
      "Epoch [79/100], Loss: 0.3674\n",
      "Epoch [80/100], Loss: 0.3661\n",
      "Epoch [81/100], Loss: 0.3600\n",
      "Epoch [82/100], Loss: 0.3598\n",
      "Epoch [83/100], Loss: 0.3534\n",
      "Epoch [84/100], Loss: 0.3498\n",
      "Epoch [85/100], Loss: 0.3506\n",
      "Epoch [86/100], Loss: 0.3450\n",
      "Epoch [87/100], Loss: 0.3430\n",
      "Epoch [88/100], Loss: 0.3434\n",
      "Epoch [89/100], Loss: 0.3409\n",
      "Epoch [90/100], Loss: 0.3350\n",
      "Epoch [91/100], Loss: 0.3342\n",
      "Epoch [92/100], Loss: 0.3327\n",
      "Epoch [93/100], Loss: 0.3304\n",
      "Epoch [94/100], Loss: 0.3270\n",
      "Epoch [95/100], Loss: 0.3242\n",
      "Epoch [96/100], Loss: 0.3234\n",
      "Epoch [97/100], Loss: 0.3205\n",
      "Epoch [98/100], Loss: 0.3157\n",
      "Epoch [99/100], Loss: 0.3153\n",
      "Epoch [100/100], Loss: 0.3142\n",
      "Test Accuracy Lipschitz Logit K Means: 63.19%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "lipschitz_logit_kmeans = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    logit_model_lipschitz = LogisticRegression(input_dim, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(logit_model_lipschitz.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "    previous_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logit_model_lipschitz.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = logit_model_lipschitz(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            L = max(param.grad.abs().max().item() for param in logit_model_lipschitz.parameters() if param.grad is not None)\n",
    "            adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "            # Update optimizer's learning rate\n",
    "            for param_group in optimizer_sdg.param_groups:\n",
    "                param_group['lr'] = adaptive_lr\n",
    "\n",
    "\n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if previous_loss - current_loss < tolerance:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "                break\n",
    "        else:\n",
    "            epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "        previous_loss = current_loss\n",
    "\n",
    "    logit_model_lipschitz.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 28*28)\n",
    "            outputs = logit_model_lipschitz(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy Lipschitz Logit K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    lipschitz_logit_kmeans[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.408654</td>\n",
       "      <td>68.548077</td>\n",
       "      <td>65.918269</td>\n",
       "      <td>64.716346</td>\n",
       "      <td>63.1875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       75000      50000      10000      5000     1000 \n",
       "0  69.408654  68.548077  65.918269  64.716346  63.1875"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lipschitz_logit_kmeans_df = pd.DataFrame(lipschitz_logit_kmeans)\n",
    "lipschitz_logit_kmeans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 94.5194\n",
      "Epoch [1/50], Val Loss: 0.2846\n",
      "Epoch [2/50], Loss: 44.7997\n",
      "Epoch [2/50], Val Loss: 0.2217\n",
      "Epoch [3/50], Loss: 36.9589\n",
      "Epoch [3/50], Val Loss: 0.1807\n",
      "Epoch [4/50], Loss: 32.0297\n",
      "Epoch [4/50], Val Loss: 0.1678\n",
      "Epoch [5/50], Loss: 28.7951\n",
      "Epoch [5/50], Val Loss: 0.1569\n",
      "Epoch [6/50], Loss: 25.9148\n",
      "Epoch [6/50], Val Loss: 0.1677\n",
      "Epoch [7/50], Loss: 23.0563\n",
      "Epoch [7/50], Val Loss: 0.1464\n",
      "Epoch [8/50], Loss: 21.1001\n",
      "Epoch [8/50], Val Loss: 0.1412\n",
      "Epoch [9/50], Loss: 18.7614\n",
      "Epoch [9/50], Val Loss: 0.1422\n",
      "Epoch [10/50], Loss: 17.3641\n",
      "Epoch [10/50], Val Loss: 0.1389\n",
      "Epoch [11/50], Loss: 15.4320\n",
      "Epoch [11/50], Val Loss: 0.1325\n",
      "Epoch [12/50], Loss: 14.2311\n",
      "Epoch [12/50], Val Loss: 0.1361\n",
      "Epoch [13/50], Loss: 13.0377\n",
      "Epoch [13/50], Val Loss: 0.1347\n",
      "Epoch [14/50], Loss: 12.0162\n",
      "Epoch [14/50], Val Loss: 0.1372\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 92.61%\n",
      "Epoch [1/50], Loss: 78.8384\n",
      "Epoch [1/50], Val Loss: 0.3505\n",
      "Epoch [2/50], Loss: 35.6237\n",
      "Epoch [2/50], Val Loss: 0.2643\n",
      "Epoch [3/50], Loss: 29.1912\n",
      "Epoch [3/50], Val Loss: 0.2298\n",
      "Epoch [4/50], Loss: 24.9857\n",
      "Epoch [4/50], Val Loss: 0.2113\n",
      "Epoch [5/50], Loss: 22.2271\n",
      "Epoch [5/50], Val Loss: 0.1987\n",
      "Epoch [6/50], Loss: 19.7962\n",
      "Epoch [6/50], Val Loss: 0.1962\n",
      "Epoch [7/50], Loss: 18.0930\n",
      "Epoch [7/50], Val Loss: 0.1918\n",
      "Epoch [8/50], Loss: 16.2062\n",
      "Epoch [8/50], Val Loss: 0.1915\n",
      "Epoch [9/50], Loss: 14.4370\n",
      "Epoch [9/50], Val Loss: 0.1884\n",
      "Epoch [10/50], Loss: 13.1155\n",
      "Epoch [10/50], Val Loss: 0.2031\n",
      "Epoch [11/50], Loss: 11.8399\n",
      "Epoch [11/50], Val Loss: 0.1967\n",
      "Epoch [12/50], Loss: 10.8224\n",
      "Epoch [12/50], Val Loss: 0.1954\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 92.01%\n",
      "Epoch [1/50], Loss: 34.8576\n",
      "Epoch [1/50], Val Loss: 0.9705\n",
      "Epoch [2/50], Loss: 13.5302\n",
      "Epoch [2/50], Val Loss: 0.7309\n",
      "Epoch [3/50], Loss: 9.0522\n",
      "Epoch [3/50], Val Loss: 0.5429\n",
      "Epoch [4/50], Loss: 6.9517\n",
      "Epoch [4/50], Val Loss: 0.4767\n",
      "Epoch [5/50], Loss: 5.5631\n",
      "Epoch [5/50], Val Loss: 0.4429\n",
      "Epoch [6/50], Loss: 4.8080\n",
      "Epoch [6/50], Val Loss: 0.4965\n",
      "Epoch [7/50], Loss: 4.1766\n",
      "Epoch [7/50], Val Loss: 0.4482\n",
      "Epoch [8/50], Loss: 3.3512\n",
      "Epoch [8/50], Val Loss: 0.4881\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 86.12%\n",
      "Epoch [1/50], Loss: 22.8670\n",
      "Epoch [1/50], Val Loss: 1.7310\n",
      "Epoch [2/50], Loss: 10.4631\n",
      "Epoch [2/50], Val Loss: 1.1706\n",
      "Epoch [3/50], Loss: 6.8546\n",
      "Epoch [3/50], Val Loss: 0.8604\n",
      "Epoch [4/50], Loss: 5.3420\n",
      "Epoch [4/50], Val Loss: 0.7471\n",
      "Epoch [5/50], Loss: 4.0402\n",
      "Epoch [5/50], Val Loss: 0.7278\n",
      "Epoch [6/50], Loss: 3.2088\n",
      "Epoch [6/50], Val Loss: 0.6474\n",
      "Epoch [7/50], Loss: 2.4712\n",
      "Epoch [7/50], Val Loss: 0.7096\n",
      "Epoch [8/50], Loss: 2.2840\n",
      "Epoch [8/50], Val Loss: 0.6558\n",
      "Epoch [9/50], Loss: 1.6999\n",
      "Epoch [9/50], Val Loss: 0.6444\n",
      "Epoch [10/50], Loss: 1.2476\n",
      "Epoch [10/50], Val Loss: 0.7589\n",
      "Epoch [11/50], Loss: 1.5028\n",
      "Epoch [11/50], Val Loss: 0.7283\n",
      "Epoch [12/50], Loss: 1.7875\n",
      "Epoch [12/50], Val Loss: 0.6422\n",
      "Epoch [13/50], Loss: 0.9213\n",
      "Epoch [13/50], Val Loss: 0.6809\n",
      "Epoch [14/50], Loss: 0.7393\n",
      "Epoch [14/50], Val Loss: 0.6699\n",
      "Epoch [15/50], Loss: 0.6189\n",
      "Epoch [15/50], Val Loss: 0.7913\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 83.13%\n",
      "Epoch [1/50], Loss: 6.5613\n",
      "Epoch [1/50], Val Loss: 3.1654\n",
      "Epoch [2/50], Loss: 6.0576\n",
      "Epoch [2/50], Val Loss: 2.8341\n",
      "Epoch [3/50], Loss: 4.9463\n",
      "Epoch [3/50], Val Loss: 1.9671\n",
      "Epoch [4/50], Loss: 3.0551\n",
      "Epoch [4/50], Val Loss: 1.8394\n",
      "Epoch [5/50], Loss: 2.4933\n",
      "Epoch [5/50], Val Loss: 2.0541\n",
      "Epoch [6/50], Loss: 2.1651\n",
      "Epoch [6/50], Val Loss: 1.7257\n",
      "Epoch [7/50], Loss: 1.8843\n",
      "Epoch [7/50], Val Loss: 1.4265\n",
      "Epoch [8/50], Loss: 1.3555\n",
      "Epoch [8/50], Val Loss: 1.3664\n",
      "Epoch [9/50], Loss: 0.9983\n",
      "Epoch [9/50], Val Loss: 1.3859\n",
      "Epoch [10/50], Loss: 0.7834\n",
      "Epoch [10/50], Val Loss: 1.4936\n",
      "Epoch [11/50], Loss: 0.7114\n",
      "Epoch [11/50], Val Loss: 1.5427\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 66.23%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "cnn_kmeans = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    cnn_model = EMNIST_CNN(num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(cnn_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "\n",
    "    def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)  # Set all weights to zero\n",
    "                m.bias.data.fill_(0.01)\n",
    "    cnn_model.apply(init_weights)\n",
    "\n",
    "    best_val_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cnn_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = cnn_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        cnn_model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader_cnn:\n",
    "                images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "                outputs = cnn_model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader_cnn)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "                break\n",
    "\n",
    "\n",
    "    cnn_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "            outputs = cnn_model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy CNN K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    cnn_kmeans[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.610577</td>\n",
       "      <td>92.009615</td>\n",
       "      <td>86.120192</td>\n",
       "      <td>83.129808</td>\n",
       "      <td>66.225962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       75000      50000      10000      5000       1000 \n",
       "0  92.610577  92.009615  86.120192  83.129808  66.225962"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_kmeans_df = pd.DataFrame(cnn_kmeans)\n",
    "cnn_kmeans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Lipschitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 99.0845\n",
      "Epoch [1/50], Val Loss: 0.2954\n",
      "Epoch [2/50], Loss: 45.3697\n",
      "Epoch [2/50], Val Loss: 0.2222\n",
      "Epoch [3/50], Loss: 37.5560\n",
      "Epoch [3/50], Val Loss: 0.1874\n",
      "Epoch [4/50], Loss: 32.9338\n",
      "Epoch [4/50], Val Loss: 0.1738\n",
      "Epoch [5/50], Loss: 29.2415\n",
      "Epoch [5/50], Val Loss: 0.1615\n",
      "Epoch [6/50], Loss: 26.3568\n",
      "Epoch [6/50], Val Loss: 0.1512\n",
      "Epoch [7/50], Loss: 24.1001\n",
      "Epoch [7/50], Val Loss: 0.1534\n",
      "Epoch [8/50], Loss: 21.8716\n",
      "Epoch [8/50], Val Loss: 0.1401\n",
      "Epoch [9/50], Loss: 20.1402\n",
      "Epoch [9/50], Val Loss: 0.1351\n",
      "Epoch [10/50], Loss: 18.3980\n",
      "Epoch [10/50], Val Loss: 0.1399\n",
      "Epoch [11/50], Loss: 16.6092\n",
      "Epoch [11/50], Val Loss: 0.1396\n",
      "Epoch [12/50], Loss: 15.2523\n",
      "Epoch [12/50], Val Loss: 0.1343\n",
      "Epoch [13/50], Loss: 13.6351\n",
      "Epoch [13/50], Val Loss: 0.1301\n",
      "Epoch [14/50], Loss: 12.9812\n",
      "Epoch [14/50], Val Loss: 0.1333\n",
      "Epoch [15/50], Loss: 11.8987\n",
      "Epoch [15/50], Val Loss: 0.1358\n",
      "Epoch [16/50], Loss: 10.9193\n",
      "Epoch [16/50], Val Loss: 0.1405\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 92.48%\n",
      "Epoch [1/50], Loss: 83.4219\n",
      "Epoch [1/50], Val Loss: 0.3732\n",
      "Epoch [2/50], Loss: 36.5396\n",
      "Epoch [2/50], Val Loss: 0.2834\n",
      "Epoch [3/50], Loss: 28.9895\n",
      "Epoch [3/50], Val Loss: 0.2489\n",
      "Epoch [4/50], Loss: 25.2347\n",
      "Epoch [4/50], Val Loss: 0.2105\n",
      "Epoch [5/50], Loss: 22.4048\n",
      "Epoch [5/50], Val Loss: 0.2209\n",
      "Epoch [6/50], Loss: 20.1296\n",
      "Epoch [6/50], Val Loss: 0.1959\n",
      "Epoch [7/50], Loss: 18.0182\n",
      "Epoch [7/50], Val Loss: 0.1929\n",
      "Epoch [8/50], Loss: 16.5464\n",
      "Epoch [8/50], Val Loss: 0.1974\n",
      "Epoch [9/50], Loss: 14.3980\n",
      "Epoch [9/50], Val Loss: 0.1942\n",
      "Epoch [10/50], Loss: 13.0829\n",
      "Epoch [10/50], Val Loss: 0.1899\n",
      "Epoch [11/50], Loss: 12.0602\n",
      "Epoch [11/50], Val Loss: 0.2065\n",
      "Epoch [12/50], Loss: 10.4938\n",
      "Epoch [12/50], Val Loss: 0.1984\n",
      "Epoch [13/50], Loss: 9.9832\n",
      "Epoch [13/50], Val Loss: 0.2187\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 91.23%\n",
      "Epoch [1/50], Loss: 36.0117\n",
      "Epoch [1/50], Val Loss: 1.0175\n",
      "Epoch [2/50], Loss: 14.5665\n",
      "Epoch [2/50], Val Loss: 0.6806\n",
      "Epoch [3/50], Loss: 9.8354\n",
      "Epoch [3/50], Val Loss: 0.5532\n",
      "Epoch [4/50], Loss: 7.2486\n",
      "Epoch [4/50], Val Loss: 0.4739\n",
      "Epoch [5/50], Loss: 5.6743\n",
      "Epoch [5/50], Val Loss: 0.4197\n",
      "Epoch [6/50], Loss: 4.4558\n",
      "Epoch [6/50], Val Loss: 0.4304\n",
      "Epoch [7/50], Loss: 3.8596\n",
      "Epoch [7/50], Val Loss: 0.3961\n",
      "Epoch [8/50], Loss: 3.3227\n",
      "Epoch [8/50], Val Loss: 0.4375\n",
      "Epoch [9/50], Loss: 2.8865\n",
      "Epoch [9/50], Val Loss: 0.4311\n",
      "Epoch [10/50], Loss: 2.6032\n",
      "Epoch [10/50], Val Loss: 0.4898\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 86.19%\n",
      "Epoch [1/50], Loss: 23.5910\n",
      "Epoch [1/50], Val Loss: 1.6075\n",
      "Epoch [2/50], Loss: 10.7505\n",
      "Epoch [2/50], Val Loss: 1.1170\n",
      "Epoch [3/50], Loss: 7.0084\n",
      "Epoch [3/50], Val Loss: 0.9519\n",
      "Epoch [4/50], Loss: 5.5982\n",
      "Epoch [4/50], Val Loss: 0.7515\n",
      "Epoch [5/50], Loss: 4.3762\n",
      "Epoch [5/50], Val Loss: 0.7059\n",
      "Epoch [6/50], Loss: 3.4438\n",
      "Epoch [6/50], Val Loss: 0.6523\n",
      "Epoch [7/50], Loss: 2.7197\n",
      "Epoch [7/50], Val Loss: 0.7467\n",
      "Epoch [8/50], Loss: 2.6797\n",
      "Epoch [8/50], Val Loss: 0.6930\n",
      "Epoch [9/50], Loss: 2.0006\n",
      "Epoch [9/50], Val Loss: 0.6375\n",
      "Epoch [10/50], Loss: 1.9482\n",
      "Epoch [10/50], Val Loss: 0.7452\n",
      "Epoch [11/50], Loss: 2.0218\n",
      "Epoch [11/50], Val Loss: 0.6863\n",
      "Epoch [12/50], Loss: 1.2104\n",
      "Epoch [12/50], Val Loss: 0.7405\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 82.30%\n",
      "Epoch [1/50], Loss: 6.5562\n",
      "Epoch [1/50], Val Loss: 3.1466\n",
      "Epoch [2/50], Loss: 6.0220\n",
      "Epoch [2/50], Val Loss: 2.7708\n",
      "Epoch [3/50], Loss: 4.7302\n",
      "Epoch [3/50], Val Loss: 1.9356\n",
      "Epoch [4/50], Loss: 3.0510\n",
      "Epoch [4/50], Val Loss: 2.3672\n",
      "Epoch [5/50], Loss: 3.4902\n",
      "Epoch [5/50], Val Loss: 2.3603\n",
      "Epoch [6/50], Loss: 2.7312\n",
      "Epoch [6/50], Val Loss: 1.5064\n",
      "Epoch [7/50], Loss: 1.8762\n",
      "Epoch [7/50], Val Loss: 1.3192\n",
      "Epoch [8/50], Loss: 1.2337\n",
      "Epoch [8/50], Val Loss: 1.7087\n",
      "Epoch [9/50], Loss: 0.9843\n",
      "Epoch [9/50], Val Loss: 1.5345\n",
      "Epoch [10/50], Loss: 0.8076\n",
      "Epoch [10/50], Val Loss: 1.5200\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 65.16%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "cnn_lipschitz_kmeans = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    cnn_model_lipschitz = EMNIST_CNN(num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(cnn_model_lipschitz.parameters(), lr=0.005, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "\n",
    "    def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)  # Set all weights to zero\n",
    "                m.bias.data.fill_(0.01)\n",
    "    cnn_model_lipschitz.apply(init_weights)\n",
    "\n",
    "    best_val_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cnn_model_lipschitz.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = cnn_model_lipschitz(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            L = max(param.grad.abs().max().item() for param in cnn_model_lipschitz.parameters() if param.grad is not None)\n",
    "            adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "            # Update optimizer's learning rate\n",
    "            for param_group in optimizer_sdg.param_groups:\n",
    "                param_group['lr'] = adaptive_lr\n",
    "                \n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        cnn_model_lipschitz.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader_cnn:\n",
    "                images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "                outputs = cnn_model_lipschitz(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader_cnn)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "                break\n",
    "\n",
    "\n",
    "    cnn_model_lipschitz.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "            outputs = cnn_model_lipschitz(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy Lipschitz CNN K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    cnn_lipschitz_kmeans[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.480769</td>\n",
       "      <td>91.230769</td>\n",
       "      <td>86.192308</td>\n",
       "      <td>82.302885</td>\n",
       "      <td>65.163462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       75000      50000      10000      5000       1000 \n",
       "0  92.480769  91.230769  86.192308  82.302885  65.163462"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_lipschitz_kmeans_df = pd.DataFrame(cnn_lipschitz_kmeans)\n",
    "cnn_lipschitz_kmeans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 174.9038\n",
      "Epoch [2/100], Loss: 158.5837\n",
      "Epoch [3/100], Loss: 156.6188\n",
      "Epoch [4/100], Loss: 155.2408\n",
      "Epoch [5/100], Loss: 154.1766\n",
      "Epoch [6/100], Loss: 153.3897\n",
      "Epoch [7/100], Loss: 152.9343\n",
      "Epoch [8/100], Loss: 152.2684\n",
      "Epoch [9/100], Loss: 151.7720\n",
      "Epoch [10/100], Loss: 151.3398\n",
      "Epoch [11/100], Loss: 151.4978\n",
      "Epoch [12/100], Loss: 150.7178\n",
      "Epoch [13/100], Loss: 150.9036\n",
      "Epoch [14/100], Loss: 150.8395\n",
      "Epoch [15/100], Loss: 150.1416\n",
      "Epoch [16/100], Loss: 149.7245\n",
      "Epoch [17/100], Loss: 150.3522\n",
      "Epoch [18/100], Loss: 149.7974\n",
      "Epoch [19/100], Loss: 149.3703\n",
      "Epoch [20/100], Loss: 149.4271\n",
      "Epoch [21/100], Loss: 149.0711\n",
      "Epoch [22/100], Loss: 148.9810\n",
      "Epoch [23/100], Loss: 148.9032\n",
      "Epoch [24/100], Loss: 148.5919\n",
      "Epoch [25/100], Loss: 148.9894\n",
      "Epoch [26/100], Loss: 148.5118\n",
      "Epoch [27/100], Loss: 148.0841\n",
      "Epoch [28/100], Loss: 148.3945\n",
      "Epoch [29/100], Loss: 148.4000\n",
      "Epoch [30/100], Loss: 148.2652\n",
      "Epoch [31/100], Loss: 148.2148\n",
      "Epoch [32/100], Loss: 148.0385\n",
      "Epoch [33/100], Loss: 148.0330\n",
      "Epoch [34/100], Loss: 147.7681\n",
      "Epoch [35/100], Loss: 147.9898\n",
      "Epoch [36/100], Loss: 147.9973\n",
      "Epoch [37/100], Loss: 147.9655\n",
      "Epoch [38/100], Loss: 147.5687\n",
      "Epoch [39/100], Loss: 148.0634\n",
      "Epoch [40/100], Loss: 147.9298\n",
      "Epoch [41/100], Loss: 147.2302\n",
      "Epoch [42/100], Loss: 147.3659\n",
      "Epoch [43/100], Loss: 147.1660\n",
      "Epoch [44/100], Loss: 147.1083\n",
      "Epoch [45/100], Loss: 147.1138\n",
      "Epoch [46/100], Loss: 147.2823\n",
      "Epoch [47/100], Loss: 147.2486\n",
      "Epoch [48/100], Loss: 146.9936\n",
      "Epoch [49/100], Loss: 147.0457\n",
      "Epoch [50/100], Loss: 146.9130\n",
      "Epoch [51/100], Loss: 146.9753\n",
      "Epoch [52/100], Loss: 146.8406\n",
      "Epoch [53/100], Loss: 146.5515\n",
      "Epoch [54/100], Loss: 146.7627\n",
      "Epoch [55/100], Loss: 146.7513\n",
      "Epoch [56/100], Loss: 146.8656\n",
      "Epoch [57/100], Loss: 146.2744\n",
      "Epoch [58/100], Loss: 146.6849\n",
      "Epoch [59/100], Loss: 146.3815\n",
      "Epoch [60/100], Loss: 146.9031\n",
      "Epoch [61/100], Loss: 146.8424\n",
      "Epoch [62/100], Loss: 146.5523\n",
      "Epoch [63/100], Loss: 146.8293\n",
      "Epoch [64/100], Loss: 146.2725\n",
      "Epoch [65/100], Loss: 146.2832\n",
      "Epoch [66/100], Loss: 146.4593\n",
      "Epoch [67/100], Loss: 146.5209\n",
      "Stopping early at epoch 67 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Base Logit K Means: 69.21%\n",
      "Epoch [1/100], Loss: 120.8990\n",
      "Epoch [2/100], Loss: 106.6844\n",
      "Epoch [3/100], Loss: 104.5239\n",
      "Epoch [4/100], Loss: 102.9559\n",
      "Epoch [5/100], Loss: 102.5706\n",
      "Epoch [6/100], Loss: 101.6481\n",
      "Epoch [7/100], Loss: 101.0998\n",
      "Epoch [8/100], Loss: 100.7840\n",
      "Epoch [9/100], Loss: 100.3689\n",
      "Epoch [10/100], Loss: 100.2488\n",
      "Epoch [11/100], Loss: 100.0303\n",
      "Epoch [12/100], Loss: 99.5009\n",
      "Epoch [13/100], Loss: 99.1561\n",
      "Epoch [14/100], Loss: 99.0193\n",
      "Epoch [15/100], Loss: 98.8522\n",
      "Epoch [16/100], Loss: 98.6089\n",
      "Epoch [17/100], Loss: 98.3207\n",
      "Epoch [18/100], Loss: 98.2073\n",
      "Epoch [19/100], Loss: 98.1473\n",
      "Epoch [20/100], Loss: 98.0328\n",
      "Epoch [21/100], Loss: 97.7724\n",
      "Epoch [22/100], Loss: 97.5507\n",
      "Epoch [23/100], Loss: 97.3380\n",
      "Epoch [24/100], Loss: 97.1284\n",
      "Epoch [25/100], Loss: 97.3307\n",
      "Epoch [26/100], Loss: 97.2387\n",
      "Epoch [27/100], Loss: 96.7963\n",
      "Epoch [28/100], Loss: 96.8000\n",
      "Epoch [29/100], Loss: 96.8423\n",
      "Epoch [30/100], Loss: 96.5181\n",
      "Epoch [31/100], Loss: 96.4152\n",
      "Epoch [32/100], Loss: 96.5236\n",
      "Epoch [33/100], Loss: 96.2724\n",
      "Epoch [34/100], Loss: 96.1925\n",
      "Epoch [35/100], Loss: 96.3500\n",
      "Epoch [36/100], Loss: 95.9254\n",
      "Epoch [37/100], Loss: 95.8942\n",
      "Epoch [38/100], Loss: 96.0630\n",
      "Epoch [39/100], Loss: 96.1209\n",
      "Epoch [40/100], Loss: 95.7884\n",
      "Epoch [41/100], Loss: 96.0905\n",
      "Epoch [42/100], Loss: 95.9403\n",
      "Epoch [43/100], Loss: 95.4983\n",
      "Epoch [44/100], Loss: 95.6244\n",
      "Epoch [45/100], Loss: 95.4685\n",
      "Epoch [46/100], Loss: 95.4135\n",
      "Epoch [47/100], Loss: 95.6025\n",
      "Epoch [48/100], Loss: 95.4333\n",
      "Epoch [49/100], Loss: 95.0268\n",
      "Epoch [50/100], Loss: 95.1214\n",
      "Epoch [51/100], Loss: 95.0520\n",
      "Epoch [52/100], Loss: 95.1505\n",
      "Epoch [53/100], Loss: 95.0083\n",
      "Epoch [54/100], Loss: 94.9539\n",
      "Epoch [55/100], Loss: 95.1746\n",
      "Epoch [56/100], Loss: 95.0879\n",
      "Epoch [57/100], Loss: 94.8130\n",
      "Epoch [58/100], Loss: 95.0772\n",
      "Epoch [59/100], Loss: 94.9730\n",
      "Epoch [60/100], Loss: 94.5036\n",
      "Epoch [61/100], Loss: 94.7527\n",
      "Epoch [62/100], Loss: 94.7576\n",
      "Epoch [63/100], Loss: 94.7066\n",
      "Epoch [64/100], Loss: 94.4627\n",
      "Epoch [65/100], Loss: 94.7394\n",
      "Epoch [66/100], Loss: 94.7682\n",
      "Epoch [67/100], Loss: 94.3883\n",
      "Epoch [68/100], Loss: 94.5116\n",
      "Epoch [69/100], Loss: 94.5649\n",
      "Epoch [70/100], Loss: 94.4813\n",
      "Epoch [71/100], Loss: 94.4596\n",
      "Epoch [72/100], Loss: 94.3875\n",
      "Epoch [73/100], Loss: 94.4612\n",
      "Epoch [74/100], Loss: 94.4203\n",
      "Epoch [75/100], Loss: 94.2874\n",
      "Epoch [76/100], Loss: 94.3039\n",
      "Epoch [77/100], Loss: 94.1068\n",
      "Epoch [78/100], Loss: 94.4664\n",
      "Epoch [79/100], Loss: 94.4219\n",
      "Epoch [80/100], Loss: 93.8059\n",
      "Epoch [81/100], Loss: 94.1793\n",
      "Epoch [82/100], Loss: 94.0532\n",
      "Epoch [83/100], Loss: 94.0156\n",
      "Epoch [84/100], Loss: 93.8599\n",
      "Epoch [85/100], Loss: 94.1369\n",
      "Epoch [86/100], Loss: 93.8196\n",
      "Epoch [87/100], Loss: 93.8509\n",
      "Epoch [88/100], Loss: 93.9633\n",
      "Epoch [89/100], Loss: 93.6183\n",
      "Epoch [90/100], Loss: 94.0369\n",
      "Epoch [91/100], Loss: 93.7293\n",
      "Epoch [92/100], Loss: 93.8607\n",
      "Epoch [93/100], Loss: 93.6698\n",
      "Epoch [94/100], Loss: 93.6021\n",
      "Epoch [95/100], Loss: 93.7318\n",
      "Epoch [96/100], Loss: 94.0519\n",
      "Epoch [97/100], Loss: 93.7536\n",
      "Epoch [98/100], Loss: 94.1886\n",
      "Epoch [99/100], Loss: 93.6905\n",
      "Epoch [100/100], Loss: 93.6557\n",
      "Test Accuracy Base Logit K Means: 68.43%\n",
      "Epoch [1/100], Loss: 30.2291\n",
      "Epoch [2/100], Loss: 20.5994\n",
      "Epoch [3/100], Loss: 18.9491\n",
      "Epoch [4/100], Loss: 18.1979\n",
      "Epoch [5/100], Loss: 17.7677\n",
      "Epoch [6/100], Loss: 17.2763\n",
      "Epoch [7/100], Loss: 16.8764\n",
      "Epoch [8/100], Loss: 16.6834\n",
      "Epoch [9/100], Loss: 16.4931\n",
      "Epoch [10/100], Loss: 16.2115\n",
      "Epoch [11/100], Loss: 16.3233\n",
      "Epoch [12/100], Loss: 15.9606\n",
      "Epoch [13/100], Loss: 15.8063\n",
      "Epoch [14/100], Loss: 15.7633\n",
      "Epoch [15/100], Loss: 15.6970\n",
      "Epoch [16/100], Loss: 15.4702\n",
      "Epoch [17/100], Loss: 15.2625\n",
      "Epoch [18/100], Loss: 15.2718\n",
      "Epoch [19/100], Loss: 15.2863\n",
      "Epoch [20/100], Loss: 15.1817\n",
      "Epoch [21/100], Loss: 14.8124\n",
      "Epoch [22/100], Loss: 14.8066\n",
      "Epoch [23/100], Loss: 14.6198\n",
      "Epoch [24/100], Loss: 14.7108\n",
      "Epoch [25/100], Loss: 14.5334\n",
      "Epoch [26/100], Loss: 14.4391\n",
      "Epoch [27/100], Loss: 14.3014\n",
      "Epoch [28/100], Loss: 14.3113\n",
      "Epoch [29/100], Loss: 14.2745\n",
      "Epoch [30/100], Loss: 14.2091\n",
      "Epoch [31/100], Loss: 14.1291\n",
      "Epoch [32/100], Loss: 14.0858\n",
      "Epoch [33/100], Loss: 14.0123\n",
      "Epoch [34/100], Loss: 14.0846\n",
      "Epoch [35/100], Loss: 13.9954\n",
      "Epoch [36/100], Loss: 13.8202\n",
      "Epoch [37/100], Loss: 13.6721\n",
      "Epoch [38/100], Loss: 13.7699\n",
      "Epoch [39/100], Loss: 13.7353\n",
      "Epoch [40/100], Loss: 13.6520\n",
      "Epoch [41/100], Loss: 13.6501\n",
      "Epoch [42/100], Loss: 13.6868\n",
      "Epoch [43/100], Loss: 13.5666\n",
      "Epoch [44/100], Loss: 13.5825\n",
      "Epoch [45/100], Loss: 13.3869\n",
      "Epoch [46/100], Loss: 13.3050\n",
      "Epoch [47/100], Loss: 13.3461\n",
      "Epoch [48/100], Loss: 13.1878\n",
      "Epoch [49/100], Loss: 13.2695\n",
      "Epoch [50/100], Loss: 13.1651\n",
      "Epoch [51/100], Loss: 13.3299\n",
      "Epoch [52/100], Loss: 13.1628\n",
      "Epoch [53/100], Loss: 12.9577\n",
      "Epoch [54/100], Loss: 13.1290\n",
      "Epoch [55/100], Loss: 13.0808\n",
      "Epoch [56/100], Loss: 13.0499\n",
      "Epoch [57/100], Loss: 12.8111\n",
      "Epoch [58/100], Loss: 12.9700\n",
      "Epoch [59/100], Loss: 12.8807\n",
      "Epoch [60/100], Loss: 12.8616\n",
      "Epoch [61/100], Loss: 12.8065\n",
      "Epoch [62/100], Loss: 12.6935\n",
      "Epoch [63/100], Loss: 12.7437\n",
      "Epoch [64/100], Loss: 12.6219\n",
      "Epoch [65/100], Loss: 12.5862\n",
      "Epoch [66/100], Loss: 12.6382\n",
      "Epoch [67/100], Loss: 12.6642\n",
      "Epoch [68/100], Loss: 12.5451\n",
      "Epoch [69/100], Loss: 12.5668\n",
      "Epoch [70/100], Loss: 12.5909\n",
      "Epoch [71/100], Loss: 12.5322\n",
      "Epoch [72/100], Loss: 12.5106\n",
      "Epoch [73/100], Loss: 12.4391\n",
      "Epoch [74/100], Loss: 12.2834\n",
      "Epoch [75/100], Loss: 12.4430\n",
      "Epoch [76/100], Loss: 12.2983\n",
      "Epoch [77/100], Loss: 12.4050\n",
      "Epoch [78/100], Loss: 12.3761\n",
      "Epoch [79/100], Loss: 12.2275\n",
      "Epoch [80/100], Loss: 12.2767\n",
      "Epoch [81/100], Loss: 12.2380\n",
      "Epoch [82/100], Loss: 12.1863\n",
      "Epoch [83/100], Loss: 12.2303\n",
      "Epoch [84/100], Loss: 12.2134\n",
      "Epoch [85/100], Loss: 12.3589\n",
      "Epoch [86/100], Loss: 12.3352\n",
      "Epoch [87/100], Loss: 12.1556\n",
      "Epoch [88/100], Loss: 12.0191\n",
      "Epoch [89/100], Loss: 12.1443\n",
      "Epoch [90/100], Loss: 12.0793\n",
      "Epoch [91/100], Loss: 12.1917\n",
      "Epoch [92/100], Loss: 11.9896\n",
      "Epoch [93/100], Loss: 11.9436\n",
      "Epoch [94/100], Loss: 11.9434\n",
      "Epoch [95/100], Loss: 11.8834\n",
      "Epoch [96/100], Loss: 11.9485\n",
      "Epoch [97/100], Loss: 11.9009\n",
      "Epoch [98/100], Loss: 11.7608\n",
      "Epoch [99/100], Loss: 11.8179\n",
      "Epoch [100/100], Loss: 11.9314\n",
      "Test Accuracy Base Logit K Means: 64.03%\n",
      "Epoch [1/100], Loss: 17.8016\n",
      "Epoch [2/100], Loss: 10.3073\n",
      "Epoch [3/100], Loss: 9.1423\n",
      "Epoch [4/100], Loss: 8.7570\n",
      "Epoch [5/100], Loss: 8.6933\n",
      "Epoch [6/100], Loss: 8.1531\n",
      "Epoch [7/100], Loss: 7.4551\n",
      "Epoch [8/100], Loss: 7.4221\n",
      "Epoch [9/100], Loss: 7.2996\n",
      "Epoch [10/100], Loss: 7.4707\n",
      "Epoch [11/100], Loss: 6.9177\n",
      "Epoch [12/100], Loss: 6.7474\n",
      "Epoch [13/100], Loss: 6.6855\n",
      "Epoch [14/100], Loss: 6.7692\n",
      "Epoch [15/100], Loss: 6.5982\n",
      "Epoch [16/100], Loss: 6.5677\n",
      "Epoch [17/100], Loss: 6.2668\n",
      "Epoch [18/100], Loss: 6.4216\n",
      "Epoch [19/100], Loss: 6.3312\n",
      "Epoch [20/100], Loss: 6.1062\n",
      "Epoch [21/100], Loss: 6.1332\n",
      "Epoch [22/100], Loss: 6.2292\n",
      "Epoch [23/100], Loss: 6.0220\n",
      "Epoch [24/100], Loss: 5.9088\n",
      "Epoch [25/100], Loss: 5.8362\n",
      "Epoch [26/100], Loss: 5.8509\n",
      "Epoch [27/100], Loss: 5.7399\n",
      "Epoch [28/100], Loss: 5.8692\n",
      "Epoch [29/100], Loss: 5.8768\n",
      "Epoch [30/100], Loss: 5.7268\n",
      "Epoch [31/100], Loss: 5.6826\n",
      "Epoch [32/100], Loss: 5.6840\n",
      "Epoch [33/100], Loss: 5.7176\n",
      "Epoch [34/100], Loss: 5.7088\n",
      "Epoch [35/100], Loss: 5.5612\n",
      "Epoch [36/100], Loss: 5.3994\n",
      "Epoch [37/100], Loss: 5.3014\n",
      "Epoch [38/100], Loss: 5.4657\n",
      "Epoch [39/100], Loss: 5.3549\n",
      "Epoch [40/100], Loss: 5.3409\n",
      "Epoch [41/100], Loss: 5.4943\n",
      "Epoch [42/100], Loss: 5.3095\n",
      "Epoch [43/100], Loss: 5.1821\n",
      "Epoch [44/100], Loss: 5.2130\n",
      "Epoch [45/100], Loss: 5.0945\n",
      "Epoch [46/100], Loss: 5.1325\n",
      "Epoch [47/100], Loss: 5.5264\n",
      "Epoch [48/100], Loss: 5.1262\n",
      "Epoch [49/100], Loss: 5.0361\n",
      "Epoch [50/100], Loss: 4.9883\n",
      "Epoch [51/100], Loss: 5.0101\n",
      "Epoch [52/100], Loss: 4.9223\n",
      "Epoch [53/100], Loss: 4.8228\n",
      "Epoch [54/100], Loss: 4.9069\n",
      "Epoch [55/100], Loss: 5.0137\n",
      "Epoch [56/100], Loss: 4.8052\n",
      "Epoch [57/100], Loss: 4.9445\n",
      "Epoch [58/100], Loss: 5.3780\n",
      "Epoch [59/100], Loss: 4.8849\n",
      "Epoch [60/100], Loss: 4.8483\n",
      "Epoch [61/100], Loss: 4.8027\n",
      "Epoch [62/100], Loss: 4.7343\n",
      "Epoch [63/100], Loss: 4.7786\n",
      "Epoch [64/100], Loss: 4.7037\n",
      "Epoch [65/100], Loss: 4.7320\n",
      "Epoch [66/100], Loss: 4.7702\n",
      "Epoch [67/100], Loss: 4.6807\n",
      "Epoch [68/100], Loss: 4.6931\n",
      "Epoch [69/100], Loss: 4.6800\n",
      "Epoch [70/100], Loss: 4.7267\n",
      "Epoch [71/100], Loss: 4.8056\n",
      "Epoch [72/100], Loss: 4.6670\n",
      "Epoch [73/100], Loss: 4.5711\n",
      "Epoch [74/100], Loss: 4.4555\n",
      "Epoch [75/100], Loss: 4.7466\n",
      "Epoch [76/100], Loss: 4.7165\n",
      "Epoch [77/100], Loss: 4.4931\n",
      "Epoch [78/100], Loss: 4.4841\n",
      "Epoch [79/100], Loss: 4.4901\n",
      "Epoch [80/100], Loss: 4.5916\n",
      "Epoch [81/100], Loss: 4.6324\n",
      "Stopping early at epoch 81 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Base Logit K Means: 63.98%\n",
      "Epoch [1/100], Loss: 5.9017\n",
      "Epoch [2/100], Loss: 3.5789\n",
      "Epoch [3/100], Loss: 2.3565\n",
      "Epoch [4/100], Loss: 1.8888\n",
      "Epoch [5/100], Loss: 1.6429\n",
      "Epoch [6/100], Loss: 1.4818\n",
      "Epoch [7/100], Loss: 1.3578\n",
      "Epoch [8/100], Loss: 1.2706\n",
      "Epoch [9/100], Loss: 1.1971\n",
      "Epoch [10/100], Loss: 1.1330\n",
      "Epoch [11/100], Loss: 1.0752\n",
      "Epoch [12/100], Loss: 1.0374\n",
      "Epoch [13/100], Loss: 0.9877\n",
      "Epoch [14/100], Loss: 0.9651\n",
      "Epoch [15/100], Loss: 0.9347\n",
      "Epoch [16/100], Loss: 0.9010\n",
      "Epoch [17/100], Loss: 0.8644\n",
      "Epoch [18/100], Loss: 0.8450\n",
      "Epoch [19/100], Loss: 0.8192\n",
      "Epoch [20/100], Loss: 0.7952\n",
      "Epoch [21/100], Loss: 0.7787\n",
      "Epoch [22/100], Loss: 0.7640\n",
      "Epoch [23/100], Loss: 0.7423\n",
      "Epoch [24/100], Loss: 0.7295\n",
      "Epoch [25/100], Loss: 0.7081\n",
      "Epoch [26/100], Loss: 0.6957\n",
      "Epoch [27/100], Loss: 0.6851\n",
      "Epoch [28/100], Loss: 0.6714\n",
      "Epoch [29/100], Loss: 0.6540\n",
      "Epoch [30/100], Loss: 0.6398\n",
      "Epoch [31/100], Loss: 0.6328\n",
      "Epoch [32/100], Loss: 0.6183\n",
      "Epoch [33/100], Loss: 0.6094\n",
      "Epoch [34/100], Loss: 0.5981\n",
      "Epoch [35/100], Loss: 0.5906\n",
      "Epoch [36/100], Loss: 0.5776\n",
      "Epoch [37/100], Loss: 0.5748\n",
      "Epoch [38/100], Loss: 0.5602\n",
      "Epoch [39/100], Loss: 0.5545\n",
      "Epoch [40/100], Loss: 0.5411\n",
      "Epoch [41/100], Loss: 0.5339\n",
      "Epoch [42/100], Loss: 0.5260\n",
      "Epoch [43/100], Loss: 0.5177\n",
      "Epoch [44/100], Loss: 0.5125\n",
      "Epoch [45/100], Loss: 0.5049\n",
      "Epoch [46/100], Loss: 0.4950\n",
      "Epoch [47/100], Loss: 0.4911\n",
      "Epoch [48/100], Loss: 0.4830\n",
      "Epoch [49/100], Loss: 0.4757\n",
      "Epoch [50/100], Loss: 0.4728\n",
      "Epoch [51/100], Loss: 0.4663\n",
      "Epoch [52/100], Loss: 0.4583\n",
      "Epoch [53/100], Loss: 0.4513\n",
      "Epoch [54/100], Loss: 0.4510\n",
      "Epoch [55/100], Loss: 0.4433\n",
      "Epoch [56/100], Loss: 0.4378\n",
      "Epoch [57/100], Loss: 0.4339\n",
      "Epoch [58/100], Loss: 0.4329\n",
      "Epoch [59/100], Loss: 0.4232\n",
      "Epoch [60/100], Loss: 0.4187\n",
      "Epoch [61/100], Loss: 0.4154\n",
      "Epoch [62/100], Loss: 0.4108\n",
      "Epoch [63/100], Loss: 0.4037\n",
      "Epoch [64/100], Loss: 0.4011\n",
      "Epoch [65/100], Loss: 0.3951\n",
      "Epoch [66/100], Loss: 0.3914\n",
      "Epoch [67/100], Loss: 0.3871\n",
      "Epoch [68/100], Loss: 0.3844\n",
      "Epoch [69/100], Loss: 0.3791\n",
      "Epoch [70/100], Loss: 0.3759\n",
      "Epoch [71/100], Loss: 0.3713\n",
      "Epoch [72/100], Loss: 0.3689\n",
      "Epoch [73/100], Loss: 0.3667\n",
      "Epoch [74/100], Loss: 0.3622\n",
      "Epoch [75/100], Loss: 0.3568\n",
      "Epoch [76/100], Loss: 0.3550\n",
      "Epoch [77/100], Loss: 0.3543\n",
      "Epoch [78/100], Loss: 0.3457\n",
      "Epoch [79/100], Loss: 0.3428\n",
      "Epoch [80/100], Loss: 0.3417\n",
      "Epoch [81/100], Loss: 0.3376\n",
      "Epoch [82/100], Loss: 0.3333\n",
      "Epoch [83/100], Loss: 0.3325\n",
      "Epoch [84/100], Loss: 0.3291\n",
      "Epoch [85/100], Loss: 0.3264\n",
      "Epoch [86/100], Loss: 0.3229\n",
      "Epoch [87/100], Loss: 0.3199\n",
      "Epoch [88/100], Loss: 0.3163\n",
      "Epoch [89/100], Loss: 0.3134\n",
      "Epoch [90/100], Loss: 0.3109\n",
      "Epoch [91/100], Loss: 0.3116\n",
      "Epoch [92/100], Loss: 0.3082\n",
      "Epoch [93/100], Loss: 0.3046\n",
      "Epoch [94/100], Loss: 0.3022\n",
      "Epoch [95/100], Loss: 0.3010\n",
      "Epoch [96/100], Loss: 0.2984\n",
      "Epoch [97/100], Loss: 0.2946\n",
      "Epoch [98/100], Loss: 0.2937\n",
      "Epoch [99/100], Loss: 0.2908\n",
      "Epoch [100/100], Loss: 0.2889\n",
      "Test Accuracy Base Logit K Means: 62.68%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "base_logit_kmeans_plus = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_plus_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_plus_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    logit_model = LogisticRegression(input_dim, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(logit_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "    previous_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logit_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = logit_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if previous_loss - current_loss < tolerance:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "                break\n",
    "        else:\n",
    "            epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "        previous_loss = current_loss\n",
    "\n",
    "    logit_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 28*28)\n",
    "            outputs = logit_model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy Base Logit K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    base_logit_kmeans_plus[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.206731</td>\n",
       "      <td>68.427885</td>\n",
       "      <td>64.028846</td>\n",
       "      <td>63.975962</td>\n",
       "      <td>62.682692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       75000      50000      10000      5000       1000 \n",
       "0  69.206731  68.427885  64.028846  63.975962  62.682692"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_logit_kmeans_plus_df = pd.DataFrame(base_logit_kmeans_plus)\n",
    "base_logit_kmeans_plus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipschitz Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 174.9405\n",
      "Epoch [2/100], Loss: 159.1250\n",
      "Epoch [3/100], Loss: 156.4808\n",
      "Epoch [4/100], Loss: 154.8477\n",
      "Epoch [5/100], Loss: 153.9503\n",
      "Epoch [6/100], Loss: 153.3894\n",
      "Epoch [7/100], Loss: 152.9291\n",
      "Epoch [8/100], Loss: 152.2461\n",
      "Epoch [9/100], Loss: 151.7452\n",
      "Epoch [10/100], Loss: 151.9274\n",
      "Epoch [11/100], Loss: 151.0914\n",
      "Epoch [12/100], Loss: 151.0563\n",
      "Epoch [13/100], Loss: 150.9092\n",
      "Epoch [14/100], Loss: 150.3412\n",
      "Epoch [15/100], Loss: 150.1272\n",
      "Epoch [16/100], Loss: 150.0380\n",
      "Epoch [17/100], Loss: 149.6670\n",
      "Epoch [18/100], Loss: 149.4642\n",
      "Epoch [19/100], Loss: 149.5227\n",
      "Epoch [20/100], Loss: 149.3764\n",
      "Epoch [21/100], Loss: 149.3691\n",
      "Epoch [22/100], Loss: 149.1877\n",
      "Epoch [23/100], Loss: 148.8774\n",
      "Epoch [24/100], Loss: 148.4891\n",
      "Epoch [25/100], Loss: 148.5754\n",
      "Epoch [26/100], Loss: 148.4985\n",
      "Epoch [27/100], Loss: 148.2859\n",
      "Epoch [28/100], Loss: 148.5809\n",
      "Epoch [29/100], Loss: 148.2279\n",
      "Epoch [30/100], Loss: 147.9092\n",
      "Epoch [31/100], Loss: 148.0905\n",
      "Epoch [32/100], Loss: 147.9455\n",
      "Epoch [33/100], Loss: 148.0844\n",
      "Epoch [34/100], Loss: 147.8019\n",
      "Epoch [35/100], Loss: 148.0716\n",
      "Epoch [36/100], Loss: 147.5828\n",
      "Epoch [37/100], Loss: 147.7961\n",
      "Epoch [38/100], Loss: 147.6227\n",
      "Epoch [39/100], Loss: 147.6509\n",
      "Epoch [40/100], Loss: 147.4855\n",
      "Epoch [41/100], Loss: 147.3830\n",
      "Epoch [42/100], Loss: 147.2884\n",
      "Epoch [43/100], Loss: 147.2878\n",
      "Epoch [44/100], Loss: 147.0250\n",
      "Epoch [45/100], Loss: 147.0257\n",
      "Epoch [46/100], Loss: 147.3934\n",
      "Epoch [47/100], Loss: 147.0344\n",
      "Epoch [48/100], Loss: 146.9226\n",
      "Epoch [49/100], Loss: 147.1524\n",
      "Epoch [50/100], Loss: 146.4305\n",
      "Epoch [51/100], Loss: 146.8398\n",
      "Epoch [52/100], Loss: 146.7030\n",
      "Epoch [53/100], Loss: 146.8918\n",
      "Epoch [54/100], Loss: 146.7543\n",
      "Epoch [55/100], Loss: 146.6081\n",
      "Epoch [56/100], Loss: 146.7860\n",
      "Epoch [57/100], Loss: 146.6392\n",
      "Epoch [58/100], Loss: 146.5767\n",
      "Epoch [59/100], Loss: 146.2732\n",
      "Epoch [60/100], Loss: 146.5110\n",
      "Epoch [61/100], Loss: 146.4605\n",
      "Epoch [62/100], Loss: 146.6328\n",
      "Epoch [63/100], Loss: 146.6047\n",
      "Epoch [64/100], Loss: 146.3802\n",
      "Epoch [65/100], Loss: 146.2324\n",
      "Epoch [66/100], Loss: 146.4378\n",
      "Epoch [67/100], Loss: 146.2899\n",
      "Epoch [68/100], Loss: 146.3065\n",
      "Epoch [69/100], Loss: 146.0450\n",
      "Epoch [70/100], Loss: 146.3828\n",
      "Epoch [71/100], Loss: 146.4483\n",
      "Epoch [72/100], Loss: 146.5170\n",
      "Stopping early at epoch 72 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Lipschitz Logit K Means: 68.88%\n",
      "Epoch [1/100], Loss: 121.1539\n",
      "Epoch [2/100], Loss: 106.3630\n",
      "Epoch [3/100], Loss: 104.2895\n",
      "Epoch [4/100], Loss: 103.2809\n",
      "Epoch [5/100], Loss: 102.2511\n",
      "Epoch [6/100], Loss: 101.6895\n",
      "Epoch [7/100], Loss: 101.4350\n",
      "Epoch [8/100], Loss: 100.9163\n",
      "Epoch [9/100], Loss: 100.3193\n",
      "Epoch [10/100], Loss: 100.0680\n",
      "Epoch [11/100], Loss: 99.6303\n",
      "Epoch [12/100], Loss: 99.1730\n",
      "Epoch [13/100], Loss: 98.7304\n",
      "Epoch [14/100], Loss: 99.0177\n",
      "Epoch [15/100], Loss: 98.6315\n",
      "Epoch [16/100], Loss: 98.6896\n",
      "Epoch [17/100], Loss: 98.3841\n",
      "Epoch [18/100], Loss: 98.2535\n",
      "Epoch [19/100], Loss: 98.2456\n",
      "Epoch [20/100], Loss: 97.8216\n",
      "Epoch [21/100], Loss: 97.6362\n",
      "Epoch [22/100], Loss: 97.4598\n",
      "Epoch [23/100], Loss: 97.7911\n",
      "Epoch [24/100], Loss: 97.2635\n",
      "Epoch [25/100], Loss: 96.9533\n",
      "Epoch [26/100], Loss: 97.1062\n",
      "Epoch [27/100], Loss: 97.0876\n",
      "Epoch [28/100], Loss: 96.8318\n",
      "Epoch [29/100], Loss: 96.7694\n",
      "Epoch [30/100], Loss: 96.9733\n",
      "Epoch [31/100], Loss: 96.6383\n",
      "Epoch [32/100], Loss: 96.4912\n",
      "Epoch [33/100], Loss: 96.0464\n",
      "Epoch [34/100], Loss: 96.5777\n",
      "Epoch [35/100], Loss: 96.1743\n",
      "Epoch [36/100], Loss: 95.9662\n",
      "Epoch [37/100], Loss: 95.9814\n",
      "Epoch [38/100], Loss: 96.0926\n",
      "Epoch [39/100], Loss: 95.8907\n",
      "Epoch [40/100], Loss: 96.0201\n",
      "Epoch [41/100], Loss: 95.9185\n",
      "Epoch [42/100], Loss: 95.5442\n",
      "Epoch [43/100], Loss: 95.7431\n",
      "Epoch [44/100], Loss: 95.6599\n",
      "Epoch [45/100], Loss: 95.7392\n",
      "Epoch [46/100], Loss: 95.1334\n",
      "Epoch [47/100], Loss: 95.5591\n",
      "Epoch [48/100], Loss: 95.1859\n",
      "Epoch [49/100], Loss: 95.2932\n",
      "Epoch [50/100], Loss: 95.2463\n",
      "Epoch [51/100], Loss: 95.1271\n",
      "Epoch [52/100], Loss: 95.3291\n",
      "Epoch [53/100], Loss: 95.0214\n",
      "Epoch [54/100], Loss: 94.7562\n",
      "Epoch [55/100], Loss: 94.9805\n",
      "Epoch [56/100], Loss: 94.9510\n",
      "Epoch [57/100], Loss: 94.7638\n",
      "Epoch [58/100], Loss: 95.0958\n",
      "Epoch [59/100], Loss: 94.9061\n",
      "Epoch [60/100], Loss: 94.7788\n",
      "Epoch [61/100], Loss: 94.9253\n",
      "Epoch [62/100], Loss: 94.8411\n",
      "Epoch [63/100], Loss: 94.8071\n",
      "Epoch [64/100], Loss: 94.6044\n",
      "Epoch [65/100], Loss: 94.4600\n",
      "Epoch [66/100], Loss: 94.7599\n",
      "Epoch [67/100], Loss: 94.7031\n",
      "Epoch [68/100], Loss: 94.4970\n",
      "Epoch [69/100], Loss: 94.4466\n",
      "Epoch [70/100], Loss: 94.3382\n",
      "Epoch [71/100], Loss: 94.4818\n",
      "Epoch [72/100], Loss: 94.4329\n",
      "Epoch [73/100], Loss: 94.4995\n",
      "Epoch [74/100], Loss: 94.1070\n",
      "Epoch [75/100], Loss: 94.1747\n",
      "Epoch [76/100], Loss: 94.2005\n",
      "Epoch [77/100], Loss: 94.3905\n",
      "Stopping early at epoch 77 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Lipschitz Logit K Means: 68.66%\n",
      "Epoch [1/100], Loss: 30.2349\n",
      "Epoch [2/100], Loss: 20.5571\n",
      "Epoch [3/100], Loss: 19.0565\n",
      "Epoch [4/100], Loss: 18.2890\n",
      "Epoch [5/100], Loss: 17.8786\n",
      "Epoch [6/100], Loss: 17.5182\n",
      "Epoch [7/100], Loss: 17.0793\n",
      "Epoch [8/100], Loss: 16.7228\n",
      "Epoch [9/100], Loss: 16.6709\n",
      "Epoch [10/100], Loss: 16.3961\n",
      "Epoch [11/100], Loss: 16.1241\n",
      "Epoch [12/100], Loss: 15.7716\n",
      "Epoch [13/100], Loss: 15.7977\n",
      "Epoch [14/100], Loss: 15.7831\n",
      "Epoch [15/100], Loss: 15.5932\n",
      "Epoch [16/100], Loss: 15.2909\n",
      "Epoch [17/100], Loss: 15.1808\n",
      "Epoch [18/100], Loss: 15.2619\n",
      "Epoch [19/100], Loss: 15.0469\n",
      "Epoch [20/100], Loss: 14.9687\n",
      "Epoch [21/100], Loss: 14.9143\n",
      "Epoch [22/100], Loss: 14.9129\n",
      "Epoch [23/100], Loss: 14.5578\n",
      "Epoch [24/100], Loss: 14.5888\n",
      "Epoch [25/100], Loss: 14.5893\n",
      "Epoch [26/100], Loss: 14.4676\n",
      "Epoch [27/100], Loss: 14.4364\n",
      "Epoch [28/100], Loss: 14.2486\n",
      "Epoch [29/100], Loss: 14.2309\n",
      "Epoch [30/100], Loss: 14.1328\n",
      "Epoch [31/100], Loss: 14.0442\n",
      "Epoch [32/100], Loss: 14.0439\n",
      "Epoch [33/100], Loss: 14.0445\n",
      "Epoch [34/100], Loss: 14.0950\n",
      "Stopping early at epoch 34 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Lipschitz Logit K Means: 66.22%\n",
      "Epoch [1/100], Loss: 17.8250\n",
      "Epoch [2/100], Loss: 10.4112\n",
      "Epoch [3/100], Loss: 9.1402\n",
      "Epoch [4/100], Loss: 8.5687\n",
      "Epoch [5/100], Loss: 8.1242\n",
      "Epoch [6/100], Loss: 8.1063\n",
      "Epoch [7/100], Loss: 7.6545\n",
      "Epoch [8/100], Loss: 7.5286\n",
      "Epoch [9/100], Loss: 7.2415\n",
      "Epoch [10/100], Loss: 7.2629\n",
      "Epoch [11/100], Loss: 7.1187\n",
      "Epoch [12/100], Loss: 6.9197\n",
      "Epoch [13/100], Loss: 6.7011\n",
      "Epoch [14/100], Loss: 6.5376\n",
      "Epoch [15/100], Loss: 6.4615\n",
      "Epoch [16/100], Loss: 6.4079\n",
      "Epoch [17/100], Loss: 6.4733\n",
      "Epoch [18/100], Loss: 6.4516\n",
      "Epoch [19/100], Loss: 6.1479\n",
      "Epoch [20/100], Loss: 6.1819\n",
      "Epoch [21/100], Loss: 6.1267\n",
      "Epoch [22/100], Loss: 5.9909\n",
      "Epoch [23/100], Loss: 5.9680\n",
      "Epoch [24/100], Loss: 6.0543\n",
      "Epoch [25/100], Loss: 5.9809\n",
      "Epoch [26/100], Loss: 5.7465\n",
      "Epoch [27/100], Loss: 5.6935\n",
      "Epoch [28/100], Loss: 5.7399\n",
      "Epoch [29/100], Loss: 5.9134\n",
      "Epoch [30/100], Loss: 5.8291\n",
      "Epoch [31/100], Loss: 5.6603\n",
      "Epoch [32/100], Loss: 5.4810\n",
      "Epoch [33/100], Loss: 5.4228\n",
      "Epoch [34/100], Loss: 5.3861\n",
      "Epoch [35/100], Loss: 5.4681\n",
      "Epoch [36/100], Loss: 5.4742\n",
      "Epoch [37/100], Loss: 5.6129\n",
      "Stopping early at epoch 37 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Lipschitz Logit K Means: 65.29%\n",
      "Epoch [1/100], Loss: 5.8936\n",
      "Epoch [2/100], Loss: 3.5857\n",
      "Epoch [3/100], Loss: 2.3701\n",
      "Epoch [4/100], Loss: 1.9147\n",
      "Epoch [5/100], Loss: 1.6647\n",
      "Epoch [6/100], Loss: 1.4740\n",
      "Epoch [7/100], Loss: 1.3449\n",
      "Epoch [8/100], Loss: 1.2671\n",
      "Epoch [9/100], Loss: 1.1868\n",
      "Epoch [10/100], Loss: 1.1323\n",
      "Epoch [11/100], Loss: 1.0805\n",
      "Epoch [12/100], Loss: 1.0350\n",
      "Epoch [13/100], Loss: 0.9944\n",
      "Epoch [14/100], Loss: 0.9561\n",
      "Epoch [15/100], Loss: 0.9252\n",
      "Epoch [16/100], Loss: 0.8903\n",
      "Epoch [17/100], Loss: 0.8775\n",
      "Epoch [18/100], Loss: 0.8519\n",
      "Epoch [19/100], Loss: 0.8290\n",
      "Epoch [20/100], Loss: 0.8047\n",
      "Epoch [21/100], Loss: 0.7739\n",
      "Epoch [22/100], Loss: 0.7664\n",
      "Epoch [23/100], Loss: 0.7421\n",
      "Epoch [24/100], Loss: 0.7284\n",
      "Epoch [25/100], Loss: 0.7046\n",
      "Epoch [26/100], Loss: 0.6896\n",
      "Epoch [27/100], Loss: 0.6855\n",
      "Epoch [28/100], Loss: 0.6626\n",
      "Epoch [29/100], Loss: 0.6511\n",
      "Epoch [30/100], Loss: 0.6468\n",
      "Epoch [31/100], Loss: 0.6320\n",
      "Epoch [32/100], Loss: 0.6166\n",
      "Epoch [33/100], Loss: 0.6085\n",
      "Epoch [34/100], Loss: 0.5996\n",
      "Epoch [35/100], Loss: 0.5894\n",
      "Epoch [36/100], Loss: 0.5782\n",
      "Epoch [37/100], Loss: 0.5768\n",
      "Epoch [38/100], Loss: 0.5626\n",
      "Epoch [39/100], Loss: 0.5517\n",
      "Epoch [40/100], Loss: 0.5449\n",
      "Epoch [41/100], Loss: 0.5321\n",
      "Epoch [42/100], Loss: 0.5252\n",
      "Epoch [43/100], Loss: 0.5173\n",
      "Epoch [44/100], Loss: 0.5152\n",
      "Epoch [45/100], Loss: 0.5021\n",
      "Epoch [46/100], Loss: 0.5052\n",
      "Epoch [47/100], Loss: 0.4908\n",
      "Epoch [48/100], Loss: 0.4869\n",
      "Epoch [49/100], Loss: 0.4783\n",
      "Epoch [50/100], Loss: 0.4697\n",
      "Epoch [51/100], Loss: 0.4646\n",
      "Epoch [52/100], Loss: 0.4580\n",
      "Epoch [53/100], Loss: 0.4525\n",
      "Epoch [54/100], Loss: 0.4520\n",
      "Epoch [55/100], Loss: 0.4413\n",
      "Epoch [56/100], Loss: 0.4401\n",
      "Epoch [57/100], Loss: 0.4379\n",
      "Epoch [58/100], Loss: 0.4296\n",
      "Epoch [59/100], Loss: 0.4261\n",
      "Epoch [60/100], Loss: 0.4220\n",
      "Epoch [61/100], Loss: 0.4128\n",
      "Epoch [62/100], Loss: 0.4069\n",
      "Epoch [63/100], Loss: 0.4021\n",
      "Epoch [64/100], Loss: 0.3980\n",
      "Epoch [65/100], Loss: 0.3929\n",
      "Epoch [66/100], Loss: 0.3894\n",
      "Epoch [67/100], Loss: 0.3865\n",
      "Epoch [68/100], Loss: 0.3830\n",
      "Epoch [69/100], Loss: 0.3834\n",
      "Epoch [70/100], Loss: 0.3775\n",
      "Epoch [71/100], Loss: 0.3717\n",
      "Epoch [72/100], Loss: 0.3675\n",
      "Epoch [73/100], Loss: 0.3620\n",
      "Epoch [74/100], Loss: 0.3617\n",
      "Epoch [75/100], Loss: 0.3578\n",
      "Epoch [76/100], Loss: 0.3544\n",
      "Epoch [77/100], Loss: 0.3528\n",
      "Epoch [78/100], Loss: 0.3445\n",
      "Epoch [79/100], Loss: 0.3464\n",
      "Epoch [80/100], Loss: 0.3430\n",
      "Epoch [81/100], Loss: 0.3371\n",
      "Epoch [82/100], Loss: 0.3342\n",
      "Epoch [83/100], Loss: 0.3323\n",
      "Epoch [84/100], Loss: 0.3280\n",
      "Epoch [85/100], Loss: 0.3249\n",
      "Epoch [86/100], Loss: 0.3252\n",
      "Epoch [87/100], Loss: 0.3192\n",
      "Epoch [88/100], Loss: 0.3183\n",
      "Epoch [89/100], Loss: 0.3139\n",
      "Epoch [90/100], Loss: 0.3127\n",
      "Epoch [91/100], Loss: 0.3081\n",
      "Epoch [92/100], Loss: 0.3071\n",
      "Epoch [93/100], Loss: 0.3069\n",
      "Epoch [94/100], Loss: 0.3027\n",
      "Epoch [95/100], Loss: 0.3006\n",
      "Epoch [96/100], Loss: 0.2977\n",
      "Epoch [97/100], Loss: 0.2960\n",
      "Epoch [98/100], Loss: 0.2940\n",
      "Epoch [99/100], Loss: 0.2914\n",
      "Epoch [100/100], Loss: 0.2874\n",
      "Test Accuracy Lipschitz Logit K Means: 62.70%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "lipschitz_logit_kmeans_plus = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_plus_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_plus_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    logit_model_lipschitz = LogisticRegression(input_dim, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(logit_model_lipschitz.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "    previous_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logit_model_lipschitz.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = logit_model_lipschitz(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            L = max(param.grad.abs().max().item() for param in logit_model_lipschitz.parameters() if param.grad is not None)\n",
    "            adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "            # Update optimizer's learning rate\n",
    "            for param_group in optimizer_sdg.param_groups:\n",
    "                param_group['lr'] = adaptive_lr\n",
    "\n",
    "\n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if previous_loss - current_loss < tolerance:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "                break\n",
    "        else:\n",
    "            epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "        previous_loss = current_loss\n",
    "\n",
    "    logit_model_lipschitz.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 28*28)\n",
    "            outputs = logit_model_lipschitz(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy Lipschitz Logit K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    lipschitz_logit_kmeans_plus[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68.879808</td>\n",
       "      <td>68.663462</td>\n",
       "      <td>66.216346</td>\n",
       "      <td>65.293269</td>\n",
       "      <td>62.701923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       75000      50000      10000      5000       1000 \n",
       "0  68.879808  68.663462  66.216346  65.293269  62.701923"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lipschitz_logit_kmeans_plus_df = pd.DataFrame(lipschitz_logit_kmeans_plus)\n",
    "lipschitz_logit_kmeans_plus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 98.8394\n",
      "Epoch [1/50], Val Loss: 0.3164\n",
      "Epoch [2/50], Loss: 45.7895\n",
      "Epoch [2/50], Val Loss: 0.2282\n",
      "Epoch [3/50], Loss: 37.1084\n",
      "Epoch [3/50], Val Loss: 0.2005\n",
      "Epoch [4/50], Loss: 32.3625\n",
      "Epoch [4/50], Val Loss: 0.1843\n",
      "Epoch [5/50], Loss: 29.0090\n",
      "Epoch [5/50], Val Loss: 0.1614\n",
      "Epoch [6/50], Loss: 25.9818\n",
      "Epoch [6/50], Val Loss: 0.1706\n",
      "Epoch [7/50], Loss: 23.6929\n",
      "Epoch [7/50], Val Loss: 0.1518\n",
      "Epoch [8/50], Loss: 21.1231\n",
      "Epoch [8/50], Val Loss: 0.1482\n",
      "Epoch [9/50], Loss: 19.3193\n",
      "Epoch [9/50], Val Loss: 0.1493\n",
      "Epoch [10/50], Loss: 17.2926\n",
      "Epoch [10/50], Val Loss: 0.1447\n",
      "Epoch [11/50], Loss: 16.0399\n",
      "Epoch [11/50], Val Loss: 0.1499\n",
      "Epoch [12/50], Loss: 14.5679\n",
      "Epoch [12/50], Val Loss: 0.1418\n",
      "Epoch [13/50], Loss: 12.9405\n",
      "Epoch [13/50], Val Loss: 0.1339\n",
      "Epoch [14/50], Loss: 11.7933\n",
      "Epoch [14/50], Val Loss: 0.1485\n",
      "Epoch [15/50], Loss: 11.2073\n",
      "Epoch [15/50], Val Loss: 0.1450\n",
      "Epoch [16/50], Loss: 10.1059\n",
      "Epoch [16/50], Val Loss: 0.1372\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 92.44%\n",
      "Epoch [1/50], Loss: 79.6455\n",
      "Epoch [1/50], Val Loss: 0.3353\n",
      "Epoch [2/50], Loss: 35.3983\n",
      "Epoch [2/50], Val Loss: 0.2615\n",
      "Epoch [3/50], Loss: 28.9746\n",
      "Epoch [3/50], Val Loss: 0.2189\n",
      "Epoch [4/50], Loss: 25.0847\n",
      "Epoch [4/50], Val Loss: 0.2248\n",
      "Epoch [5/50], Loss: 22.3897\n",
      "Epoch [5/50], Val Loss: 0.2017\n",
      "Epoch [6/50], Loss: 19.9190\n",
      "Epoch [6/50], Val Loss: 0.2095\n",
      "Epoch [7/50], Loss: 18.1137\n",
      "Epoch [7/50], Val Loss: 0.1873\n",
      "Epoch [8/50], Loss: 16.2575\n",
      "Epoch [8/50], Val Loss: 0.1859\n",
      "Epoch [9/50], Loss: 14.6779\n",
      "Epoch [9/50], Val Loss: 0.1910\n",
      "Epoch [10/50], Loss: 13.2997\n",
      "Epoch [10/50], Val Loss: 0.1976\n",
      "Epoch [11/50], Loss: 12.3116\n",
      "Epoch [11/50], Val Loss: 0.1926\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 91.96%\n",
      "Epoch [1/50], Loss: 34.6392\n",
      "Epoch [1/50], Val Loss: 0.9076\n",
      "Epoch [2/50], Loss: 13.7607\n",
      "Epoch [2/50], Val Loss: 0.6791\n",
      "Epoch [3/50], Loss: 9.4296\n",
      "Epoch [3/50], Val Loss: 0.5000\n",
      "Epoch [4/50], Loss: 6.9871\n",
      "Epoch [4/50], Val Loss: 0.5336\n",
      "Epoch [5/50], Loss: 5.9858\n",
      "Epoch [5/50], Val Loss: 0.4566\n",
      "Epoch [6/50], Loss: 4.8837\n",
      "Epoch [6/50], Val Loss: 0.4708\n",
      "Epoch [7/50], Loss: 4.0746\n",
      "Epoch [7/50], Val Loss: 0.4697\n",
      "Epoch [8/50], Loss: 3.3256\n",
      "Epoch [8/50], Val Loss: 0.4775\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 86.44%\n",
      "Epoch [1/50], Loss: 23.8466\n",
      "Epoch [1/50], Val Loss: 1.5893\n",
      "Epoch [2/50], Loss: 10.2531\n",
      "Epoch [2/50], Val Loss: 1.0127\n",
      "Epoch [3/50], Loss: 6.3196\n",
      "Epoch [3/50], Val Loss: 0.8121\n",
      "Epoch [4/50], Loss: 4.8543\n",
      "Epoch [4/50], Val Loss: 0.7624\n",
      "Epoch [5/50], Loss: 3.7286\n",
      "Epoch [5/50], Val Loss: 0.7091\n",
      "Epoch [6/50], Loss: 2.7810\n",
      "Epoch [6/50], Val Loss: 0.7495\n",
      "Epoch [7/50], Loss: 3.0251\n",
      "Epoch [7/50], Val Loss: 0.6113\n",
      "Epoch [8/50], Loss: 2.0885\n",
      "Epoch [8/50], Val Loss: 0.6813\n",
      "Epoch [9/50], Loss: 1.7407\n",
      "Epoch [9/50], Val Loss: 0.6613\n",
      "Epoch [10/50], Loss: 2.2892\n",
      "Epoch [10/50], Val Loss: 0.6647\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 82.15%\n",
      "Epoch [1/50], Loss: 6.4542\n",
      "Epoch [1/50], Val Loss: 3.0250\n",
      "Epoch [2/50], Loss: 5.4195\n",
      "Epoch [2/50], Val Loss: 2.1392\n",
      "Epoch [3/50], Loss: 3.1603\n",
      "Epoch [3/50], Val Loss: 2.2225\n",
      "Epoch [4/50], Loss: 2.9674\n",
      "Epoch [4/50], Val Loss: 1.9394\n",
      "Epoch [5/50], Loss: 1.9878\n",
      "Epoch [5/50], Val Loss: 1.4359\n",
      "Epoch [6/50], Loss: 1.5160\n",
      "Epoch [6/50], Val Loss: 1.3640\n",
      "Epoch [7/50], Loss: 1.1500\n",
      "Epoch [7/50], Val Loss: 1.3946\n",
      "Epoch [8/50], Loss: 0.8608\n",
      "Epoch [8/50], Val Loss: 1.4663\n",
      "Epoch [9/50], Loss: 0.7196\n",
      "Epoch [9/50], Val Loss: 1.4861\n",
      "Stopping early at epoch 9 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 66.57%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "cnn_kmeans_plus = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_plus_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_plus_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    cnn_model = EMNIST_CNN(num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(cnn_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "\n",
    "    def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)  # Set all weights to zero\n",
    "                m.bias.data.fill_(0.01)\n",
    "    cnn_model.apply(init_weights)\n",
    "\n",
    "    best_val_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cnn_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = cnn_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        cnn_model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader_cnn:\n",
    "                images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "                outputs = cnn_model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader_cnn)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "                break\n",
    "\n",
    "\n",
    "    cnn_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "            outputs = cnn_model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy CNN K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    cnn_kmeans_plus[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.4375</td>\n",
       "      <td>91.956731</td>\n",
       "      <td>86.4375</td>\n",
       "      <td>82.149038</td>\n",
       "      <td>66.567308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     75000      50000    10000      5000       1000 \n",
       "0  92.4375  91.956731  86.4375  82.149038  66.567308"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_kmeans_plus_df = pd.DataFrame(cnn_kmeans_plus)\n",
    "cnn_kmeans_plus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Lipschitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 97.5875\n",
      "Epoch [1/50], Val Loss: 0.2850\n",
      "Epoch [2/50], Loss: 44.8981\n",
      "Epoch [2/50], Val Loss: 0.2182\n",
      "Epoch [3/50], Loss: 36.7626\n",
      "Epoch [3/50], Val Loss: 0.1898\n",
      "Epoch [4/50], Loss: 32.6282\n",
      "Epoch [4/50], Val Loss: 0.1711\n",
      "Epoch [5/50], Loss: 29.0655\n",
      "Epoch [5/50], Val Loss: 0.1668\n",
      "Epoch [6/50], Loss: 26.2135\n",
      "Epoch [6/50], Val Loss: 0.1503\n",
      "Epoch [7/50], Loss: 23.9570\n",
      "Epoch [7/50], Val Loss: 0.1415\n",
      "Epoch [8/50], Loss: 21.4524\n",
      "Epoch [8/50], Val Loss: 0.1406\n",
      "Epoch [9/50], Loss: 19.6562\n",
      "Epoch [9/50], Val Loss: 0.1407\n",
      "Epoch [10/50], Loss: 18.0470\n",
      "Epoch [10/50], Val Loss: 0.1359\n",
      "Epoch [11/50], Loss: 16.3945\n",
      "Epoch [11/50], Val Loss: 0.1422\n",
      "Epoch [12/50], Loss: 14.8937\n",
      "Epoch [12/50], Val Loss: 0.1500\n",
      "Epoch [13/50], Loss: 13.9249\n",
      "Epoch [13/50], Val Loss: 0.1435\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 92.48%\n",
      "Epoch [1/50], Loss: 80.2956\n",
      "Epoch [1/50], Val Loss: 0.3712\n",
      "Epoch [2/50], Loss: 36.1110\n",
      "Epoch [2/50], Val Loss: 0.2757\n",
      "Epoch [3/50], Loss: 29.0439\n",
      "Epoch [3/50], Val Loss: 0.2355\n",
      "Epoch [4/50], Loss: 24.9178\n",
      "Epoch [4/50], Val Loss: 0.2154\n",
      "Epoch [5/50], Loss: 22.2188\n",
      "Epoch [5/50], Val Loss: 0.1941\n",
      "Epoch [6/50], Loss: 19.7561\n",
      "Epoch [6/50], Val Loss: 0.1948\n",
      "Epoch [7/50], Loss: 17.8152\n",
      "Epoch [7/50], Val Loss: 0.1946\n",
      "Epoch [8/50], Loss: 16.0438\n",
      "Epoch [8/50], Val Loss: 0.1876\n",
      "Epoch [9/50], Loss: 14.2159\n",
      "Epoch [9/50], Val Loss: 0.1873\n",
      "Epoch [10/50], Loss: 12.9103\n",
      "Epoch [10/50], Val Loss: 0.1891\n",
      "Epoch [11/50], Loss: 11.4829\n",
      "Epoch [11/50], Val Loss: 0.1898\n",
      "Epoch [12/50], Loss: 10.4447\n",
      "Epoch [12/50], Val Loss: 0.1791\n",
      "Epoch [13/50], Loss: 9.1544\n",
      "Epoch [13/50], Val Loss: 0.2060\n",
      "Epoch [14/50], Loss: 8.4100\n",
      "Epoch [14/50], Val Loss: 0.1940\n",
      "Epoch [15/50], Loss: 7.4777\n",
      "Epoch [15/50], Val Loss: 0.1958\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 91.91%\n",
      "Epoch [1/50], Loss: 36.1468\n",
      "Epoch [1/50], Val Loss: 0.9986\n",
      "Epoch [2/50], Loss: 13.8770\n",
      "Epoch [2/50], Val Loss: 0.7040\n",
      "Epoch [3/50], Loss: 9.5173\n",
      "Epoch [3/50], Val Loss: 0.6172\n",
      "Epoch [4/50], Loss: 7.5540\n",
      "Epoch [4/50], Val Loss: 0.4771\n",
      "Epoch [5/50], Loss: 6.1603\n",
      "Epoch [5/50], Val Loss: 0.4646\n",
      "Epoch [6/50], Loss: 4.9987\n",
      "Epoch [6/50], Val Loss: 0.4515\n",
      "Epoch [7/50], Loss: 4.2564\n",
      "Epoch [7/50], Val Loss: 0.4293\n",
      "Epoch [8/50], Loss: 3.5379\n",
      "Epoch [8/50], Val Loss: 0.4548\n",
      "Epoch [9/50], Loss: 2.9682\n",
      "Epoch [9/50], Val Loss: 0.4687\n",
      "Epoch [10/50], Loss: 2.5923\n",
      "Epoch [10/50], Val Loss: 0.4592\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 87.74%\n",
      "Epoch [1/50], Loss: 22.5938\n",
      "Epoch [1/50], Val Loss: 1.6266\n",
      "Epoch [2/50], Loss: 10.4855\n",
      "Epoch [2/50], Val Loss: 1.0686\n",
      "Epoch [3/50], Loss: 6.3642\n",
      "Epoch [3/50], Val Loss: 0.8292\n",
      "Epoch [4/50], Loss: 4.7574\n",
      "Epoch [4/50], Val Loss: 0.7917\n",
      "Epoch [5/50], Loss: 3.8095\n",
      "Epoch [5/50], Val Loss: 0.7204\n",
      "Epoch [6/50], Loss: 3.1608\n",
      "Epoch [6/50], Val Loss: 0.6718\n",
      "Epoch [7/50], Loss: 2.7837\n",
      "Epoch [7/50], Val Loss: 0.6656\n",
      "Epoch [8/50], Loss: 2.1613\n",
      "Epoch [8/50], Val Loss: 0.6437\n",
      "Epoch [9/50], Loss: 1.7567\n",
      "Epoch [9/50], Val Loss: 0.6914\n",
      "Epoch [10/50], Loss: 1.2087\n",
      "Epoch [10/50], Val Loss: 0.6542\n",
      "Epoch [11/50], Loss: 1.0892\n",
      "Epoch [11/50], Val Loss: 0.6766\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 83.59%\n",
      "Epoch [1/50], Loss: 6.5291\n",
      "Epoch [1/50], Val Loss: 3.1162\n",
      "Epoch [2/50], Loss: 5.8883\n",
      "Epoch [2/50], Val Loss: 2.6088\n",
      "Epoch [3/50], Loss: 4.2936\n",
      "Epoch [3/50], Val Loss: 1.9124\n",
      "Epoch [4/50], Loss: 3.0944\n",
      "Epoch [4/50], Val Loss: 2.0572\n",
      "Epoch [5/50], Loss: 2.4159\n",
      "Epoch [5/50], Val Loss: 1.8104\n",
      "Epoch [6/50], Loss: 2.0093\n",
      "Epoch [6/50], Val Loss: 1.4163\n",
      "Epoch [7/50], Loss: 1.4566\n",
      "Epoch [7/50], Val Loss: 1.3685\n",
      "Epoch [8/50], Loss: 1.1692\n",
      "Epoch [8/50], Val Loss: 1.4225\n",
      "Epoch [9/50], Loss: 0.8902\n",
      "Epoch [9/50], Val Loss: 1.4592\n",
      "Epoch [10/50], Loss: 0.7646\n",
      "Epoch [10/50], Val Loss: 1.3624\n",
      "Epoch [11/50], Loss: 0.6480\n",
      "Epoch [11/50], Val Loss: 1.3988\n",
      "Epoch [12/50], Loss: 0.4992\n",
      "Epoch [12/50], Val Loss: 1.4421\n",
      "Epoch [13/50], Loss: 0.4156\n",
      "Epoch [13/50], Val Loss: 1.4474\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 70.03%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "cnn_lipschitz_kmeans_plus = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_plus_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_plus_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    cnn_model_lipschitz = EMNIST_CNN(num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(cnn_model_lipschitz.parameters(), lr=0.005, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "\n",
    "    def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)  # Set all weights to zero\n",
    "                m.bias.data.fill_(0.01)\n",
    "    cnn_model_lipschitz.apply(init_weights)\n",
    "\n",
    "    best_val_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cnn_model_lipschitz.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = cnn_model_lipschitz(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            L = max(param.grad.abs().max().item() for param in cnn_model_lipschitz.parameters() if param.grad is not None)\n",
    "            adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "            # Update optimizer's learning rate\n",
    "            for param_group in optimizer_sdg.param_groups:\n",
    "                param_group['lr'] = adaptive_lr\n",
    "                \n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        cnn_model_lipschitz.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader_cnn:\n",
    "                images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "                outputs = cnn_model_lipschitz(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader_cnn)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "                break\n",
    "\n",
    "\n",
    "    cnn_model_lipschitz.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "            outputs = cnn_model_lipschitz(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy Lipschitz CNN K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    cnn_lipschitz_kmeans_plus[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.475962</td>\n",
       "      <td>91.913462</td>\n",
       "      <td>87.740385</td>\n",
       "      <td>83.586538</td>\n",
       "      <td>70.028846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       75000      50000      10000      5000       1000 \n",
       "0  92.475962  91.913462  87.740385  83.586538  70.028846"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_lipschitz_kmeans_plus_df = pd.DataFrame(cnn_lipschitz_kmeans_plus)\n",
    "cnn_lipschitz_kmeans_plus_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
